{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efb809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7416b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                              texte\n",
      "0  336.txt  center display flex var content hcekrn svg con...\n",
      "1  273.txt  white paper guidelines procurement september w...\n",
      "2  106.txt  big data new challenges law ethics internation...\n",
      "3  100.txt  globan maincontent position relative important...\n",
      "4  241.txt  echnical eport april artificial intelligence p...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Chemin vers le dossier contenant les fichiers .txt\n",
    "dossier = 'data/preprocessed/'\n",
    "\n",
    "data = []\n",
    "\n",
    "for fichier in os.listdir(dossier):\n",
    "    if fichier.endswith('.txt'):\n",
    "        chemin_fichier = os.path.join(dossier, fichier)\n",
    "        with open(chemin_fichier, 'r', encoding='utf-8') as f: \n",
    "            contenu = f.read()\n",
    "            data.append({\n",
    "                'id': fichier,  \n",
    "                'texte': contenu      \n",
    "            })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fc9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "df['tokenize'] = df['texte'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cab699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad831d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, voc = None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    n_samples = len(texts)\n",
    "    if voc == None:\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            words = words.union(set(text)) # list of all words\n",
    "        n_features = len(words) # number of different words\n",
    "        vocabulary = dict(zip(words, range(n_features))) # vocab[wd] = index ; indexisation\n",
    "    else:\n",
    "        vocabulary = voc\n",
    "        n_features = len(voc)\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    for k, text in enumerate(texts): # enumeration a k for a text[k]\n",
    "        for w in text:\n",
    "            if w in vocabulary:\n",
    "                counts[k][vocabulary[w]] += 1.\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, bow = count_words(df['tokenize'])\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca97f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "vectorizer = CountVectorizer()\n",
    "Bow = vectorizer.fit_transform(df['texte'])\n",
    "bow_a = Bow.toarray()\n",
    "print(bow_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfe8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = bow.sum(axis = 0)\n",
    "top_words = np.argsort(frequency)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53791d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_voc = {i: w for w, i in voc.items()}\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(15), frequency[top_words[:15]])\n",
    "ax.set_xticks(range(15))\n",
    "ax.set_xticklabels([rev_voc[i] for i in top_words[:15]], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1ec427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(u, v):\n",
    "    return np.linalg.norm(u-v)\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / np.sqrt(u.dot(u))\n",
    "\n",
    "def cosine(u, v):\n",
    "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db07fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_neighbors(distance, texts, representations, index, k=5):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance : function\n",
    "        The distance to use to compare documents\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    representations: 2D Array\n",
    "        Vector representations of the texts, in the same order\n",
    "    index: int\n",
    "        Index of the document for which to return nearest neighbors\n",
    "    k: int\n",
    "        Number of neighbors to display    \n",
    "    \"\"\"\n",
    "    neigh = NearestNeighbors(n_neighbors=k, algorithm='brute', metric=distance)\n",
    "    neigh.fit(representations) \n",
    "    dist, ind = neigh.kneighbors([representations[index]])\n",
    "    print(\"Plus proches voisins de: \\n '%s' \\n selon la distance '%s':\" % (texts[index], distance.__name__))\n",
    "    print([[texts[i] for i in s[1:]]  for s in ind])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, df['texte'], bow, 24)\n",
    "print_neighbors(cosine, df['texte'], bow, 24)\n",
    "\n",
    "print_neighbors(euclidean, df['texte'], bow_a, 24)\n",
    "print_neighbors(cosine, df['texte'], bow_a, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4e9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def tfidf_transform(bow):\n",
    "    \"\"\"\n",
    "    Inverse document frequencies applied to our bag-of-words representations\n",
    "    \"\"\"\n",
    "    # IDF\n",
    "    d = float(bow.shape[0]) + 1.0\n",
    "    in_doc = bow.astype(bool).sum(axis=0) + 1.0\n",
    "    idfs = np.log(d / in_doc) + 1.0\n",
    "    # TF\n",
    "    sum_vec = bow.sum(axis=1)\n",
    "    tfs = bow / np.expand_dims(sum_vec + 1.0, axis=1)\n",
    "    tf_idf = tfs * np.expand_dims(idfs,axis=0)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_transform(bow)\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e27bc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "Tfidf = tfidf_vectorizer.fit_transform(df['texte'])\n",
    "tfidf_a = Tfidf.toarray()\n",
    "print(tfidf_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa855b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, df['texte'], tfidf_a, 24)\n",
    "print_neighbors(cosine, df['texte'], tfidf_a, 24)\n",
    "# Formatage\n",
    "print_neighbors(euclidean, df['texte'], tfidf, 24)\n",
    "print_neighbors(cosine, df['texte'], tfidf, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a0e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a64ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "docs_pca = pca.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05441ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': docs_pca[:,0],\n",
    "                     'y': docs_pca[:,1],\n",
    "                     'texte': df['texte']})\n",
    "                     #'Category': categories_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e086ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['texte']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea44c3",
   "metadata": {},
   "source": [
    "### III - 2 With T-SNE\n",
    "\n",
    "From the ```sklearn``` documentation: \n",
    "- t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. **with different initializations we can get different results**.\n",
    "- In particular, t-SNE has the advantage to reveal data that lie in multiple, different, manifolds or clusters.\n",
    "- It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples.\n",
    "\n",
    "From this recommendation, we will initialize ```TSNE``` with PCA (choosing the argument ```init='pca'``` when creating the class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37f51222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb2f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca').fit_transform(tfidf)\n",
    "print(docs_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab325aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': docs_tsne[:,0],\n",
    "                     'y': docs_tsne[:,1],\n",
    "                     'texte': df['texte']})\n",
    "                     #'Category': categories_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['texte']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f9d88",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "                        \n",
    "- Is there any conclusion we can draw with respect to the lexical features and how they allow us to group the documents in this dataset ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c7f96",
   "metadata": {},
   "source": [
    "### III - 3 Topic modeling\n",
    "\n",
    "Now, the goal is to re-use the bag-of-words representations we obtained earlier - but reduce their dimension before visualization. \n",
    "\n",
    "The underlying idea is to **take advantage of the latent structure in the association between the set of\n",
    "words and the set of documents**. Many methods have been designed to do this - the earliest being **topic models**. \n",
    "\n",
    "Note that this allows to obtain reduced document representations, in a **topic space, common to documents and words** - where each document is described as a vector of topics and for each topic, we have access to the importance of words. \n",
    "\n",
    "\n",
    "We will do this with two models:\n",
    "- Using the ```TruncatedSVD```, we will **linearly** reduce the dimension of our BOW representations. This is called *Latent Semantic Analysis* (LSA). \n",
    "- Using a *generative model* based on several assumptions on how a document is generated through topics, which the model will retrieve: this is ```LatentDirichletAllocation``` (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b165899",
   "metadata": {},
   "source": [
    "We use here another dataset from this [paper](https://aclanthology.org/2024.latechclfl-1.28/) which includes quite more categories and will be more interesting to explore, as we can expect it to contain clusters clearly visible through looking at lexical features. You can find the dataset on their [git repository](https://git.unistra.fr/thealtres/stage-direction-classif-french-transfer-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad1373",
   "metadata": {},
   "source": [
    "First, apply the same pipeline than before:\n",
    "- Does the data need to be cleaned and pre-processed ?\n",
    "- Obtain BOW and TF-IDF representations.\n",
    "- Visualize them with T-SNE.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_th, bow_th = count_words(df['texte'])\n",
    "print(bow_th.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_th = tfidf_transform(bow_th)\n",
    "print(tfidf_th.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8582599",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                    init='pca').fit_transform(tfidf_th)\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': df['texte']})\n",
    "                        #'Category': df['labelGeneric']\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b67a2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230bad2",
   "metadata": {},
   "source": [
    "**Latent Semantic Analysis**: let us choose an arbitrary number of topics - which will be the size of the joint *topic space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ac15c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "lsa = TruncatedSVD(n_components = n_topics)\n",
    "lsa_topics = lsa.fit_transform(tfidf_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondances between documents and topics\n",
    "print(lsa_topics.shape)\n",
    "# Correspondances between topics and words\n",
    "print(lsa.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57ad8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversing the vocabulary to retrieve words from indexes, allowing to find the most important words for each topic\n",
    "rev_voc_th = {i: w for w, i in voc_th.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8466d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important_words(n, reverse_vocabulary, topic_model):\n",
    "    out = []\n",
    "    for i, topic in enumerate(topic_model.components_):\n",
    "        out.append([reverse_vocabulary[j] for j in topic.argsort()[:-n-1:-1]])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc_th, lsa)\n",
    "for i, topic in enumerate(words[:15]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504ffd3",
   "metadata": {},
   "source": [
    "With a dataset this size, over **short texts**, it is difficult to interpret the topics (many short words, even with TF-IDF). Let's apply T-SNE ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78923ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca', metric='cosine', perplexity=50.0).fit_transform(lsa_topics)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': df['texte']})\n",
    "                        #'Category': AS13_df['labelGeneric']})\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f952cf5",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dec1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = n_topics)\n",
    "lda_topics_th = lda.fit_transform(bow_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc_th, lda)\n",
    "for i, topic in enumerate(words[:15]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f97120",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca', metric='cosine', perplexity=50.0).fit_transform(lda_topics_th)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': df['texte']})\n",
    "                        #'Category': AS13_df['labelGeneric']\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964469f9",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Further question:</div>\n",
    "  \n",
    "- Are there any other features that we could consider with the same tools (Second dataset) ?\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "- Apply the pipeline to obtain a t-sne visualisation over these proposed features. Did it work as expected ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462ba09",
   "metadata": {},
   "source": [
    "### III - 4 Take away\n",
    "\n",
    "**Idea**: the key to improving representations is to embed data capturing text statistics in a compact space.\n",
    "\n",
    "But how ? \n",
    "Let's look at how a compact **modern (deep learning based) model** can better capture what's happening in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "875f973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c85bfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is very inefficient: it will take documents one by one and make them go through the model\n",
    "# We can usually process several of them together to gain time: this is called batching\n",
    "# Batching may require a large quantity of memory, and to avoid any issue when running this locally,\n",
    "# we will keep this (very slow and) inefficient solution. \n",
    "vectors = []\n",
    "for i, example in enumerate(df['texte'].tolist()):\n",
    "    inputs = tokenizer(example, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    vectors.append(outputs.last_hidden_state[0,0,:].detach().numpy()[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model outputs vectors of size 768\n",
    "cam_rep = np.concatenate(vectors, axis=0)\n",
    "print(cam_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f07a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "\n",
    "kmeans.fit(cam_rep)\n",
    "\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8184611",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                    init='random', metric='cosine',\n",
    "                    perplexity=50.0).fit_transform(cam_rep)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': df['texte'],\n",
    "                        'labels': labels\n",
    "                        })\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='labels',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bd85c",
   "metadata": {},
   "source": [
    "We will see how such a model (*CamemBERT*) works (relatively) soon ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_projet_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
