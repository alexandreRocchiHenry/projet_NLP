{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efb809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17089685",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25dccc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprepro = pd.read_csv('data_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a60a5bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Name of the document</th>\n",
       "      <th>Institution</th>\n",
       "      <th>URL</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Affiliates</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Country</th>\n",
       "      <th>Date</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>EP</th>\n",
       "      <th>Algorithm watch</th>\n",
       "      <th>CE</th>\n",
       "      <th>Winfield</th>\n",
       "      <th>EthicalML GitHub</th>\n",
       "      <th>all sources</th>\n",
       "      <th>Checked by</th>\n",
       "      <th>Unnamed: 27</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "      <th>Unnamed: 29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Unified Framework of Five Principles for AI ...</td>\n",
       "      <td>.</td>\n",
       "      <td>https://hdsr.mitpress.mit.edu/pub/l0jsh9d1/rel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AI Decolonial Manyfesto</td>\n",
       "      <td>.</td>\n",
       "      <td>https://manyfesto.ai/</td>\n",
       "      <td>Aarathi Krishnan, Angie Abdilla, A Jung Moon, ...</td>\n",
       "      <td>.</td>\n",
       "      <td>civil society</td>\n",
       "      <td>international</td>\n",
       "      <td>2021</td>\n",
       "      <td>collective, decolonial, petition</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maria</td>\n",
       "      <td>Should be included if was cited by anyone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Asimov’s three laws of Robotics</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Intel Recommends Public Policy Principles for ...</td>\n",
       "      <td>. Intel ?</td>\n",
       "      <td>https://community.intel.com/t5/Blogs/Intel/Pol...</td>\n",
       "      <td>Naveen Rao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Murphy and Wood’s three laws of Responsible Ro...</td>\n",
       "      <td>.</td>\n",
       "      <td>https://www.inf.ufrgs.br/~prestes/Courses/Robo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>724</td>\n",
       "      <td>Regulatory Technology for the 21st Century</td>\n",
       "      <td>World Economic Forum (WEF), Global Future Coun...</td>\n",
       "      <td>https://www3.weforum.org/docs/WEF_Regulatory_T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Simon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>725</td>\n",
       "      <td>Big Data and AI Principles in Engineering</td>\n",
       "      <td>World Federation of Engineering Organizations ...</td>\n",
       "      <td>http://www.wfeo.org/big-data-and-ai-principles...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>France</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Simon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>726</td>\n",
       "      <td>Generating evidence for artificial intelligenc...</td>\n",
       "      <td>World Health Organization (WHO)</td>\n",
       "      <td>https://www.who.int/publications/i/item/978924...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>international</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Simon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>728</td>\n",
       "      <td>Algorithms and Economic Justice: A Taxonomy of...</td>\n",
       "      <td>Yale Law School</td>\n",
       "      <td>https://yjolt.org/sites/default/files/23_yale_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SImon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>729</td>\n",
       "      <td>Z-Inspection</td>\n",
       "      <td>Z-Inspection</td>\n",
       "      <td>https://iris.who.int/bitstream/handle/10665/34...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Simon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                               Name of the document  \\\n",
       "0         0  A Unified Framework of Five Principles for AI ...   \n",
       "1         1                            AI Decolonial Manyfesto   \n",
       "2         2                    Asimov’s three laws of Robotics   \n",
       "3         3  Intel Recommends Public Policy Principles for ...   \n",
       "4         5  Murphy and Wood’s three laws of Responsible Ro...   \n",
       "..      ...                                                ...   \n",
       "622     724         Regulatory Technology for the 21st Century   \n",
       "623     725          Big Data and AI Principles in Engineering   \n",
       "624     726  Generating evidence for artificial intelligenc...   \n",
       "625     728  Algorithms and Economic Justice: A Taxonomy of...   \n",
       "626     729                                       Z-Inspection   \n",
       "\n",
       "                                           Institution  \\\n",
       "0                                                    .   \n",
       "1                                                    .   \n",
       "2                                                    .   \n",
       "3                                            . Intel ?   \n",
       "4                                                    .   \n",
       "..                                                 ...   \n",
       "622  World Economic Forum (WEF), Global Future Coun...   \n",
       "623  World Federation of Engineering Organizations ...   \n",
       "624                    World Health Organization (WHO)   \n",
       "625                                    Yale Law School   \n",
       "626                                       Z-Inspection   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    https://hdsr.mitpress.mit.edu/pub/l0jsh9d1/rel...   \n",
       "1                                https://manyfesto.ai/   \n",
       "2                                                  NaN   \n",
       "3    https://community.intel.com/t5/Blogs/Intel/Pol...   \n",
       "4    https://www.inf.ufrgs.br/~prestes/Courses/Robo...   \n",
       "..                                                 ...   \n",
       "622  https://www3.weforum.org/docs/WEF_Regulatory_T...   \n",
       "623  http://www.wfeo.org/big-data-and-ai-principles...   \n",
       "624  https://www.who.int/publications/i/item/978924...   \n",
       "625  https://yjolt.org/sites/default/files/23_yale_...   \n",
       "626  https://iris.who.int/bitstream/handle/10665/34...   \n",
       "\n",
       "                                               Authors Affiliates  \\\n",
       "0                                                  NaN        NaN   \n",
       "1    Aarathi Krishnan, Angie Abdilla, A Jung Moon, ...          .   \n",
       "2                                                  NaN        NaN   \n",
       "3                                           Naveen Rao        NaN   \n",
       "4                                                  NaN        NaN   \n",
       "..                                                 ...        ...   \n",
       "622                                                NaN        NaN   \n",
       "623                                                NaN        NaN   \n",
       "624                                                NaN        NaN   \n",
       "625                                                NaN        NaN   \n",
       "626                                                NaN        NaN   \n",
       "\n",
       "            Sector        Country  Date                          Keywords  \\\n",
       "0              NaN            NaN   NaN                               NaN   \n",
       "1    civil society  international  2021  collective, decolonial, petition   \n",
       "2              NaN            NaN   NaN                               NaN   \n",
       "3              NaN            NaN   NaN                               NaN   \n",
       "4              NaN            NaN   NaN                               NaN   \n",
       "..             ...            ...   ...                               ...   \n",
       "622            NaN             EU   NaN                               NaN   \n",
       "623            NaN         France   NaN                               NaN   \n",
       "624            NaN  international   NaN                               NaN   \n",
       "625            NaN            USA   NaN                               NaN   \n",
       "626            NaN        Germany   NaN                               NaN   \n",
       "\n",
       "     ...  EP Algorithm watch   CE  Winfield  EthicalML GitHub  all sources  \\\n",
       "0    ... NaN             NaN  NaN       1.0               NaN          1.0   \n",
       "1    ... NaN             NaN  NaN       NaN               NaN          0.0   \n",
       "2    ... NaN             NaN  NaN       1.0               NaN          1.0   \n",
       "3    ... NaN             NaN  NaN       1.0               NaN          2.0   \n",
       "4    ... NaN             NaN  NaN       1.0               NaN          1.0   \n",
       "..   ...  ..             ...  ...       ...               ...          ...   \n",
       "622  ... NaN             NaN  1.0       NaN               NaN          1.0   \n",
       "623  ... NaN             NaN  1.0       NaN               NaN          1.0   \n",
       "624  ... NaN             NaN  1.0       NaN               NaN          1.0   \n",
       "625  ... NaN             NaN  1.0       NaN               NaN          1.0   \n",
       "626  ... NaN             NaN  1.0       NaN               NaN          1.0   \n",
       "\n",
       "     Checked by                                Unnamed: 27  Unnamed: 28  \\\n",
       "0         Maria                                        NaN          NaN   \n",
       "1         Maria  Should be included if was cited by anyone          NaN   \n",
       "2         Maria                                        NaN          NaN   \n",
       "3         Maria                                        NaN          NaN   \n",
       "4         Maria                                        NaN          NaN   \n",
       "..          ...                                        ...          ...   \n",
       "622       Simon                                        NaN          NaN   \n",
       "623       Simon                                        NaN          NaN   \n",
       "624       Simon                                        NaN          NaN   \n",
       "625       SImon                                        NaN          NaN   \n",
       "626       Simon                                        NaN          NaN   \n",
       "\n",
       "     Unnamed: 29  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "..           ...  \n",
       "622          NaN  \n",
       "623          NaN  \n",
       "624          NaN  \n",
       "625          NaN  \n",
       "626          NaN  \n",
       "\n",
       "[627 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "Institution,Authors,Affiliates,Sector,Keywords,Status,Label,MapAIE (ours),Jobin,EP,Algorithm watch,CE,Winfield,EthicalML GitHub,all sources,Checked by,Unnamed: 27,Unnamed: 28,Unnamed: 29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a39e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Name of the document</th>\n",
       "      <th>URL</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Affiliates</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Country</th>\n",
       "      <th>Date</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Exclusion criteria</th>\n",
       "      <th>...</th>\n",
       "      <th>EP</th>\n",
       "      <th>Algorithm watch</th>\n",
       "      <th>CE</th>\n",
       "      <th>Winfield</th>\n",
       "      <th>EthicalML GitHub</th>\n",
       "      <th>all sources</th>\n",
       "      <th>Checked by</th>\n",
       "      <th>Unnamed: 27</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "      <th>Unnamed: 29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Institution</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>European Parliament</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organisation for Economic Co-operation and Development (OECD)</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>European Commission</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Nations Educational Scientific and Cultural Organization (UNESCO)</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Council of Europe (Committee of Ministers)</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AISP</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLAI</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Academy of Medical Royal Colleges</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accenture UK</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Verivox</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    doc_id  \\\n",
       "Institution                                                  \n",
       "European Parliament                                     35   \n",
       "Organisation for Economic Co-operation and Deve...      21   \n",
       "European Commission                                     16   \n",
       "United Nations Educational Scientific and Cultu...      12   \n",
       "Council of Europe (Committee of Ministers)               8   \n",
       "...                                                    ...   \n",
       "AISP                                                     1   \n",
       "ALLAI                                                    1   \n",
       "Academy of Medical Royal Colleges                        1   \n",
       "Accenture UK                                             1   \n",
       "Verivox                                                  1   \n",
       "\n",
       "                                                    Name of the document  URL  \\\n",
       "Institution                                                                     \n",
       "European Parliament                                                   35   35   \n",
       "Organisation for Economic Co-operation and Deve...                    21   21   \n",
       "European Commission                                                   16   16   \n",
       "United Nations Educational Scientific and Cultu...                    12   12   \n",
       "Council of Europe (Committee of Ministers)                             8    8   \n",
       "...                                                                  ...  ...   \n",
       "AISP                                                                   1    1   \n",
       "ALLAI                                                                  1    1   \n",
       "Academy of Medical Royal Colleges                                      1    1   \n",
       "Accenture UK                                                           1    1   \n",
       "Verivox                                                                1    1   \n",
       "\n",
       "                                                    Authors  Affiliates  \\\n",
       "Institution                                                               \n",
       "European Parliament                                       0           0   \n",
       "Organisation for Economic Co-operation and Deve...        1           1   \n",
       "European Commission                                       1           1   \n",
       "United Nations Educational Scientific and Cultu...        1           1   \n",
       "Council of Europe (Committee of Ministers)                0           0   \n",
       "...                                                     ...         ...   \n",
       "AISP                                                      0           0   \n",
       "ALLAI                                                     0           0   \n",
       "Academy of Medical Royal Colleges                         0           0   \n",
       "Accenture UK                                              1           1   \n",
       "Verivox                                                   0           0   \n",
       "\n",
       "                                                    Sector  Country  Date  \\\n",
       "Institution                                                                 \n",
       "European Parliament                                      0       35     0   \n",
       "Organisation for Economic Co-operation and Deve...       1       21     1   \n",
       "European Commission                                      1       16     1   \n",
       "United Nations Educational Scientific and Cultu...       1       12     1   \n",
       "Council of Europe (Committee of Ministers)               0        8     0   \n",
       "...                                                    ...      ...   ...   \n",
       "AISP                                                     0        1     0   \n",
       "ALLAI                                                    0        1     0   \n",
       "Academy of Medical Royal Colleges                        0        1     0   \n",
       "Accenture UK                                             1        1     1   \n",
       "Verivox                                                  0        1     0   \n",
       "\n",
       "                                                    Keywords  \\\n",
       "Institution                                                    \n",
       "European Parliament                                        0   \n",
       "Organisation for Economic Co-operation and Deve...         0   \n",
       "European Commission                                        0   \n",
       "United Nations Educational Scientific and Cultu...         0   \n",
       "Council of Europe (Committee of Ministers)                 0   \n",
       "...                                                      ...   \n",
       "AISP                                                       0   \n",
       "ALLAI                                                      0   \n",
       "Academy of Medical Royal Colleges                          0   \n",
       "Accenture UK                                               0   \n",
       "Verivox                                                    0   \n",
       "\n",
       "                                                    Exclusion criteria  ...  \\\n",
       "Institution                                                             ...   \n",
       "European Parliament                                                  0  ...   \n",
       "Organisation for Economic Co-operation and Deve...                   0  ...   \n",
       "European Commission                                                  0  ...   \n",
       "United Nations Educational Scientific and Cultu...                   0  ...   \n",
       "Council of Europe (Committee of Ministers)                           0  ...   \n",
       "...                                                                ...  ...   \n",
       "AISP                                                                 0  ...   \n",
       "ALLAI                                                                0  ...   \n",
       "Academy of Medical Royal Colleges                                    0  ...   \n",
       "Accenture UK                                                         0  ...   \n",
       "Verivox                                                              0  ...   \n",
       "\n",
       "                                                    EP  Algorithm watch  CE  \\\n",
       "Institution                                                                   \n",
       "European Parliament                                  0                1  34   \n",
       "Organisation for Economic Co-operation and Deve...   0                1  21   \n",
       "European Commission                                  0                1  15   \n",
       "United Nations Educational Scientific and Cultu...   0                2   9   \n",
       "Council of Europe (Committee of Ministers)           0                0   8   \n",
       "...                                                 ..              ...  ..   \n",
       "AISP                                                 0                0   0   \n",
       "ALLAI                                                0                0   1   \n",
       "Academy of Medical Royal Colleges                    0                1   0   \n",
       "Accenture UK                                         0                1   1   \n",
       "Verivox                                              0                1   0   \n",
       "\n",
       "                                                    Winfield  \\\n",
       "Institution                                                    \n",
       "European Parliament                                        0   \n",
       "Organisation for Economic Co-operation and Deve...         0   \n",
       "European Commission                                        0   \n",
       "United Nations Educational Scientific and Cultu...         0   \n",
       "Council of Europe (Committee of Ministers)                 0   \n",
       "...                                                      ...   \n",
       "AISP                                                       0   \n",
       "ALLAI                                                      0   \n",
       "Academy of Medical Royal Colleges                          0   \n",
       "Accenture UK                                               0   \n",
       "Verivox                                                    0   \n",
       "\n",
       "                                                    EthicalML GitHub  \\\n",
       "Institution                                                            \n",
       "European Parliament                                                0   \n",
       "Organisation for Economic Co-operation and Deve...                 0   \n",
       "European Commission                                                0   \n",
       "United Nations Educational Scientific and Cultu...                 0   \n",
       "Council of Europe (Committee of Ministers)                         0   \n",
       "...                                                              ...   \n",
       "AISP                                                               0   \n",
       "ALLAI                                                              0   \n",
       "Academy of Medical Royal Colleges                                  0   \n",
       "Accenture UK                                                       0   \n",
       "Verivox                                                            0   \n",
       "\n",
       "                                                    all sources  Checked by  \\\n",
       "Institution                                                                   \n",
       "European Parliament                                          35          35   \n",
       "Organisation for Economic Co-operation and Deve...           21          21   \n",
       "European Commission                                          16          16   \n",
       "United Nations Educational Scientific and Cultu...           12          12   \n",
       "Council of Europe (Committee of Ministers)                    8           8   \n",
       "...                                                         ...         ...   \n",
       "AISP                                                          1           1   \n",
       "ALLAI                                                         1           1   \n",
       "Academy of Medical Royal Colleges                             1           1   \n",
       "Accenture UK                                                  1           1   \n",
       "Verivox                                                       1           1   \n",
       "\n",
       "                                                    Unnamed: 27  Unnamed: 28  \\\n",
       "Institution                                                                    \n",
       "European Parliament                                           0            0   \n",
       "Organisation for Economic Co-operation and Deve...            0            0   \n",
       "European Commission                                           0            0   \n",
       "United Nations Educational Scientific and Cultu...            0            0   \n",
       "Council of Europe (Committee of Ministers)                    0            0   \n",
       "...                                                         ...          ...   \n",
       "AISP                                                          0            0   \n",
       "ALLAI                                                         0            0   \n",
       "Academy of Medical Royal Colleges                             0            0   \n",
       "Accenture UK                                                  0            0   \n",
       "Verivox                                                       0            0   \n",
       "\n",
       "                                                    Unnamed: 29  \n",
       "Institution                                                      \n",
       "European Parliament                                           0  \n",
       "Organisation for Economic Co-operation and Deve...            0  \n",
       "European Commission                                           0  \n",
       "United Nations Educational Scientific and Cultu...            3  \n",
       "Council of Europe (Committee of Ministers)                    0  \n",
       "...                                                         ...  \n",
       "AISP                                                          0  \n",
       "ALLAI                                                         0  \n",
       "Academy of Medical Royal Colleges                             0  \n",
       "Accenture UK                                                  0  \n",
       "Verivox                                                       0  \n",
       "\n",
       "[417 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.groupby(by=  \"Institution\").count().sort_values(ascending=False, by=\"doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7416b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                              texte\n",
      "0    0.txt  unified framework five principles society issu...\n",
      "1    1.txt  decolonial manyfesto current decolonial manyfe...\n",
      "2   10.txt  scalable change business accenture skip main c...\n",
      "3  100.txt  european aviation artificial intelligence high...\n",
      "4  101.txt  report artificial intelligence human society u...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Chemin vers le dossier contenant les fichiers .txt\n",
    "dossier = 'data/preprocessed/'\n",
    "\n",
    "data = []\n",
    "\n",
    "for fichier in os.listdir(dossier):\n",
    "    if fichier.endswith('.txt'):\n",
    "        chemin_fichier = os.path.join(dossier, fichier)\n",
    "        with open(chemin_fichier, 'r', encoding='utf-8') as f: \n",
    "            contenu = f.read()\n",
    "            data.append({\n",
    "                'id': fichier,  \n",
    "                'texte': contenu      \n",
    "            })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fc9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "df['tokenize'] = df['texte'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cab699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad831d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, voc = None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    n_samples = len(texts)\n",
    "    if voc == None:\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            words = words.union(set(text)) # list of all words\n",
    "        n_features = len(words) # number of different words\n",
    "        vocabulary = dict(zip(words, range(n_features))) # vocab[wd] = index ; indexisation\n",
    "    else:\n",
    "        vocabulary = voc\n",
    "        n_features = len(voc)\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    for k, text in enumerate(texts): # enumeration a k for a text[k]\n",
    "        for w in text:\n",
    "            if w in vocabulary:\n",
    "                counts[k][vocabulary[w]] += 1.\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, bow = count_words(df['tokenize'])\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca97f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "vectorizer = CountVectorizer()\n",
    "Bow = vectorizer.fit_transform(df['texte'])\n",
    "bow_a = Bow.toarray()\n",
    "print(bow_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfe8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = bow.sum(axis = 0)\n",
    "top_words = np.argsort(frequency)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53791d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_voc = {i: w for w, i in voc.items()}\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(15), frequency[top_words[:15]])\n",
    "ax.set_xticks(range(15))\n",
    "ax.set_xticklabels([rev_voc[i] for i in top_words[:15]], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1ec427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(u, v):\n",
    "    return np.linalg.norm(u-v)\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / np.sqrt(u.dot(u))\n",
    "\n",
    "def cosine(u, v):\n",
    "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db07fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_neighbors(distance, texts, representations, index, k=5):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance : function\n",
    "        The distance to use to compare documents\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    representations: 2D Array\n",
    "        Vector representations of the texts, in the same order\n",
    "    index: int\n",
    "        Index of the document for which to return nearest neighbors\n",
    "    k: int\n",
    "        Number of neighbors to display    \n",
    "    \"\"\"\n",
    "    neigh = NearestNeighbors(n_neighbors=k, algorithm='brute', metric=distance)\n",
    "    neigh.fit(representations) \n",
    "    dist, ind = neigh.kneighbors([representations[index]])\n",
    "    print(\"Plus proches voisins de: \\n '%s' \\n selon la distance '%s':\" % (texts[index], distance.__name__))\n",
    "    print([[texts[i] for i in s[1:]]  for s in ind])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, df['texte'], bow, 24)\n",
    "print_neighbors(cosine, df['texte'], bow, 24)\n",
    "\n",
    "print_neighbors(euclidean, df['texte'], bow_a, 24)\n",
    "print_neighbors(cosine, df['texte'], bow_a, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4e9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def tfidf_transform(bow):\n",
    "    \"\"\"\n",
    "    Inverse document frequencies applied to our bag-of-words representations\n",
    "    \"\"\"\n",
    "    # IDF\n",
    "    d = float(bow.shape[0]) + 1.0\n",
    "    in_doc = bow.astype(bool).sum(axis=0) + 1.0\n",
    "    idfs = np.log(d / in_doc) + 1.0\n",
    "    # TF\n",
    "    sum_vec = bow.sum(axis=1)\n",
    "    tfs = bow / np.expand_dims(sum_vec + 1.0, axis=1)\n",
    "    tf_idf = tfs * np.expand_dims(idfs,axis=0)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_transform(bow)\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e27bc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "Tfidf = tfidf_vectorizer.fit_transform(df['texte'])\n",
    "tfidf_a = Tfidf.toarray()\n",
    "print(tfidf_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa855b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_neighbors(euclidean, df['texte'], tfidf_a, 24)\n",
    "print_neighbors(cosine, df['texte'], tfidf_a, 24)\n",
    "# Formatage\n",
    "print_neighbors(euclidean, df['texte'], tfidf, 24)\n",
    "print_neighbors(cosine, df['texte'], tfidf, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a0e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a64ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "docs_pca = pca.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05441ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': docs_pca[:,0],\n",
    "                     'y': docs_pca[:,1],\n",
    "                     'texte': df['texte']})\n",
    "                     #'Category': categories_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e086ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['texte']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea44c3",
   "metadata": {},
   "source": [
    "### III - 2 With T-SNE\n",
    "\n",
    "From the ```sklearn``` documentation: \n",
    "- t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. **with different initializations we can get different results**.\n",
    "- In particular, t-SNE has the advantage to reveal data that lie in multiple, different, manifolds or clusters.\n",
    "- It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples.\n",
    "\n",
    "From this recommendation, we will initialize ```TSNE``` with PCA (choosing the argument ```init='pca'``` when creating the class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37f51222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb2f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca').fit_transform(tfidf)\n",
    "print(docs_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab325aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': docs_tsne[:,0],\n",
    "                     'y': docs_tsne[:,1],\n",
    "                     'texte': df['texte']})\n",
    "                     #'Category': categories_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(data[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['texte']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f9d88",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "                        \n",
    "- Is there any conclusion we can draw with respect to the lexical features and how they allow us to group the documents in this dataset ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c7f96",
   "metadata": {},
   "source": [
    "### III - 3 Topic modeling\n",
    "\n",
    "Now, the goal is to re-use the bag-of-words representations we obtained earlier - but reduce their dimension before visualization. \n",
    "\n",
    "The underlying idea is to **take advantage of the latent structure in the association between the set of\n",
    "words and the set of documents**. Many methods have been designed to do this - the earliest being **topic models**. \n",
    "\n",
    "Note that this allows to obtain reduced document representations, in a **topic space, common to documents and words** - where each document is described as a vector of topics and for each topic, we have access to the importance of words. \n",
    "\n",
    "\n",
    "We will do this with two models:\n",
    "- Using the ```TruncatedSVD```, we will **linearly** reduce the dimension of our BOW representations. This is called *Latent Semantic Analysis* (LSA). \n",
    "- Using a *generative model* based on several assumptions on how a document is generated through topics, which the model will retrieve: this is ```LatentDirichletAllocation``` (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b165899",
   "metadata": {},
   "source": [
    "We use here another dataset from this [paper](https://aclanthology.org/2024.latechclfl-1.28/) which includes quite more categories and will be more interesting to explore, as we can expect it to contain clusters clearly visible through looking at lexical features. You can find the dataset on their [git repository](https://git.unistra.fr/thealtres/stage-direction-classif-french-transfer-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad1373",
   "metadata": {},
   "source": [
    "First, apply the same pipeline than before:\n",
    "- Does the data need to be cleaned and pre-processed ?\n",
    "- Obtain BOW and TF-IDF representations.\n",
    "- Visualize them with T-SNE.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_th, bow_th = count_words(df['texte'])\n",
    "print(bow_th.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_th = tfidf_transform(bow_th)\n",
    "print(tfidf_th.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8582599",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                    init='pca').fit_transform(tfidf_th)\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': df['texte']})\n",
    "                        #'Category': df['labelGeneric']\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b67a2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230bad2",
   "metadata": {},
   "source": [
    "**Latent Semantic Analysis**: let us choose an arbitrary number of topics - which will be the size of the joint *topic space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ac15c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "lsa = TruncatedSVD(n_components = n_topics)\n",
    "lsa_topics = lsa.fit_transform(tfidf_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondances between documents and topics\n",
    "print(lsa_topics.shape)\n",
    "# Correspondances between topics and words\n",
    "print(lsa.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57ad8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversing the vocabulary to retrieve words from indexes, allowing to find the most important words for each topic\n",
    "rev_voc_th = {i: w for w, i in voc_th.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8466d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important_words(n, reverse_vocabulary, topic_model):\n",
    "    out = []\n",
    "    for i, topic in enumerate(topic_model.components_):\n",
    "        out.append([reverse_vocabulary[j] for j in topic.argsort()[:-n-1:-1]])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc_th, lsa)\n",
    "for i, topic in enumerate(words[:15]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504ffd3",
   "metadata": {},
   "source": [
    "With a dataset this size, over **short texts**, it is difficult to interpret the topics (many short words, even with TF-IDF). Let's apply T-SNE ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78923ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca', metric='cosine', perplexity=50.0).fit_transform(lsa_topics)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': df['texte']})\n",
    "                        #'Category': AS13_df['labelGeneric']})\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f952cf5",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dec1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = n_topics)\n",
    "lda_topics_th = lda.fit_transform(bow_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc_th, lda)\n",
    "for i, topic in enumerate(words[:15]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f97120",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics_th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='pca', metric='cosine', perplexity=50.0).fit_transform(lda_topics_th)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': df['texte']})\n",
    "                        #'Category': AS13_df['labelGeneric']\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\",# color='Category',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964469f9",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Further question:</div>\n",
    "  \n",
    "- Are there any other features that we could consider with the same tools (Second dataset) ?\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "- Apply the pipeline to obtain a t-sne visualisation over these proposed features. Did it work as expected ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462ba09",
   "metadata": {},
   "source": [
    "### III - 4 Take away\n",
    "\n",
    "**Idea**: the key to improving representations is to embed data capturing text statistics in a compact space.\n",
    "\n",
    "But how ? \n",
    "Let's look at how a compact **modern (deep learning based) model** can better capture what's happening in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "875f973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c85bfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is very inefficient: it will take documents one by one and make them go through the model\n",
    "# We can usually process several of them together to gain time: this is called batching\n",
    "# Batching may require a large quantity of memory, and to avoid any issue when running this locally,\n",
    "# we will keep this (very slow and) inefficient solution. \n",
    "vectors = []\n",
    "for i, example in enumerate(df['texte'].tolist()):\n",
    "    inputs = tokenizer(example, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    vectors.append(outputs.last_hidden_state[0,0,:].detach().numpy()[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model outputs vectors of size 768\n",
    "cam_rep = np.concatenate(vectors, axis=0)\n",
    "print(cam_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f07a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "\n",
    "kmeans.fit(cam_rep)\n",
    "\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8184611",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                    init='random', metric='cosine',\n",
    "                    perplexity=50.0).fit_transform(cam_rep)\n",
    "print(docs_tsne_th.shape)\n",
    "\n",
    "data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                        'y': docs_tsne_th[:,1],\n",
    "                        'Text': df['texte'],\n",
    "                        'labels': labels\n",
    "                        })\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='labels',\n",
    "    tooltip=['Text']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bd85c",
   "metadata": {},
   "source": [
    "We will see how such a model (*CamemBERT*) works (relatively) soon ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_projet_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
