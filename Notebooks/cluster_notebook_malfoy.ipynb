{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/Desktop/MS_IA/MS_IA/Projet_NLP/env_NLP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/alexandre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import altair as alt\n",
    "import prince\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(457, 37)\n",
      "(627, 31)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed dataframe\n",
    "data_proprocessed = \"../Data_csv/data_preprocessed.csv\"\n",
    "data_df = pd.read_csv(data_proprocessed)\n",
    "print(data_df.shape)\n",
    "df_temp = data_df[data_df['Name of the document'] != 'AI Ethics Resources']\n",
    "df_temp = df_temp[df_temp['Name of the document'] != 'Recommendation of the Council on Artificial Intelligence']\n",
    "df_temp = df_temp.reset_index(drop=True)\n",
    "\n",
    "data_global = \"../Data_csv/metadata.csv\"\n",
    "data_df = pd.read_csv(data_global)\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings functions\n",
    "def tfidf(df):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    Tfidf = tfidf_vectorizer.fit_transform(df['text_processed'])\n",
    "    tfidf_a = Tfidf.toarray()\n",
    "    return tfidf_a\n",
    "\n",
    "def vocabulary_fct(corpus, voc_threshold):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    word_counts = {}\n",
    "    for sent in corpus:\n",
    "        for word in [word.lower() for word in word_tokenize(sent) if word.isalpha()]:\n",
    "            if (word not in stopwords):\n",
    "                if (word not in word_counts):\n",
    "                    word_counts[word] = 0\n",
    "                word_counts[word] += 1           \n",
    "    words = sorted(word_counts.keys(), key=word_counts.get, reverse=True)\n",
    "    if voc_threshold > 0:\n",
    "        words = words[:voc_threshold] + ['UNK']   \n",
    "    vocabulary = {words[i] : i for i in range(len(words))}\n",
    "    return vocabulary, {word: word_counts.get(word, 0) for word in vocabulary}\n",
    "\n",
    "def vocabulary_fct_ngram(corpus, voc_threshold,ngram_range):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    word_counts = {}\n",
    "    for sent in corpus:\n",
    "        tokens = [word.lower() for word in word_tokenize(sent) if word.isalpha() and word not in stopwords]\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_list = [' '.join(gram) for gram in ngrams(tokens, n)]\n",
    "            for ngram in ngram_list:\n",
    "                if ngram not in word_counts:\n",
    "                    word_counts[ngram] = 0\n",
    "                word_counts[ngram] += 1          \n",
    "    words = sorted(word_counts.keys(), key=word_counts.get, reverse=True)\n",
    "    if voc_threshold > 0:\n",
    "        words = words[:voc_threshold] + ['UNK']   \n",
    "    vocabulary = {words[i] : i for i in range(len(words))}\n",
    "    return vocabulary, {word: word_counts.get(word, 0) for word in vocabulary}\n",
    "\n",
    "def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    l = len(vocabulary)\n",
    "    cooc_matrix = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Get the sentence\n",
    "        sent = [word.lower() for word in word_tokenize(sent) if word.isalpha()]\n",
    "        # Obtain the indexes of the words in the sentence from the vocabulary \n",
    "        sent_idx = [vocabulary.get(word, len(vocabulary)-1) for word in sent if (word not in stopwords)]\n",
    "        # Avoid one-word sentences - can create issues in normalization:\n",
    "        if len(sent_idx) == 1:\n",
    "                sent_idx.append(len(vocabulary)-1)\n",
    "        # Go through the indexes and add 1 / dist(i,j) to M[i,j] if words of index i and j appear in the same window\n",
    "        for i, idx in enumerate(sent_idx):\n",
    "            # If we consider a limited context:\n",
    "            if window > 0:\n",
    "                # Create a list containing the indexes that are on the left of the current index 'idx_i'\n",
    "                l_ctx_idx = [sent_idx[j] for j in range(max(0,i-window),i)]                \n",
    "            # If the context is the entire document:\n",
    "            else:\n",
    "                # The list containing the left context is easier to create\n",
    "                l_ctx_idx = sent_idx[:i]\n",
    "            # Go through the list and update M[i,j]:        \n",
    "            for j, ctx_idx in enumerate(l_ctx_idx):\n",
    "                if distance_weighting:\n",
    "                    weight = 1.0 / (len(l_ctx_idx) - j)\n",
    "                else:\n",
    "                    weight = 1.0\n",
    "                cooc_matrix[idx, ctx_idx] += weight * 1.0\n",
    "                cooc_matrix[ctx_idx, idx] += weight * 1.0\n",
    "    return cooc_matrix\n",
    "\n",
    "def co_occurence_matrix_ngram(corpus, vocabulary, window=0, distance_weighting=False):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    ngram_range = (2,3)\n",
    "    l = len(vocabulary)\n",
    "    cooc_matrix = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Get the sentence\n",
    "        words = [word.lower() for word in word_tokenize(sent) if word.isalpha() and word not in stopwords]\n",
    "        ngram_list = []\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_list.extend([' '.join(ngram) for ngram in ngrams(words, n)])\n",
    "        sent_idx = [vocabulary.get(ngram, len(vocabulary)-1) for ngram in ngram_list]\n",
    "        # Avoid one-word sentences - can create issues in normalization:\n",
    "        if len(sent_idx) == 1:\n",
    "                sent_idx.append(len(vocabulary)-1)\n",
    "        # Go through the indexes and add 1 / dist(i,j) to M[i,j] if words of index i and j appear in the same window\n",
    "        for i, idx in enumerate(sent_idx):\n",
    "            # If we consider a limited context:\n",
    "            if window > 0:\n",
    "                # Create a list containing the indexes that are on the left of the current index 'idx_i'\n",
    "                l_ctx_idx = [sent_idx[j] for j in range(max(0,i-window),i)]                \n",
    "            # If the context is the entire document:\n",
    "            else:\n",
    "                # The list containing the left context is easier to create\n",
    "                l_ctx_idx = sent_idx[:i]\n",
    "            # Go through the list and update M[i,j]:        \n",
    "            for j, ctx_idx in enumerate(l_ctx_idx):\n",
    "                if distance_weighting:\n",
    "                    weight = 1.0 / (len(l_ctx_idx) - j)\n",
    "                else:\n",
    "                    weight = 1.0\n",
    "                cooc_matrix[idx, ctx_idx] += weight * 1.0\n",
    "                cooc_matrix[ctx_idx, idx] += weight * 1.0\n",
    "    return cooc_matrix\n",
    "\n",
    "def pmi(co_oc, positive=True):\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    sum_tot = sum_vec.sum()\n",
    "    with np.errstate(divide='ignore'):\n",
    "        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                   \n",
    "    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
    "    if positive:\n",
    "        pmi[pmi < 0] = 0.0\n",
    "    return pmi\n",
    "\n",
    "def bigram_trigram(corpus):\n",
    "    bigram = list(ngrams(corpus, 2))\n",
    "    trigram = list(ngrams(corpus, 3))\n",
    "    return bigram, trigram\n",
    "\n",
    "\n",
    "def get_embedding(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(100)  \n",
    "\n",
    "def glove_embeddings(df):\n",
    "    loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "    all_embeddings = []\n",
    "    for text in df['text_processed']:\n",
    "        word_vectors = []\n",
    "        for word in text.split():\n",
    "            if word in loaded_glove_model:\n",
    "                word_vectors.append(loaded_glove_model[word])\n",
    "        if word_vectors:\n",
    "            sentence_embedding = np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            sentence_embedding = np.zeros(loaded_glove_model.vector_size)\n",
    "        all_embeddings.append(sentence_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def SVD_embeddings(df):\n",
    "    svd = TruncatedSVD(n_components=300)\n",
    "    texts = df['text'].values\n",
    "    vocab,_  = vocabulary_fct(texts, 5000)\n",
    "    print('Calcul M')\n",
    "    M = co_occurence_matrix(texts, vocab, window=5, distance_weighting=False)\n",
    "    print('Calcul SVD')\n",
    "    SVDEmbeddings = svd.fit_transform(M)\n",
    "    all_embeddings = []\n",
    "    for texts in df['text_processed']:\n",
    "        words = texts.split()\n",
    "        word_indices = [vocab.get(word) for word in words if word in vocab]\n",
    "        text_embeddings = [SVDEmbeddings[idx] for idx in word_indices if idx is not None]\n",
    "        if text_embeddings:\n",
    "            document_embedding = np.mean(text_embeddings, axis=0)\n",
    "        else:\n",
    "            document_embedding = np.zeros(svd.n_components)\n",
    "        all_embeddings.append(document_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def SVD_embeddings_PPMI(df):\n",
    "    svd_ppmi = TruncatedSVD(n_components=300)\n",
    "    texts = df['text'].values\n",
    "    vocab,_  = vocabulary_fct(texts, 5000)\n",
    "    print('Calcul M')\n",
    "    M = co_occurence_matrix(texts, vocab, window=5, distance_weighting=False)\n",
    "    print('Calcul PPMI')\n",
    "    PPMI = pmi(M)\n",
    "    print('Calcul SVD')\n",
    "    SVDEmbeddings = svd_ppmi.fit_transform(PPMI)\n",
    "    all_embeddings = []\n",
    "    for texts in df['text_processed']:\n",
    "        words = texts.split()\n",
    "        word_indices = [vocab.get(word) for word in words if word in vocab]\n",
    "        text_embeddings = [SVDEmbeddings[idx] for idx in word_indices if idx is not None]\n",
    "        if text_embeddings:\n",
    "            document_embedding = np.mean(text_embeddings, axis=0)\n",
    "        else:\n",
    "            document_embedding = np.zeros(svd_ppmi.n_components)\n",
    "        all_embeddings.append(document_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def SVD_embeddings_ngram(df):\n",
    "    ngram_range = (2,3)\n",
    "    svd = TruncatedSVD(n_components=300)\n",
    "    texts = df['text'].values\n",
    "    vocab,_  = vocabulary_fct_ngram(texts, 5000, ngram_range=ngram_range)\n",
    "    M = co_occurence_matrix_ngram(texts, vocab, window=5, distance_weighting=False)\n",
    "    SVDEmbeddings = svd.fit_transform(M)\n",
    "    all_embeddings = []\n",
    "    for texts in df['text_processed']:\n",
    "        words = texts.split()\n",
    "        ngram_list = []\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_list.extend([' '.join(ngram) for ngram in ngrams(words, n)])\n",
    "        ngram_indices = [vocab.get(ngram) for ngram in ngram_list if ngram in vocab]\n",
    "        text_embeddings = [SVDEmbeddings[idx] for idx in ngram_indices if idx is not None]\n",
    "        if text_embeddings:\n",
    "            document_embedding = np.mean(text_embeddings, axis=0)\n",
    "        else:\n",
    "            document_embedding = np.zeros(svd.n_components)\n",
    "        all_embeddings.append(document_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def SVD_embeddings_PPMI_ngram(df):\n",
    "    ngram_range = (2,3)\n",
    "    svd_ppmi = TruncatedSVD(n_components=300)\n",
    "    texts = df['text'].values\n",
    "    vocab,_  = vocabulary_fct_ngram(texts, 5000, ngram_range=ngram_range)\n",
    "    M = co_occurence_matrix_ngram(texts, vocab, window=5, distance_weighting=False)\n",
    "    PPMI = pmi(M)\n",
    "    SVDEmbeddings = svd_ppmi.fit_transform(PPMI)\n",
    "    all_embeddings = []\n",
    "    for texts in df['text_processed']:\n",
    "        words = texts.split()\n",
    "        ngram_list = []\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_list.extend([' '.join(ngram) for ngram in ngrams(words, n)])\n",
    "        ngram_indices = [vocab.get(ngram) for ngram in ngram_list if ngram in vocab]\n",
    "        text_embeddings = [SVDEmbeddings[idx] for idx in ngram_indices if idx is not None]\n",
    "        if text_embeddings:\n",
    "            document_embedding = np.mean(text_embeddings, axis=0)\n",
    "        else:\n",
    "            document_embedding = np.zeros(svd_ppmi.n_components)\n",
    "        all_embeddings.append(document_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def roberta_embeddings(df):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    model = RobertaModel.from_pretrained('roberta-large')\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    embeddings = []\n",
    "    for text in df['text_processed']:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    return embeddings_array\n",
    "\n",
    "def sentence_transformer_embeddings(df):\n",
    "    model_name='roberta-base-nli-stsb-mean-tokens'\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(df['text_processed'].tolist(), show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Dimension reduction functions\n",
    "def tsne(embeddings):\n",
    "    docs_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                init='pca').fit_transform(embeddings)\n",
    "    return docs_tsne\n",
    "\n",
    "def create_contingency_table(df, cluster_labels, theme_labels):\n",
    "    df_temp = df.copy()\n",
    "    df_temp['Cluster'] = cluster_labels\n",
    "    df_temp['Theme'] = theme_labels\n",
    "    contingency_table = pd.crosstab(df_temp['Cluster'], df_temp['Theme'])\n",
    "    return contingency_table\n",
    "\n",
    "\n",
    "def perform_correspondence_analysis(contingency_table):\n",
    "    ca = prince.CA(n_components=2, n_iter=10, copy=True, check_input=True, engine='auto')\n",
    "    ca = ca.fit(contingency_table)\n",
    "    row_coords = ca.row_coordinates(contingency_table)\n",
    "    col_coords = ca.column_coordinates(contingency_table)\n",
    "    return ca, row_coords, col_coords\n",
    "\n",
    "def plot_ca_results(row_coords, col_coords, embedding_name, clustering_name):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "   \n",
    "    ax.scatter(row_coords[0], row_coords[1], c='blue', label='Clusters')\n",
    "    for i, txt in enumerate(row_coords.index):\n",
    "        ax.annotate(txt, (row_coords.iloc[i, 0], row_coords.iloc[i, 1]))\n",
    "\n",
    "\n",
    "    ax.scatter(col_coords[0], col_coords[1], c='red', label='Themes', marker='x')\n",
    "    for i, txt in enumerate(col_coords.index):\n",
    "        ax.annotate(txt, (col_coords.iloc[i, 0], col_coords.iloc[i, 1]))\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_title('Correspondence Analysis')\n",
    "    plt.savefig(f'T4_initial_clustering_{embedding_name}_{clustering_name}_CA.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_ca(embeddings, cluster_labels, df, embedding_name, clustering_name):\n",
    "    theme_labels = df['theme']\n",
    "    contingency_table = create_contingency_table(df, cluster_labels, theme_labels)\n",
    "    ca, row_coords, col_coords = perform_correspondence_analysis(contingency_table)\n",
    "    plot_ca_results(row_coords, col_coords, embedding_name, clustering_name)\n",
    "\n",
    "\n",
    "def pca(embeddings):\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_pca = pca.fit_transform(embeddings)\n",
    "    explained_variance = pca.explained_variance_ratio_ \n",
    "    print(f\"Explained variance: {explained_variance}\")\n",
    "    return embeddings_pca\n",
    "\n",
    "# Clusterings functions\n",
    "def Kmeans_clustering(n_clusters, embeddings, model_name):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(embeddings)\n",
    "    labels = kmeans.labels_\n",
    "    return labels\n",
    "\n",
    "def gaussian_clustering(n_clusters, embeddings, model_name):\n",
    "    if model_name == 'roberta_embeddings':\n",
    "        n_clusters = n_clusters - 3\n",
    "    if model_name == 'sentence_transformer_embeddings':\n",
    "        n_clusters = n_clusters\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=0, covariance_type='diag', reg_covar=1e-6)\n",
    "    gmm.fit(embeddings)\n",
    "    labels = gmm.predict(embeddings)\n",
    "    return labels\n",
    "\n",
    "def hierarchical_clustering(n_clusters, embeddings, model_name):\n",
    "    hc = AgglomerativeClustering(n_clusters=n_clusters, metric='euclidean', linkage='ward')\n",
    "    labels = hc.fit_predict(embeddings)\n",
    "    return labels\n",
    "\n",
    "def correspondence_analysis(embeddings, n_components=2):\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    embeddings_ca = svd.fit_transform(embeddings)\n",
    "    return embeddings_ca\n",
    "\n",
    "# Validation function\n",
    "def score_function(embeddings, labels):\n",
    "    silhouette_s = silhouette_score(embeddings, labels)\n",
    "    davies_bouldin_s = davies_bouldin_score(embeddings, labels)\n",
    "    calinski_harabasz_s = calinski_harabasz_score(embeddings, labels)\n",
    "    return silhouette_s, davies_bouldin_s, calinski_harabasz_s\n",
    "\n",
    "# def display_ca(embeddings, df, labels, embedding_name, clustering_name):\n",
    "#     embeddings_ca = correspondence_analysis(embeddings)\n",
    "#     data_ca = pd.DataFrame({'x': embeddings_ca[:, 0],\n",
    "#                             'y': embeddings_ca[:, 1],\n",
    "#                             'institution': df[labels],\n",
    "#                             'title': df[\"Name of the document\"],\n",
    "#                             'labels': df[labels]\n",
    "#                             })\n",
    "#     alt.data_transformers.disable_max_rows()\n",
    "#     chart = alt.Chart(data_ca).mark_circle(size=200).encode(\n",
    "#         x=\"x\", y=\"y\", color=alt.Color('labels:N', scale=alt.Scale(scheme='category20')),\n",
    "#         tooltip=['institution', \"title\"]\n",
    "#         ).interactive().properties(\n",
    "#         width=500,\n",
    "#         height=500\n",
    "#     )\n",
    "#     chart.save(f'T4_initial_clustering_{embedding_name}_{clustering_name}_CA.html')\n",
    "#     chart.show()\n",
    "    \n",
    "\n",
    "def display_pca(embeddings, df, labels, embedding_name, clustering_name):\n",
    "    embeddings_pca = pca(embeddings)\n",
    "    data_pca = pd.DataFrame({'x': embeddings_pca[:, 0],\n",
    "                            'y': embeddings_pca[:, 1],\n",
    "                            'institution': df[labels],\n",
    "                            'title': df[\"Name of the document\"],\n",
    "                            'labels': labels\n",
    "                            })\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "    chart = alt.Chart(data_pca).mark_circle(size=200).encode(\n",
    "        x=\"x\", y=\"y\", color=alt.Color('labels:N', scale=alt.Scale(scheme='category20')),\n",
    "        tooltip=['institution', \"title\"]\n",
    "        ).interactive().properties(\n",
    "        width=500,\n",
    "        height=500\n",
    "    )\n",
    "    chart.save(f'T5_initial_clustering_{embedding_name}_{clustering_name}_PCA.html')\n",
    "    chart.show()   \n",
    "\n",
    "def display_tsne(embeddings, df, labels, embedding_name, clustering_name):\n",
    "    docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                        init='random', metric='cosine',\n",
    "                        perplexity=50.0).fit_transform(embeddings)\n",
    "    print(docs_tsne_th.shape)\n",
    "\n",
    "    data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                            'y': docs_tsne_th[:,1],\n",
    "                            'institution': df[labels],\n",
    "                            'title': df[\"Name of the document\"],\n",
    "                            'labels': labels\n",
    "                            #'labels': df[\"categorie Institution\"]\n",
    "                            #'labels': df[\"theme\"]\n",
    "                            })\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "    chart = alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "        x=\"x\", y=\"y\", color=alt.Color('labels:N', \n",
    "                                    scale=alt.Scale(scheme='category20')),\n",
    "        tooltip=['institution', \"title\"]\n",
    "        ).interactive().properties(\n",
    "        width=500,\n",
    "        height=500\n",
    "    )\n",
    "    chart.save(f'T5_initial_clustering_{embedding_name}_{clustering_name}_TSNE.html')\n",
    "    chart.show()\n",
    "\n",
    "\n",
    "# Clustering pipeline\n",
    "# \n",
    "def pipeline(dataframe, embedding_method, clustering_method, taille_cluster=[10,11], reduction_method=display_pca):\n",
    "    print(f\"start embedding for {embedding_method.__name__} and {clustering_method.__name__}\")\n",
    "    embeddings = embedding_method(dataframe)\n",
    "    print(\"clustering\")\n",
    "    for i in range(taille_cluster[0], taille_cluster[1]):\n",
    "        labels = clustering_method(i, embeddings, embedding_method.__name__)\n",
    "    print(\"scoring\")\n",
    "    scores = score_function(embeddings, labels)\n",
    "    print(f\"silhouette_score: {scores[0]}, davies_bouldin_score: {scores[1]}, calinski_harabasz_score: {scores[2]}\")\n",
    "    # reduction_method(embeddings, dataframe, labels, embedding_method.__name__, clustering_method.__name__)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proprocessed = \"../Data_csv/data_preprocessed.csv\"\n",
    "data_df = pd.read_csv(data_proprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start embedding for SVD_embeddings_ngram and Kmeans_clustering\n",
      "clustering\n",
      "scoring\n",
      "silhouette_score: 0.5522506597964759, davies_bouldin_score: 0.430828274845849, calinski_harabasz_score: 2789.092463239891\n",
      "start embedding for SVD_embeddings_PPMI_ngram and Kmeans_clustering\n",
      "clustering\n",
      "scoring\n",
      "silhouette_score: 0.1006160265588731, davies_bouldin_score: 1.75189579504566, calinski_harabasz_score: 37.65350128522544\n"
     ]
    }
   ],
   "source": [
    "Clustering_methods = [Kmeans_clustering]\n",
    "Embedding_methods = [SVD_embeddings_ngram, SVD_embeddings_PPMI_ngram]\n",
    "reduction_methods = [display_tsne]\n",
    "\n",
    "results = []\n",
    "\n",
    "for embedding_method in Embedding_methods:\n",
    "    for cluster_method in Clustering_methods:\n",
    "        for reduction in reduction_methods:\n",
    "            \n",
    "            result = pipeline(dataframe=data_df, \n",
    "                            embedding_method=embedding_method,\n",
    "                            clustering_method=cluster_method,\n",
    "                            reduction_method=reduction\n",
    "                            )\n",
    "            \n",
    "            results.append({\n",
    "                'Embedding Method': embedding_method.__name__,\n",
    "                'Clustering Method': cluster_method.__name__,\n",
    "                'Reduction Method': reduction.__name__,\n",
    "                'silhoutte score': result[0],  \n",
    "                'davies score' : result[1], \n",
    "                'calinski score' : result[2],\n",
    "                    \n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering_methods = [Kmeans_clustering]\n",
    "Embedding_methods = [SVD_embeddings_ngram]\n",
    "reduction_methods = [display_tsne]\n",
    "taille_cluster=[10,11]\n",
    "\n",
    "embeddings = SVD_embeddings_ngram(data_df)\n",
    "for i in range(taille_cluster[0], taille_cluster[1]):\n",
    "        labels = Kmeans_clustering(i, embeddings, SVD_embeddings_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlabels\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
