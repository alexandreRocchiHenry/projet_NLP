{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexandre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import altair as alt\n",
    "import prince\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(472, 37)\n",
      "(627, 31)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed dataframe\n",
    "data_proprocessed = \"../Data_csv/data_preprocessed.csv\"\n",
    "data_df = pd.read_csv(data_proprocessed)\n",
    "print(data_df.shape)\n",
    "df_temp = data_df[data_df['Name of the document'] != 'AI Ethics Resources']\n",
    "df_temp = df_temp[df_temp['Name of the document'] != 'Recommendation of the Council on Artificial Intelligence']\n",
    "df_temp = df_temp.reset_index(drop=True)\n",
    "\n",
    "data_global = \"../Data_csv/metadata.csv\"\n",
    "data_df = pd.read_csv(data_global)\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings functions\n",
    "def tfidf(df):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    Tfidf = tfidf_vectorizer.fit_transform(df['text_processed'])\n",
    "    tfidf_a = Tfidf.toarray()\n",
    "    return tfidf_a\n",
    "\n",
    "def vocabulary_fct(corpus, voc_threshold):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    word_counts = {}\n",
    "    for sent in corpus:\n",
    "        for word in [word.lower() for word in word_tokenize(sent) if word.isalpha()]:\n",
    "            if (word not in stopwords):\n",
    "                if (word not in word_counts):\n",
    "                    word_counts[word] = 0\n",
    "                word_counts[word] += 1           \n",
    "    words = sorted(word_counts.keys(), key=word_counts.get, reverse=True)\n",
    "    if voc_threshold > 0:\n",
    "        words = words[:voc_threshold] + ['UNK']   \n",
    "    vocabulary = {words[i] : i for i in range(len(words))}\n",
    "    return vocabulary, {word: word_counts.get(word, 0) for word in vocabulary}\n",
    "\n",
    "def vocabulary_fct_ngram(corpus, voc_threshold,ngram_range):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    word_counts = {}\n",
    "    for sent in corpus:\n",
    "        tokens = [word.lower() for word in word_tokenize(sent) if word.isalpha() and word not in stopwords]\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_list = [' '.join(gram) for gram in ngrams(tokens, n)]\n",
    "            for ngram in ngram_list:\n",
    "                if ngram not in word_counts:\n",
    "                    word_counts[ngram] = 0\n",
    "                word_counts[ngram] += 1          \n",
    "    words = sorted(word_counts.keys(), key=word_counts.get, reverse=True)\n",
    "    if voc_threshold > 0:\n",
    "        words = words[:voc_threshold] + ['UNK']   \n",
    "    vocabulary = {words[i] : i for i in range(len(words))}\n",
    "    return vocabulary, {word: word_counts.get(word, 0) for word in vocabulary}\n",
    "\n",
    "def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    l = len(vocabulary)\n",
    "    cooc_matrix = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Get the sentence\n",
    "        sent = [word.lower() for word in word_tokenize(sent) if word.isalpha()]\n",
    "        # Obtain the indexes of the words in the sentence from the vocabulary \n",
    "        sent_idx = [vocabulary.get(word, len(vocabulary)-1) for word in sent if (word not in stopwords)]\n",
    "        # Avoid one-word sentences - can create issues in normalization:\n",
    "        if len(sent_idx) == 1:\n",
    "                sent_idx.append(len(vocabulary)-1)\n",
    "        # Go through the indexes and add 1 / dist(i,j) to M[i,j] if words of index i and j appear in the same window\n",
    "        for i, idx in enumerate(sent_idx):\n",
    "            # If we consider a limited context:\n",
    "            if window > 0:\n",
    "                # Create a list containing the indexes that are on the left of the current index 'idx_i'\n",
    "                l_ctx_idx = [sent_idx[j] for j in range(max(0,i-window),i)]                \n",
    "            # If the context is the entire document:\n",
    "            else:\n",
    "                # The list containing the left context is easier to create\n",
    "                l_ctx_idx = sent_idx[:i]\n",
    "            # Go through the list and update M[i,j]:        \n",
    "            for j, ctx_idx in enumerate(l_ctx_idx):\n",
    "                if distance_weighting:\n",
    "                    weight = 1.0 / (len(l_ctx_idx) - j)\n",
    "                else:\n",
    "                    weight = 1.0\n",
    "                cooc_matrix[idx, ctx_idx] += weight * 1.0\n",
    "                cooc_matrix[ctx_idx, idx] += weight * 1.0\n",
    "    return cooc_matrix\n",
    "\n",
    "def co_occurence_matrix_ngram(corpus, vocabulary, window=0, distance_weighting=False):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    ngram_range = (2,3)\n",
    "    l = len(vocabulary)\n",
    "    cooc_matrix = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Get the sentence\n",
    "        words = [word.lower() for word in word_tokenize(sent) if word.isalpha() and word not in stopwords]\n",
    "        ngram_list = []\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_list.extend([' '.join(ngram) for ngram in ngrams(words, n)])\n",
    "        sent_idx = [vocabulary.get(ngram, len(vocabulary)-1) for ngram in ngram_list]\n",
    "        # Avoid one-word sentences - can create issues in normalization:\n",
    "        if len(sent_idx) == 1:\n",
    "                sent_idx.append(len(vocabulary)-1)\n",
    "        # Go through the indexes and add 1 / dist(i,j) to M[i,j] if words of index i and j appear in the same window\n",
    "        for i, idx in enumerate(sent_idx):\n",
    "            # If we consider a limited context:\n",
    "            if window > 0:\n",
    "                # Create a list containing the indexes that are on the left of the current index 'idx_i'\n",
    "                l_ctx_idx = [sent_idx[j] for j in range(max(0,i-window),i)]                \n",
    "            # If the context is the entire document:\n",
    "            else:\n",
    "                # The list containing the left context is easier to create\n",
    "                l_ctx_idx = sent_idx[:i]\n",
    "            # Go through the list and update M[i,j]:        \n",
    "            for j, ctx_idx in enumerate(l_ctx_idx):\n",
    "                if distance_weighting:\n",
    "                    weight = 1.0 / (len(l_ctx_idx) - j)\n",
    "                else:\n",
    "                    weight = 1.0\n",
    "                cooc_matrix[idx, ctx_idx] += weight * 1.0\n",
    "                cooc_matrix[ctx_idx, idx] += weight * 1.0\n",
    "    return cooc_matrix\n",
    "\n",
    "def pmi(co_oc, positive=True):\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    sum_tot = sum_vec.sum()\n",
    "    with np.errstate(divide='ignore'):\n",
    "        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                   \n",
    "    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
    "    if positive:\n",
    "        pmi[pmi < 0] = 0.0\n",
    "    return pmi\n",
    "\n",
    "def bigram_trigram(corpus):\n",
    "    bigram = list(ngrams(corpus, 2))\n",
    "    trigram = list(ngrams(corpus, 3))\n",
    "    return bigram, trigram\n",
    "\n",
    "\n",
    "def get_embedding(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(100)  \n",
    "\n",
    "def glove_embeddings(df):\n",
    "    loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "    all_embeddings = []\n",
    "    for text in df['text_processed']:\n",
    "        word_vectors = []\n",
    "        for word in text.split():\n",
    "            if word in loaded_glove_model:\n",
    "                word_vectors.append(loaded_glove_model[word])\n",
    "        if word_vectors:\n",
    "            sentence_embedding = np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            sentence_embedding = np.zeros(loaded_glove_model.vector_size)\n",
    "        all_embeddings.append(sentence_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def SVD_embeddings(df):\n",
    "    svd = TruncatedSVD(n_components=300)\n",
    "    texts = df['text'].values\n",
    "    vocab,_  = vocabulary_fct(texts, 5000)\n",
    "    M = co_occurence_matrix(texts, vocab, window=5, distance_weighting=False)\n",
    "    SVDEmbeddings = svd.fit_transform(M)\n",
    "    all_embeddings = []\n",
    "    for texts in df['text_processed']:\n",
    "        words = texts.split()\n",
    "        word_indices = [vocab.get(word) for word in words if word in vocab]\n",
    "        text_embeddings = [SVDEmbeddings[idx] for idx in word_indices if idx is not None]\n",
    "        if text_embeddings:\n",
    "            document_embedding = np.mean(text_embeddings, axis=0)\n",
    "        else:\n",
    "            document_embedding = np.zeros(svd.n_components)\n",
    "        all_embeddings.append(document_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def SVD_embeddings_PPMI(df):\n",
    "    svd_ppmi = TruncatedSVD(n_components=300)\n",
    "    texts = df['text'].values\n",
    "    vocab,_  = vocabulary_fct(texts, 5000)\n",
    "    M = co_occurence_matrix(texts, vocab, window=5, distance_weighting=False)\n",
    "    PPMI = pmi(M)\n",
    "    SVDEmbeddings = svd_ppmi.fit_transform(PPMI)\n",
    "    all_embeddings = []\n",
    "    for texts in df['text_processed']:\n",
    "        words = texts.split()\n",
    "        word_indices = [vocab.get(word) for word in words if word in vocab]\n",
    "        text_embeddings = [SVDEmbeddings[idx] for idx in word_indices if idx is not None]\n",
    "        if text_embeddings:\n",
    "            document_embedding = np.mean(text_embeddings, axis=0)\n",
    "        else:\n",
    "            document_embedding = np.zeros(svd_ppmi.n_components)\n",
    "        all_embeddings.append(document_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def SVD_embeddings_ngram(df):\n",
    "    ngram_range = (2,3)\n",
    "    svd = TruncatedSVD(n_components=300)\n",
    "    texts = df['text'].values\n",
    "    vocab,_  = vocabulary_fct_ngram(texts, 5000, ngram_range=ngram_range)\n",
    "    M = co_occurence_matrix_ngram(texts, vocab, window=5, distance_weighting=False)\n",
    "    SVDEmbeddings = svd.fit_transform(M)\n",
    "    all_embeddings = []\n",
    "    for texts in df['text_processed']:\n",
    "        words = texts.split()\n",
    "        ngram_list = []\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngram_list.extend([' '.join(ngram) for ngram in ngrams(words, n)])\n",
    "        ngram_indices = [vocab.get(ngram) for ngram in ngram_list if ngram in vocab]\n",
    "        text_embeddings = [SVDEmbeddings[idx] for idx in ngram_indices if idx is not None]\n",
    "        if text_embeddings:\n",
    "            document_embedding = np.mean(text_embeddings, axis=0)\n",
    "        else:\n",
    "            document_embedding = np.zeros(svd.n_components)\n",
    "        all_embeddings.append(document_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def SVD_embeddings_PPMI_ngram(df):\n",
    "    svd_ppmi = TruncatedSVD(n_components=300)\n",
    "    texts = df['text'].values\n",
    "    vocab,_  = vocabulary_fct(texts, 5000)\n",
    "    M = co_occurence_matrix_ngram(texts, vocab, window=5, distance_weighting=False)\n",
    "    PPMI = pmi(M)\n",
    "    SVDEmbeddings = svd_ppmi.fit_transform(PPMI)\n",
    "    all_embeddings = []\n",
    "    for texts in df['text_processed']:\n",
    "        words = texts.split()\n",
    "        ngram_list = [' '.join(ngram) for ngram in ngrams(words, 3)]\n",
    "        ngram_indices = [vocab.get(ngram) for ngram in ngram_list if ngram in vocab]\n",
    "        text_embeddings = [SVDEmbeddings[idx] for idx in ngram_indices if idx is not None]\n",
    "        if text_embeddings:\n",
    "            document_embedding = np.mean(text_embeddings, axis=0)\n",
    "        else:\n",
    "            document_embedding = np.zeros(svd_ppmi.n_components)\n",
    "        all_embeddings.append(document_embedding)\n",
    "    all_embeddings_a = np.array(all_embeddings)\n",
    "    return all_embeddings_a\n",
    "\n",
    "def roberta_embeddings(df):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    model = RobertaModel.from_pretrained('roberta-large')\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    embeddings = []\n",
    "    for text in df['text_processed']:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    return embeddings_array\n",
    "\n",
    "def sentence_transformer_embeddings(df):\n",
    "    model_name='roberta-base-nli-stsb-mean-tokens'\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(df['text_processed'].tolist(), show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Dimension reduction functions\n",
    "def tsne(embeddings):\n",
    "    docs_tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "                init='pca').fit_transform(embeddings)\n",
    "    return docs_tsne\n",
    "\n",
    "def create_contingency_table(df, cluster_labels, theme_labels):\n",
    "    df_temp = df.copy()\n",
    "    df_temp['Cluster'] = cluster_labels\n",
    "    df_temp['Theme'] = theme_labels\n",
    "    contingency_table = pd.crosstab(df_temp['Cluster'], df_temp['Theme'])\n",
    "    return contingency_table\n",
    "\n",
    "\n",
    "def perform_correspondence_analysis(contingency_table):\n",
    "    ca = prince.CA(n_components=2, n_iter=10, copy=True, check_input=True, engine='auto')\n",
    "    ca = ca.fit(contingency_table)\n",
    "    row_coords = ca.row_coordinates(contingency_table)\n",
    "    col_coords = ca.column_coordinates(contingency_table)\n",
    "    return ca, row_coords, col_coords\n",
    "\n",
    "def plot_ca_results(row_coords, col_coords, embedding_name, clustering_name):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "   \n",
    "    ax.scatter(row_coords[0], row_coords[1], c='blue', label='Clusters')\n",
    "    for i, txt in enumerate(row_coords.index):\n",
    "        ax.annotate(txt, (row_coords.iloc[i, 0], row_coords.iloc[i, 1]))\n",
    "\n",
    "\n",
    "    ax.scatter(col_coords[0], col_coords[1], c='red', label='Themes', marker='x')\n",
    "    for i, txt in enumerate(col_coords.index):\n",
    "        ax.annotate(txt, (col_coords.iloc[i, 0], col_coords.iloc[i, 1]))\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.set_title('Correspondence Analysis')\n",
    "    plt.savefig(f'T4_initial_clustering_{embedding_name}_{clustering_name}_CA.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_ca(embeddings, cluster_labels, df, embedding_name, clustering_name):\n",
    "    theme_labels = df['theme']\n",
    "    contingency_table = create_contingency_table(df, cluster_labels, theme_labels)\n",
    "    ca, row_coords, col_coords = perform_correspondence_analysis(contingency_table)\n",
    "    plot_ca_results(row_coords, col_coords, embedding_name, clustering_name)\n",
    "\n",
    "\n",
    "def pca(embeddings):\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_pca = pca.fit_transform(embeddings)\n",
    "    explained_variance = pca.explained_variance_ratio_ \n",
    "    print(f\"Explained variance: {explained_variance}\")\n",
    "    return embeddings_pca\n",
    "\n",
    "# Clusterings functions\n",
    "def Kmeans_clustering(n_clusters, embeddings, model_name):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(embeddings)\n",
    "    labels = kmeans.labels_\n",
    "    return labels\n",
    "\n",
    "def gaussian_clustering(n_clusters, embeddings, model_name):\n",
    "    if model_name == 'roberta_embeddings':\n",
    "        n_clusters = n_clusters - 3\n",
    "    if model_name == 'sentence_transformer_embeddings':\n",
    "        n_clusters = n_clusters\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=0, covariance_type='diag', reg_covar=1e-6)\n",
    "    gmm.fit(embeddings)\n",
    "    labels = gmm.predict(embeddings)\n",
    "    return labels\n",
    "\n",
    "def hierarchical_clustering(n_clusters, embeddings, model_name):\n",
    "    hc = AgglomerativeClustering(n_clusters=n_clusters, metric='euclidean', linkage='ward')\n",
    "    labels = hc.fit_predict(embeddings)\n",
    "    return labels\n",
    "\n",
    "def correspondence_analysis(embeddings, n_components=2):\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    embeddings_ca = svd.fit_transform(embeddings)\n",
    "    return embeddings_ca\n",
    "\n",
    "# Validation function\n",
    "def score_function(embeddings, labels):\n",
    "    silhouette_s = silhouette_score(embeddings, labels)\n",
    "    davies_bouldin_s = davies_bouldin_score(embeddings, labels)\n",
    "    calinski_harabasz_s = calinski_harabasz_score(embeddings, labels)\n",
    "    return silhouette_s, davies_bouldin_s, calinski_harabasz_s\n",
    "\n",
    "# def display_ca(embeddings, df, labels, embedding_name, clustering_name):\n",
    "#     embeddings_ca = correspondence_analysis(embeddings)\n",
    "#     data_ca = pd.DataFrame({'x': embeddings_ca[:, 0],\n",
    "#                             'y': embeddings_ca[:, 1],\n",
    "#                             'institution': df[labels],\n",
    "#                             'title': df[\"Name of the document\"],\n",
    "#                             'labels': df[labels]\n",
    "#                             })\n",
    "#     alt.data_transformers.disable_max_rows()\n",
    "#     chart = alt.Chart(data_ca).mark_circle(size=200).encode(\n",
    "#         x=\"x\", y=\"y\", color=alt.Color('labels:N', scale=alt.Scale(scheme='category20')),\n",
    "#         tooltip=['institution', \"title\"]\n",
    "#         ).interactive().properties(\n",
    "#         width=500,\n",
    "#         height=500\n",
    "#     )\n",
    "#     chart.save(f'T4_initial_clustering_{embedding_name}_{clustering_name}_CA.html')\n",
    "#     chart.show()\n",
    "    \n",
    "\n",
    "def display_pca(embeddings, df, labels, embedding_name, clustering_name):\n",
    "    embeddings_pca = pca(embeddings)\n",
    "    data_pca = pd.DataFrame({'x': embeddings_pca[:, 0],\n",
    "                            'y': embeddings_pca[:, 1],\n",
    "                            'institution': df[labels],\n",
    "                            'title': df[\"Name of the document\"],\n",
    "                            'labels': labels\n",
    "                            })\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "    chart = alt.Chart(data_pca).mark_circle(size=200).encode(\n",
    "        x=\"x\", y=\"y\", color=alt.Color('labels:N', scale=alt.Scale(scheme='category20')),\n",
    "        tooltip=['institution', \"title\"]\n",
    "        ).interactive().properties(\n",
    "        width=500,\n",
    "        height=500\n",
    "    )\n",
    "    chart.save(f'T5_initial_clustering_{embedding_name}_{clustering_name}_PCA.html')\n",
    "    chart.show()   \n",
    "\n",
    "def display_tsne(embeddings, df, labels, embedding_name, clustering_name):\n",
    "    docs_tsne_th = TSNE(n_components=2, learning_rate='auto',\n",
    "                        init='random', metric='cosine',\n",
    "                        perplexity=50.0).fit_transform(embeddings)\n",
    "    print(docs_tsne_th.shape)\n",
    "\n",
    "    data_th = pd.DataFrame({'x': docs_tsne_th[:,0],\n",
    "                            'y': docs_tsne_th[:,1],\n",
    "                            'institution': df[labels],\n",
    "                            'title': df[\"Name of the document\"],\n",
    "                            'labels': labels\n",
    "                            #'labels': df[\"categorie Institution\"]\n",
    "                            #'labels': df[\"theme\"]\n",
    "                            })\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "    chart = alt.Chart(data_th[:]).mark_circle(size=200).encode(\n",
    "        x=\"x\", y=\"y\", color=alt.Color('labels:N', \n",
    "                                    scale=alt.Scale(scheme='category20')),\n",
    "        tooltip=['institution', \"title\"]\n",
    "        ).interactive().properties(\n",
    "        width=500,\n",
    "        height=500\n",
    "    )\n",
    "    chart.save(f'T5_initial_clustering_{embedding_name}_{clustering_name}_TSNE.html')\n",
    "    chart.show()\n",
    "\n",
    "\n",
    "# Clustering pipeline\n",
    "def pipeline(dataframe, embedding_method, clustering_method, taille_cluster=[10,11], reduction_method=display_pca):\n",
    "    print(f\"start embedding for {embedding_method.__name__} and {clustering_method.__name__}\")\n",
    "    embeddings = embedding_method(dataframe)\n",
    "    print(\"clustering\")\n",
    "    for i in range(taille_cluster[0], taille_cluster[1]):\n",
    "        labels = clustering_method(i, embeddings, embedding_method.__name__)\n",
    "    print(\"scoring\")\n",
    "    scores = score_function(embeddings, labels)\n",
    "    print(f\"silhouette_score: {scores[0]}, davies_bouldin_score: {scores[1]}, calinski_harabasz_score: {scores[2]}\")\n",
    "    reduction_method(embeddings, dataframe, labels, embedding_method.__name__, clustering_method.__name__)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proprocessed = \"../Data_csv/data_preprocessed.csv\"\n",
    "data_df = pd.read_csv(data_proprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start embedding for SVD_embeddings_ngram and Kmeans_clustering\n",
      "clustering\n",
      "scoring\n",
      "silhouette_score: 0.5522523175572207, davies_bouldin_score: 0.43082652997123994, calinski_harabasz_score: 2789.108398469199\n",
      "(457, 2)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([1, 1, 1, 1, 1, 7, 7, 9, 8, 9,\\n       ...\\n       0, 9, 9, 8, 8, 8, 8, 0, 8, 1],\\n      dtype='int32', length=457)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_method \u001b[38;5;129;01min\u001b[39;00m Clustering_methods:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m reduction \u001b[38;5;129;01min\u001b[39;00m reduction_methods:\n\u001b[0;32m---> 11\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43membedding_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mclustering_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreduction_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmbedding Method\u001b[39m\u001b[38;5;124m'\u001b[39m: embedding_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClustering Method\u001b[39m\u001b[38;5;124m'\u001b[39m: cluster_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \n\u001b[1;32m     25\u001b[0m         })\n",
      "Cell \u001b[0;32mIn[19], line 420\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(dataframe, embedding_method, clustering_method, taille_cluster, reduction_method)\u001b[0m\n\u001b[1;32m    418\u001b[0m scores \u001b[38;5;241m=\u001b[39m score_function(embeddings, labels)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilhouette_score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, davies_bouldin_score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, calinski_harabasz_score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 420\u001b[0m \u001b[43mreduction_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclustering_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "Cell \u001b[0;32mIn[19], line 391\u001b[0m, in \u001b[0;36mdisplay_tsne\u001b[0;34m(embeddings, df, labels, embedding_name, clustering_name)\u001b[0m\n\u001b[1;32m    384\u001b[0m docs_tsne_th \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    385\u001b[0m                     init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    386\u001b[0m                     perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50.0\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(embeddings)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs_tsne_th\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    389\u001b[0m data_th \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: docs_tsne_th[:,\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    390\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: docs_tsne_th[:,\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m--> 391\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstitution\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    392\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of the document\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    393\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: labels\n\u001b[1;32m    394\u001b[0m                         \u001b[38;5;66;03m#'labels': df[\"categorie Institution\"]\u001b[39;00m\n\u001b[1;32m    395\u001b[0m                         \u001b[38;5;66;03m#'labels': df[\"theme\"]\u001b[39;00m\n\u001b[1;32m    396\u001b[0m                         })\n\u001b[1;32m    397\u001b[0m alt\u001b[38;5;241m.\u001b[39mdata_transformers\u001b[38;5;241m.\u001b[39mdisable_max_rows()\n\u001b[1;32m    398\u001b[0m chart \u001b[38;5;241m=\u001b[39m alt\u001b[38;5;241m.\u001b[39mChart(data_th[:])\u001b[38;5;241m.\u001b[39mmark_circle(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m    399\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39malt\u001b[38;5;241m.\u001b[39mColor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels:N\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    400\u001b[0m                                 scale\u001b[38;5;241m=\u001b[39malt\u001b[38;5;241m.\u001b[39mScale(scheme\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory20\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m     height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m    405\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/MS_IA/MS_IA/Projet_NLP/env_NLP/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/MS_IA/MS_IA/Projet_NLP/env_NLP/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MS_IA/MS_IA/Projet_NLP/env_NLP/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([1, 1, 1, 1, 1, 7, 7, 9, 8, 9,\\n       ...\\n       0, 9, 9, 8, 8, 8, 8, 0, 8, 1],\\n      dtype='int32', length=457)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "Clustering_methods = [Kmeans_clustering]\n",
    "Embedding_methods = [SVD_embeddings_ngram]\n",
    "reduction_methods = [display_tsne]\n",
    "\n",
    "results = []\n",
    "\n",
    "for embedding_method in Embedding_methods:\n",
    "    for cluster_method in Clustering_methods:\n",
    "        for reduction in reduction_methods:\n",
    "            \n",
    "            result = pipeline(dataframe=data_df, \n",
    "                            embedding_method=embedding_method,\n",
    "                            clustering_method=cluster_method,\n",
    "                            reduction_method=reduction\n",
    "                            )\n",
    "            \n",
    "            results.append({\n",
    "                'Embedding Method': embedding_method.__name__,\n",
    "                'Clustering Method': cluster_method.__name__,\n",
    "                'Reduction Method': reduction.__name__,\n",
    "                'silhoutte score': result[0],  \n",
    "                'davies score' : result[1], \n",
    "                'calinski score' : result[2],\n",
    "                    \n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustering_methods = [Kmeans_clustering]\n",
    "Embedding_methods = [SVD_embeddings_ngram]\n",
    "reduction_methods = [display_tsne]\n",
    "taille_cluster=[10,11]\n",
    "\n",
    "embeddings = SVD_embeddings_ngram(data_df)\n",
    "for i in range(taille_cluster[0], taille_cluster[1]):\n",
    "        labels = Kmeans_clustering(i, embeddings, SVD_embeddings_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 7 7 9 8 9 7 8 9 7 2 9 9 2 5 5 0 8 0 2 5 9 2 8 0 9 8 6 6 6 0 0 0\n",
      " 9 4 5 8 0 0 0 0 1 9 0 8 2 9 4 8 8 8 8 8 9 2 8 8 9 8 9 9 7 1 1 4 7 8 1 5 9\n",
      " 5 8 9 1 4 1 1 8 2 9 8 9 9 9 8 5 5 2 5 9 2 8 5 5 9 9 5 8 6 8 1 5 8 2 2 9 2\n",
      " 0 8 9 8 2 7 9 2 0 0 4 6 4 1 6 0 8 8 2 1 9 0 2 0 8 8 2 0 1 2 5 5 7 2 0 2 2\n",
      " 9 1 8 8 9 1 2 2 9 1 0 2 0 8 5 1 8 2 2 5 1 9 2 2 2 9 8 9 1 5 8 9 5 1 2 5 2\n",
      " 2 2 1 2 2 2 2 8 2 8 8 2 2 2 8 1 1 5 8 0 6 6 9 7 1 9 9 2 2 5 8 9 9 8 5 6 8\n",
      " 2 8 5 0 2 8 9 8 8 2 5 2 7 4 9 6 0 0 0 0 9 6 9 2 9 2 1 2 0 9 9 1 8 2 7 5 2\n",
      " 9 1 5 8 0 1 2 9 1 8 4 4 5 2 0 5 8 8 0 7 1 8 1 6 6 8 0 8 8 9 1 2 8 2 6 8 6\n",
      " 8 5 1 4 5 1 2 2 0 0 0 0 9 5 9 9 5 5 5 2 2 7 5 8 9 8 0 6 0 5 2 0 8 8 8 8 2\n",
      " 2 2 6 2 7 7 2 5 2 2 0 0 6 9 8 9 2 8 5 0 4 3 5 5 5 5 9 2 2 8 0 2 9 0 5 0 5\n",
      " 2 1 2 4 8 5 9 6 1 6 8 2 8 1 9 5 9 7 7 5 9 5 7 9 9 7 7 6 0 8 1 0 6 9 9 6 0\n",
      " 8 2 5 7 0 9 7 6 8 0 1 9 7 6 6 4 4 0 8 1 9 9 6 2 9 0 9 9 9 2 9 1 1 2 6 7 0\n",
      " 0 8 2 0 9 9 8 8 8 8 0 8 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
