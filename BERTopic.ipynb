{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import os.path as op\n",
    "import re \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the preprocess doc\n",
    "\n",
    "df = pd.read_csv('./Data_csv/data_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['doc_id', 'Name of the document', 'Institution', 'URL', 'Authors',\n",
       "       'Affiliates', 'Sector', 'Country', 'Date', 'Keywords',\n",
       "       'Exclusion criteria', 'Status', 'Label', 'MapAIE (ours)', 'Jobin',\n",
       "       'Fjeld', 'Tidjon', 'Hagendorff', 'Floridi', 'Zeng (LAIP)',\n",
       "       'Attard-Frost', 'EP', 'Algorithm watch', 'CE', 'Winfield',\n",
       "       'EthicalML GitHub', 'all sources', 'Checked by', 'Unnamed: 27',\n",
       "       'Unnamed: 28', 'Unnamed: 29', 'text', 'langue', 'text_processed',\n",
       "       'tfidf', 'categorie Institution', 'theme'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_id                   457\n",
       "Name of the document     457\n",
       "Institution              457\n",
       "URL                      457\n",
       "Authors                   33\n",
       "Affiliates                29\n",
       "Sector                    42\n",
       "Country                  446\n",
       "Date                      44\n",
       "Keywords                   6\n",
       "Exclusion criteria         0\n",
       "Status                   457\n",
       "Label                    394\n",
       "MapAIE (ours)            457\n",
       "Jobin                     59\n",
       "Fjeld                     21\n",
       "Tidjon                    24\n",
       "Hagendorff                12\n",
       "Floridi                    4\n",
       "Zeng (LAIP)               42\n",
       "Attard-Frost              28\n",
       "EP                         8\n",
       "Algorithm watch          103\n",
       "CE                       377\n",
       "Winfield                  15\n",
       "EthicalML GitHub          13\n",
       "all sources              457\n",
       "Checked by               457\n",
       "Unnamed: 27                2\n",
       "Unnamed: 28                0\n",
       "Unnamed: 29                2\n",
       "text                     457\n",
       "langue                   457\n",
       "text_processed           457\n",
       "tfidf                    457\n",
       "categorie Institution    457\n",
       "theme                    457\n",
       "dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['text_processed'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "Bow = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Calculer la fréquence totale de chaque mot\n",
    "frequency = Bow.toarray().sum(axis=0)\n",
    "\n",
    "# Obtenir les 20 mots les plus fréquents\n",
    "top_indices = np.argsort(frequency)[-20:]  # Indices des 20 mots les plus fréquents\n",
    "vocabulary = np.array(vectorizer.get_feature_names_out())  # Le vocabulaire\n",
    "top_words = vocabulary[top_indices] \n",
    "\n",
    "stopwords_to_remove = set(top_words)\n",
    "\n",
    "dates = set([str(i) for i in range(1990, 2021)])\n",
    "useless_words = set(('may','also','see'))\n",
    "html_stopwords = set({'maxwidth', 'px', 'maxheight', 'img', 'src', 'https', 'com', 'www', 'http', 'jpg', 'png', 'gif', 'jpeg', 'pdf', 'html'})\n",
    "css_stopwords = set({'grid','body', ':root', '\\n', 'padding',',button','margintop', 'px', 'margin', 'border', 'width', 'height', 'color', 'font', 'size', 'background', 'position', 'left', 'right', 'top', 'bottom', 'display', 'flex', 'align', 'justify', 'content', 'center', 'float', 'clear', 'overflow', 'hidden', 'zindex', 'cursor', 'pointer', 'hover', 'active', 'focus', 'transition', 'transform', 'rotate', 'scale', 'translate', 'opacity', 'box', 'shadow', 'outline', 'none', 'block', 'inline', 'inlineblock', 'relative', 'absolute', 'fixed', 'static', 'sticky', 'visible', 'hidden'})\n",
    "\n",
    "stopwords_to_remove = stopwords_to_remove.union(dates)\n",
    "stopwords_to_remove = stopwords_to_remove.union(html_stopwords)\n",
    "stopwords_to_remove = stopwords_to_remove.union(css_stopwords)\n",
    "stopwords_to_remove = stopwords_to_remove.union(useless_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_frequent_words(doc, stopwords):\n",
    "    words = doc.split()\n",
    "    cleaned_words = [word for word in words if word not in stopwords]\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "\n",
    "documents = [remove_frequent_words(doc, stopwords_to_remove) for doc in documents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=1, cluster_selection_epsilon=0.01)\n",
    "topic_model = BERTopic(hdbscan_model=hdbscan_model, language=\"english\", nr_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 0\n",
    "while n_topics < 10:\n",
    "    topics, probs = topic_model.fit_transform(documents)\n",
    "    n_topics = len(set(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'] = topics\n",
    "df['probs'] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                                           Name  \\\n",
      "0     -1     99         -1_also_government_technologies_report   \n",
      "1      0     34                   0_system_also_may_protection   \n",
      "2      1     23            1_uk_government_strategy_innovation   \n",
      "3      2     97                2_ethics_ethical_machine_system   \n",
      "4      3     20             3_biometric_facial_law_recognition   \n",
      "5      4     35  4_cooperation_international_government_global   \n",
      "6      5     31   5_algorithmic_algorithms_bias_decisionmaking   \n",
      "7      6     35             6_model_protection_project_privacy   \n",
      "8      7     41                     7_law_legal_europe_council   \n",
      "9      8     42                  8_energy_policy_national_oecd   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [also, government, technologies, report, may, ...   \n",
      "1  [system, also, may, protection, personal, stan...   \n",
      "2  [uk, government, strategy, innovation, health,...   \n",
      "3  [ethics, ethical, machine, system, may, also, ...   \n",
      "4  [biometric, facial, law, recognition, system, ...   \n",
      "5  [cooperation, international, government, globa...   \n",
      "6  [algorithmic, algorithms, bias, decisionmaking...   \n",
      "7  [model, protection, project, privacy, system, ...   \n",
      "8  [law, legal, europe, council, also, system, pr...   \n",
      "9  [energy, policy, national, oecd, also, luxembo...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [october national strategic plan national scie...  \n",
      "1  [privacy report january 201 8 2 3 contents rep...  \n",
      "2  [roadmapuk council council independent expert ...  \n",
      "3  [machine power promise computers learn example...  \n",
      "4  [study panel future science eprs parliamentary...  \n",
      "5  [age interdependence 1 age interdependence rep...  \n",
      "6  [automating society aking stock automated deci...  \n",
      "7  [responsible policy project formulation manual...  \n",
      "8  [democracy rule law david leslie christopher b...  \n",
      "9  [independent high-level expert group set commi...  \n"
     ]
    }
   ],
   "source": [
    "# Afficher les topics les plus fréquents\n",
    "print(topic_model.get_topic_info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('also', 0.012075053366697045), ('government', 0.011012281133032223), ('technologies', 0.01053147377932307), ('report', 0.010365084364266171), ('may', 0.010207114452158206), ('social', 0.009950220350407444), ('algorithms', 0.009826026538115519), ('work', 0.009598106524077629), ('services', 0.009059133029213729), ('could', 0.008648757892195335)]\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.get_topic(-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           0,
           "system | also | may | protection | personal",
           34
          ],
          [
           1,
           "uk | government | strategy | innovation | health",
           23
          ],
          [
           2,
           "ethics | ethical | machine | system | may",
           97
          ],
          [
           3,
           "biometric | facial | law | recognition | system",
           20
          ],
          [
           4,
           "cooperation | international | government | global | education",
           35
          ],
          [
           5,
           "algorithmic | algorithms | bias | decisionmaking | see",
           31
          ],
          [
           6,
           "model | protection | project | privacy | system",
           35
          ],
          [
           7,
           "law | legal | europe | council | also",
           41
          ],
          [
           8,
           "energy | policy | national | oecd | also",
           42
          ]
         ],
         "hovertemplate": "<b>Topic %{customdata[0]}</b><br>%{customdata[1]}<br>Size: %{customdata[2]}",
         "legendgroup": "",
         "marker": {
          "color": "#B0BEC5",
          "line": {
           "color": "DarkSlateGrey",
           "width": 2
          },
          "size": [
           34,
           23,
           97,
           20,
           35,
           31,
           35,
           41,
           42
          ],
          "sizemode": "area",
          "sizeref": 0.060625,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -3.571624279022217,
          14.224100112915039,
          14.950775146484375,
          14.489611625671387,
          -4.3004984855651855,
          13.713029861450195,
          14.55005168914795,
          -3.4330968856811523,
          -3.773045063018799
         ],
         "xaxis": "x",
         "y": [
          -5.484594345092773,
          19.74711036682129,
          19.439708709716797,
          18.81562614440918,
          -6.4237518310546875,
          19.055709838867188,
          19.236602783203125,
          -6.087552070617676,
          -6.589860439300537
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "D1",
          "x": -4.945573258399963,
          "y": 7.565418708324431,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "D2",
          "x": 6.123909080028534,
          "xshift": 10,
          "y": 22.70917692184448
         }
        ],
        "height": 650,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "legend": {
         "itemsizing": "constant",
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "#CFD8DC",
           "width": 2
          },
          "type": "line",
          "x0": 6.123909080028534,
          "x1": 6.123909080028534,
          "y0": -7.5783395051956175,
          "y1": 22.70917692184448
         },
         {
          "line": {
           "color": "#9E9E9E",
           "width": 2
          },
          "type": "line",
          "x0": -4.945573258399963,
          "x1": 17.193391418457033,
          "y0": 7.565418708324431,
          "y1": 7.565418708324431
         }
        ],
        "sliders": [
         {
          "active": 0,
          "pad": {
           "t": 50
          },
          "steps": [
           {
            "args": [
             {
              "marker.color": [
               [
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 0",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 1",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 2",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 3",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 4",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 5",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 6",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 7",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red"
               ]
              ]
             }
            ],
            "label": "Topic 8",
            "method": "update"
           }
          ]
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Intertopic Distance Map</b>",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 650,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "range": [
          -4.945573258399963,
          17.193391418457033
         ],
         "title": {
          "text": ""
         },
         "visible": false
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          -7.5783395051956175,
          22.70917692184448
         ],
         "title": {
          "text": ""
         },
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation des topics\n",
    "topic_model.visualize_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0,
          0.11955052876957528,
          0.11955052876957528,
          0
         ],
         "xaxis": "x",
         "y": [
          -15,
          -15,
          -25,
          -25
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0,
          0.2113159115069398,
          0.2113159115069398,
          0.11955052876957528
         ],
         "xaxis": "x",
         "y": [
          -5,
          -5,
          -20,
          -20
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0.2113159115069398,
          0.22646729957049114,
          0.22646729957049114,
          0
         ],
         "xaxis": "x",
         "y": [
          -12.5,
          -12.5,
          -35,
          -35
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0,
          0.22501420345573198,
          0.22501420345573198,
          0
         ],
         "xaxis": "x",
         "y": [
          -45,
          -45,
          -55,
          -55
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0.22646729957049114,
          0.2619884279570702,
          0.2619884279570702,
          0.22501420345573198
         ],
         "xaxis": "x",
         "y": [
          -23.75,
          -23.75,
          -50,
          -50
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0,
          0.17309903867867005,
          0.17309903867867005,
          0
         ],
         "xaxis": "x",
         "y": [
          -65,
          -65,
          -75,
          -75
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0.17309903867867005,
          0.3117009556961368,
          0.3117009556961368,
          0
         ],
         "xaxis": "x",
         "y": [
          -70,
          -70,
          -85,
          -85
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0.2619884279570702,
          0.5887708706117566,
          0.5887708706117566,
          0.3117009556961368
         ],
         "xaxis": "x",
         "y": [
          -36.875,
          -36.875,
          -77.5,
          -77.5
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "autosize": false,
        "height": 335,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "hovermode": "closest",
        "plot_bgcolor": "#ECEFF1",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Hierarchical Clustering</b>",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "mirror": "allticks",
         "rangemode": "tozero",
         "showgrid": false,
         "showline": true,
         "showticklabels": true,
         "ticks": "outside",
         "type": "linear",
         "zeroline": false
        },
        "yaxis": {
         "mirror": "allticks",
         "range": [
          -90,
          0
         ],
         "rangemode": "tozero",
         "showgrid": false,
         "showline": true,
         "showticklabels": true,
         "tickmode": "array",
         "ticks": "outside",
         "ticktext": [
          "6_model_protection_project",
          "0_system_also_may",
          "7_law_legal_europe",
          "3_biometric_facial_law",
          "5_algorithmic_algorithms_bias",
          "2_ethics_ethical_machine",
          "8_energy_policy_national",
          "4_cooperation_international...",
          "1_uk_government_strategy"
         ],
         "tickvals": [
          -5,
          -15,
          -25,
          -35,
          -45,
          -55,
          -65,
          -75,
          -85
         ],
         "type": "linear",
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Visualisation des hiérarchies entre les topics\n",
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.012195469219743075,
          0.014162396829092205,
          0.01620261742689139,
          0.016508357329856865,
          0.017777212674527955
         ],
         "xaxis": "x",
         "y": [
          "personal  ",
          "protection  ",
          "may  ",
          "also  ",
          "system  "
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#0072B2"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.018826655748931106,
          0.01892077969931654,
          0.022094502861218855,
          0.02407634737526727,
          0.04463952152478731
         ],
         "xaxis": "x2",
         "y": [
          "health  ",
          "innovation  ",
          "strategy  ",
          "government  ",
          "uk  "
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": "#CC79A7"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.014036110163735037,
          0.01553251356382719,
          0.017154015916053336,
          0.018892049450077344,
          0.02401217265664033
         ],
         "xaxis": "x3",
         "y": [
          "may  ",
          "system  ",
          "machine  ",
          "ethical  ",
          "ethics  "
         ],
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": "#E69F00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.014593948923980046,
          0.01470619663375795,
          0.016275241999019065,
          0.01876054810308927,
          0.019660971715448727
         ],
         "xaxis": "x4",
         "y": [
          "education  ",
          "global  ",
          "government  ",
          "international  ",
          "cooperation  "
         ],
         "yaxis": "y4"
        },
        {
         "marker": {
          "color": "#56B4E9"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.01533871716750507,
          0.017819624806936035,
          0.019265647889241757,
          0.019367546835080733,
          0.02029358568688808
         ],
         "xaxis": "x5",
         "y": [
          "see  ",
          "decisionmaking  ",
          "bias  ",
          "algorithms  ",
          "algorithmic  "
         ],
         "yaxis": "y5"
        },
        {
         "marker": {
          "color": "#009E73"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.015272753401590834,
          0.015384035567919773,
          0.01614478779414148,
          0.016698873437244587,
          0.029348423122089864
         ],
         "xaxis": "x6",
         "y": [
          "system  ",
          "privacy  ",
          "project  ",
          "protection  ",
          "model  "
         ],
         "yaxis": "y6"
        },
        {
         "marker": {
          "color": "#F0E442"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.013968083974556279,
          0.014122064128780566,
          0.014209876293190177,
          0.017069243976522742,
          0.02060344527214566
         ],
         "xaxis": "x7",
         "y": [
          "also  ",
          "council  ",
          "europe  ",
          "legal  ",
          "law  "
         ],
         "yaxis": "y7"
        },
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.012718339735727946,
          0.012892906862663712,
          0.013455891853291772,
          0.013890731866558249,
          0.01685311077434304
         ],
         "xaxis": "x8",
         "y": [
          "also  ",
          "oecd  ",
          "national  ",
          "policy  ",
          "energy  "
         ],
         "yaxis": "y8"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 0",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 1",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 2",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 4",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 5",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 6",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 7",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 8",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 500,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "Topic Word Scores",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis7": {
         "anchor": "y7",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Visualisation des relations entre les topics\n",
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>Similarity Score: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "0_system_also_may",
          "1_uk_government_strategy",
          "2_ethics_ethical_machine",
          "3_biometric_facial_law",
          "6_model_protection_project",
          "4_cooperation_international...",
          "8_energy_policy_national",
          "5_algorithmic_algorithms_bias",
          "7_law_legal_europe"
         ],
         "xaxis": "x",
         "y": [
          "0_system_also_may",
          "1_uk_government_strategy",
          "2_ethics_ethical_machine",
          "3_biometric_facial_law",
          "6_model_protection_project",
          "4_cooperation_international...",
          "8_energy_policy_national",
          "5_algorithmic_algorithms_bias",
          "7_law_legal_europe"
         ],
         "yaxis": "y",
         "z": [
          [
           0.9999997615814209,
           0.4954816401004791,
           0.5496042966842651,
           0.6416627168655396,
           0.6483479738235474,
           0.5175851583480835,
           0.5718090534210205,
           0.6206514239311218,
           0.6649094820022583
          ],
          [
           0.4954816401004791,
           0.9999998807907104,
           0.7176145315170288,
           0.7495504021644592,
           0.7945390939712524,
           0.7553051710128784,
           0.6998888850212097,
           0.6760573983192444,
           0.7153728604316711
          ],
          [
           0.5496042966842651,
           0.7176145315170288,
           1.0000001192092896,
           0.783993661403656,
           0.8403233289718628,
           0.7012712359428406,
           0.6486106514930725,
           0.8236281871795654,
           0.7018183469772339
          ],
          [
           0.6416627168655396,
           0.7495504021644592,
           0.783993661403656,
           1,
           0.8935489654541016,
           0.820594072341919,
           0.8237396478652954,
           0.8092707395553589,
           0.8453941345214844
          ],
          [
           0.6483479738235474,
           0.7945390939712524,
           0.8403233289718628,
           0.8935489654541016,
           0.9999997615814209,
           0.8152601718902588,
           0.7763234376907349,
           0.863854706287384,
           0.8522824048995972
          ],
          [
           0.5175851583480835,
           0.7553051710128784,
           0.7012712359428406,
           0.820594072341919,
           0.8152601718902588,
           1,
           0.8514046669006348,
           0.6548728346824646,
           0.773518443107605
          ],
          [
           0.5718090534210205,
           0.6998888850212097,
           0.6486106514930725,
           0.8237396478652954,
           0.7763234376907349,
           0.8514046669006348,
           0.9999998211860657,
           0.6635326147079468,
           0.8531901836395264
          ],
          [
           0.6206514239311218,
           0.6760573983192444,
           0.8236281871795654,
           0.8092707395553589,
           0.863854706287384,
           0.6548728346824646,
           0.6635326147079468,
           1,
           0.7525414824485779
          ],
          [
           0.6649094820022583,
           0.7153728604316711,
           0.7018183469772339,
           0.8453941345214844,
           0.8522824048995972,
           0.773518443107605,
           0.8531901836395264,
           0.7525414824485779,
           1.0000001192092896
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "Similarity Score"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(247,252,240)"
          ],
          [
           0.125,
           "rgb(224,243,219)"
          ],
          [
           0.25,
           "rgb(204,235,197)"
          ],
          [
           0.375,
           "rgb(168,221,181)"
          ],
          [
           0.5,
           "rgb(123,204,196)"
          ],
          [
           0.625,
           "rgb(78,179,211)"
          ],
          [
           0.75,
           "rgb(43,140,190)"
          ],
          [
           0.875,
           "rgb(8,104,172)"
          ],
          [
           1,
           "rgb(8,64,129)"
          ]
         ]
        },
        "height": 800,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "legend": {
         "title": {
          "text": "Trend"
         }
        },
        "margin": {
         "t": 60
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Similarity Matrix</b>",
         "x": 0.55,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_heatmap(n_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"privacy report january 201 8 2 3 contents report ................................ ................................ ................................ ................................ ................. 4 legal sources terminology ................................ ................................ ................................ ............... 4 intelligen ce protecti ................................ ................................ ................................ ..... 5 ntelligence work ................................ ................................ ................................ ............ 7 machine ................................ ................................ ................................ ................................ ................. 7 results ................................ ................................ ................................ ................................ ............. 10 training better ................................ ................................ ................................ ................... 11 black ................................ ................................ ................................ ................................ .................... 12 intelligen ce meets gdpr ................................ ................................ ................................ .......... 15 fundamental principles protection ................................ ................................ ................................ ..... 15 algorithmic bias meets fairness principle ................................ ................................ ................................ 16 meets principle purpose limitation ................................ ................................ .... 16 meets minimisation ................................ ................................ ............................... 18 black meets principle transparen processing ................................ ................................ ......... 19 controlling algorithms ................................ ................................ ................................ ............................ 23 dpa ’ supervisory c ompetence ................................ ................................ ................................ ................ 23 investigating ................................ ................................ ................................ ............................... 23 deep investigation go ................................ ................................ ................................ ................. 23 inspect “ black ” ................................ ................................ ................................ ......................... 24 solutions recomme ndations ................................ ................................ ................................ ...................... 25 assess protection impact – build privacy system ................................ .................... 25 tools methods good protection ................................ ................................ ......................... 26 recommendations privacy friendly developm ent ................................ ............................. 28 4 report applications require huge volumes order learn make intelligent decisions high agenda sectors due potential radically improved services commercial break throughs financial gains future face range legal ethical dilemmas search balance considerable social advances name fundamental privacy report aims desc ribe help us understand privacy affected dev elopment application norwegian protection authority dpa believes imperative knowledge privacy implications discuss order safeguard privacy individual also meet requirements society large people trust handled properly may limit willingness share – example heir doctor social media find situation sections population refuse share feel personal integrity violated fac ed major challenges freedom speech people ’ trust authorities refusal share personal also represent considerable challenge regard commercial sectors media retail trade finance services repo rt elaborates legal opinions technologies described report « big – protection principles pressure » 1 report provide greater technical detail describing also ta king closer look four relevant challenges associated protection principles embodied gdpr \\uf0b7 fairness discrimination \\uf0b7 purpose limitation \\uf0b7 minimisation \\uf0b7 transparency list exhaustive represents selection protection concerns opinion relevance today addition 1 //www.datatilsynet.no/om -personvern/rapporter -ogutredninger/temarapporter/big -data/ 2 gdpr text //eur -lex.europa.eu/legal content/en/txt/pdf/ uri=oj l:2016:119 full report considers role dpa supervisory applications finally provide numb er examples methods tools recommendations safeguarding privacy target group report consists people work reasons interested ho pe engineers social scientists lawyers specialists find report useful producing report process staff norwegian dpa learned lot experiences appraisals protection stakeholders touch process grateful inmeta privacy international financial supervisory authority norway google sintef norwegian university science echnology ntnu big insight university oslo norwegian computing sparebank 1 stavanger commissioner ’ office n uk office privacy commissioner canada office auditor general norwa centre university agder legal sources terminology report collective term describing various aspects including machine deep l earning basis report 's general protection regulation gdpr regulation enshrined norwegian law form personal act come force may 25 2018.2 also drawn upon recitals regulation interpreting contents articles recitals legally binding explains articles furthermore also cited statements made article 29 working party guidelines set individually automated decisions profiling.3 article 29 working party commission ’ senior advisory protection security matters 3 //ec.europa.eu/newsroom/just/item -detail.cfm item_i d=50083 5 prote ction concept describe computer able learn experiences solve complex problems different situations – abilities previously thought uniqu e mankind many cases personal fuels enabling learn become intelligent made major advances recent years potential appears promising better ore efficient sector methods climate environmental protection safer society perhaps even cure cancer words embarking venture without doubt considerable impact society accordingly important us engage discussion sort regulatory framework need order grasp opportunities offered assured manner escape fact raises number concerns w ith respect ethics security legal responsibility etc report devoted one concern personal issue privacy winter spring – concept known far back 1950s people high hopes success initial progress made however followed many decades often called winter early expectations met recent years though witnessed coming spring today see solve specific tasks example image speech recognition often called specialised general refers versatile humans comes p roblem solving probably several decades achieved spring dawned thanks availability huge amounts coupled increase 4 //ico.org.uk/for -organisations/guide -to-data -protection/big -data/ processing power access cheaper greater storage capacity b ig often refers vast volumes extracted ultiple sources often real time.4 enormous streams utilised benefit society means analysi finding patterns connections make difference traditional analytical methods need programmed find connections links learns sees computer therefore respond continuously adjust analyses without interventi thus helps remove technical barriers traditional methods run analysing big greater demand stringent regulations protection regulation enters force may strengthen privacy intensifying requirements made processing organisations bear responsibility processing person al accordance regulation transparency requirements stringent ti requirements intensified demand growing ai-based system become intelligent enough relevant learn intelligent chatbot computer program people interact means ordinary speech written input analyse informa tion fed – combina tion questions posed customers responses communicated customer service analys chatbot “ understand ” customer asking therefore able give meaningful answer greater volume chatbot base analys better precise reply gives 6 intelligen ce machine deep intelligen ce machine deep terms often synonym even though conceptually imprecise llustra tion depicts relation ship terms time umbrella term embraces man different types machine chine described “ set techniques tools allow computers ‘ think ’ creating mathemati cal algor ithms base accumul ated ” .5 ystem reason independently input build algorit hms deep form machine types deep build principles brain ’ neural net work type often based known set training helps selflearning algorit hms carry task conditional network able de termine correct respons e solving task .6 ethod crucial enabling alphago computer program defeat one world ’ best players chines e board game go see fact considered important milestone continuing possible c ombine appropriate protection compiling report spoken number developers users impression sectors adopted relatively 5 //iq.intel.com/artificial -intelligence -and-machine -learning/ restrictive manner techniques frequently limited co rresponds fairly well limited case portfolio protection authority requests guidance received regard privacy still early phase time ensure technologie comply rules society lays ans wer question w hether possible protect people ’ yes possible necessary order safeguard fundamental personal protection 6 //no.wikipedia.org/wiki/nevralt_nettverk //en.wikipedia.org/wiki/deep_learning alphago alphago computer program defeated one wor ld ’ best players chinese board game go go game many possible combinations currently impossible calculate needed therefore mo intelligent approach game basic calculat ing capacity could offer alphago developed deepmind deep experts could pply part p rogram program developed reviewing historical drawn many games played humans program played learn moves strategies produced best result s. one interesting results apart fact alphago program adopted strategie previousl unknown published go players kilde //www.blog.google/topics/machine learning/alphago -machine -learning -game -go/ 7 work two main aspects particular relevance privacy first software make decisions second system develops experience order computer system learn needs experience obtains experience feed input may several different formats system sought perform image recognition analysis experiential input naturally enough consist images tasks input consist text speech numbers utilise personal linked individuals machine order understand needs huge volumes necessary understand system learns developing requires input experiential machine generally proceeds way illustra ted figure 1 1. starts selected containing patterns similarities 2. using machine patterns found identified 3. model generated recognise patterns emerge fresh processed model model umbrella term final outcome many different types models commercial applications — predicting type streamed tv se ries consumer prefers models common contain essential training model process future seldom completely identical training generalisation required certain deviate main bulk training therefore usually removed model model works illustrated figure 1 1. model receives similar 2. model decid es pattern resembles 3. model produces estimated result 8 several forms depending whether labelled labelled tagged consists images labels tags may example gender ethnicity dog cat listed main forms describe supervised supervised involves labelled means supervision performed dataset split two usually 80/20 split 80 per cent train model remaining 20 per cent verify precisely model processes unknown good model performs accurately using training inaccurately using unknown model well adjusted training call overfitting produce satisfactory results using therefore model requires certain degree generalisation training may example consist images labelled contents image supervised may compa red teaching child example point number objects child give names show number cats child child gradually learn recognise cats originally shown similar fashion machin e model develop ability recognise objects based labelled images one working dataset wishes separate men women one different features relevance features depend basic available example women live longer men average life duration relevance differentiating genders feature however prove somewhat narrow cases mentioned example one ’ basis consists images hair length make -up jewellery may relevant features example illustrates two different features takes place follows illustrated figure 2 1. set labelled 2. depending type considered relevant features circles triangles selected labelled denote answer 3. model built based features produce label 9 often also know features labelled decisive correct categorisation producing result important persons w ith sound knowledge field question order identify relevant features correct selection relevant features may much importance amount issue addressing later one advantage labelled enables easy check model ’ precision model following takes place fig 2 1. type training fed system 2. relevant features fed model nd processed 3. model produces result corresponds labels training unsupervised unsupervised pre-labelled aim system group similar th e sake simplicity consider consisting cat dog images goal would greatest extent possible sorted two groups – one consisting images dogs cat images proceeds follows fig.3 1. dataset must certain number similarities patterns meaningful 2. patterns revealed 3. model built recognise differentiate patterns takes place using model fig 3 1. unlabelled type training fed system 2. model identifies patterns 3. model tells group belongs disadvantage method model place groups discovered process therefore important training basis representative reinforcement 10 form based trial error well optimisation model learns actions targeted towards goal means less needed system learn results regardless algorithms methods machine result “ model ” fact umbrella term machine model fed produce desired type result may example labelling degree probability similar worth noting model normally hold source directly holds aggregate representation train system 7 //www.telegraph.co.uk/technology/2016/03/24/microsofts -teen -girlai-turns -into-a-hitler -loving -sex-robot -wit/ decision trees represent one exception contain varying degree model ’ basis limits depend whether tree “ pruned ” level limitation set one normally chosen model generalise overfit deep -learning model basic da ta represented numerical values neural network therefore possible retrieve personal train model shall take closer look models little later section entitled black model – dynamic offline/online model may two ways first way offline model change model always name suggests operate way produce results throughout entire lifecycle model training take place test environment changes require model replaced version means full control maintained model possibility provided dynamic online model model similar fashion model however difference dynamic model able avail input order improve adjust changes may example necessary connection monitoring credit card transactions order reveal fraud transactions may change according user ’ life situation relation job example taking place comple tely locations usage patterns could well labelled suspicious model potentially result blocked credit card model therefore become less accurate time continuously updated spam filter provide good example typical area application dynamic model improved user indicating emails wrongly labelled disadvantage dynamic models less control model ’ changes immediate effect good example microsoft chatbot tay learned conversations internet users brief period twitter chatbot described “ hitler -loving sex robot ” media microsoft decid ed remove tay 24 hours launched.7 alphago zero earlier w e mentioned alphago example machine alphago first trained us ing set consisting 30 000 games go order improve alphago ’ ability play go programmed play experiential basis could enlarged considerably trial error without nee ding obtain games also gave alphago pportunity discovering moves strategies original training set latest version – alphago zero – devised order start playing without using training programmed rules go fed inform ation previously played games learned play 40 days able beat pr evious alphago version 100-0. also interesting note zero version alphago requires much less computing power achieve result s. source //deepmind.com/blog/alphago -zero -scratch/ 11 training better training feed model better result typical mantra frequently heard connection machine instances computer require lot humans order learn hing currently sets limit machine compensated utilising considerable amounts – often greater would able manage worth noting quality training well fe atures many instances substantially important quantity training model important selection training representative task solved later huge volumes little help th ey cover fraction model subsequently working correct labelling extremely important conducting supervised incorrectly labelled obviously negative impact training outco saying goes garbage garbage breadth depth efficiency machine heavily influenced basic presented algorithms develop models also features one chooses like spreadsheet dataset machine may consist rows columns one person -related columns may well denote person ’ age gender address marital status weight nationality etc rows represent dividual persons consideration must given quantity personal needed order train desired models well relevance chosen purpose selecting relevant features often need persons expert relevant fields ’ always case basic tells whole story good selection important otherwise one risks ending many features specialists call “ curse dimensionality » put simply thi means excessive number features result matches lost amongst non -matching mean enormous volumes needed way compensation one disadvantage reducing scope feature selectio n one may lose possible matches patterns previously known thought partly necessary include persons domain knowledge project phase one also consider constitu tes good enough result worth mentioning deep somewhat exception respect selection adjusting features important methods example feature selection cond ucted via value weights neural network disadvantage makin g selections means one needs vastly greater volume training feature engineering important factor achieving good results dataset presented relevant correlations may concealed properly many instances great deal gained smart increasing amoun dates one example let us consider date 1.10.2017 tells us first day month tenth month year might well would useful could example us hospital undertook trial categorise risk complications patients suffering pneumonia result patients suffering asthma pneumonia categorised low -risk patients – doctors ’ great surprise though patients ran higher risk survival rate better model able detect apparently low risk result patients getting tter care intensive treatment illustrates risks inherent using without domain knowledge basic dataset always tell whole story kilde //royalsociety.org/~/media/policy/projects/machine learning/publications/machine -learning -report.pdf 12 convert show day week sunday case norway four quite distinct seasons might consider grouping months order represent better way month 10 could represented autumn autumn could represented numerical value 3 spring would 1 summer 2 winter 4. way could derive features item reduce number different values extracted multiple sources steps aken ensure format us example month denoted 1 day 10 formula 1.10.2017. normalisation features may also necessary order ensure certain features create imbalance training extrem e values adversely affect rest put simply 8 //www.coursera.org/learn/machine learning/lecture/kont7/learning -curves say important ensure everything similarly scaled features change 0.1 signifies much change 1000 another feature essential re-aligned enough enough difficult outset estimate amount need ed depend type machine employed number characteristics features selected quality basic also relevance degree accuracy model needs objective achieved per son job 75 per cent accurate good enough model goal 100 per cent accuracy substantial amount needed area application define reasonable using personal training one would pursue objective diagnosing fatal illnesses differently one would go profiling someone order target advertisements accurately possible stick minimisation principle woul natural commence restricted amount training monitor model ’ accuracy fed curve one tool assessing this.8 enable one see started limited set dat curve flattens ceases add training value black one concern relation machine one always know result produced features combinations features impor tant model often produce result without explanation question arises whether possible study model thus find arrived specific result mentioned specialists norwegian tax admi nistration built predictive model helps select tax returns scrutinised closely state following “ build model way ’ necessarily know gives tax payer high ranking err risk ranking result complex aggregation model. ” //www.ritchieng.com/machinelearning -learning -curve/ norwegian tax administration norwegian tax administration nta developed predictive tool help select tax returns check errors tax evasion tested roughly 500 different variables th revealed regarding tax payer ’ demography life history details his/her tax returns 30 variables built final model include details regarding deductions made current previous year age fin ancial details income assets well details regarding individual tax return items provides good example always necessary available order achieve desired purpose without knowing th e nta decided feature selection project see set limits confirm sufficient achieve goal source skatteetatens analysenytt 1 -2016 //www.skatteetaten.no/globalassets/pdfer/skat teetate ns_analysenytt/analysenytt -1_2016_web_hele.pdf 13 statement nta underscores relevancy black issue case 30 different features possible system lot th would even difficult identify relevant outcome understand explain ’ behind machine employed end product model comes machine models ease w ith results checked varies greatly even though training deep neural networks often first elements mentioned black issues discussed without defining issue fully consider two examples represent extremes ease difficulty understanding checking models namely -called decision trees deep neural networks decision trees decision tree one simplest models basic form broken way placed tree one starts level selects branch based particular feature ’ value one continues base tree final outcome – decision – found see figure type model affords high degree transparency least tree based manageable amount possible move tree see criteria result base d. increasing amounts however point reached difficult person obtain overview understanding 14 neural networks neural networks met hodology largely inspired understanding way brain functions networks built basically simple component perceptron many components create large complex net works perceptron illustrated variable number inputs one output « leg » perceptron weight value value determines great influence input feature final result values adjusted network trained give desired results often carried working backwards network adjust values relevant perceptrons final result backpropagation automated proce ss part process neural network consists three parts input layer one layers output layer 9 //blogs.microsoft.com/ai/2015/12/10/micro soft-researchers -winimagenet -computer -vision -challenge/ one layer considered deep figure single neural network input move emerge result several variants neural networks form loops also send within network final result produced one challenges input viewed isolation many situations work c ontext example words carry different meanings depending context context need formed sentence part reason neural networks form short -term memory allows pro duce different outputs based processed previously course makes difficult determine result derived also means difficult merely examine algorithms find work decisions reach number layers neural network may vary example microsoft image recognition competition using network consisting 152 layers.9 network number connections depend number input values layers interconnected clearly neural network mentioned way beyond comprehended examined without help suitable tools shal l looking tools final chapter 15 meets gdpr provisions gdpr govern controller ’ duties subject personal processed gdpr therefore applies help personal also analyse reach decisions individuals chapter review principles protection articles gdpr especially relevant fundamental principles protection rules governing processing personal basis fundamental princip les article 5 gdpr lists principles apply personal processing essence principles personal shall utilised way protects privacy subject best possible way individual decide personal personal challenges several principles summary principles require personal \\uf0b7 processed lawful fair transparent manner principle legality fairness transparency \\uf0b7 collected specific expressly stated justified purposes treated way incompatible purposes principle purpos e limitation \\uf0b7 adequate relevant limited necessary fulfilling purposes processed principle minimisation \\uf0b7 correct necessary updated accuracy principle \\uf0b7 stored identifiable form longer periods necessary purposes principle relating retention periods \\uf0b7 processed way ensures adequate personal protection principle integrity confidentiality personal personal means relating identified identifiable natural person gdpr art icle 4 1 may directly linked person name identificati number location may also indirectly linked person means person identified basis combination one elements specific person ’ physical physiological genetic mental economic cultural social identity processing processing means operation set operations performed personal collection recording organisation structuring storage adaptation alteration retrieval consultation us e disclosure transmission dissemination otherwise making available alignment combination restriction erasure destruction gdpr article 4 2 controller controller means natural legal person authority agency alone jointly others determines purposes means processing personal gdpr article 4 7 16 addition controller responsible shall abl e prove compliance principles accountability principle following review important protection challenges associated developing using look challenges light protection principles relevant – namely th e principles fairness purpose limitation minimisation transparency algorithmic bias meets fairness principle easy think able perform objective analyses therefore reach better deci sions beings affected low blood sugar bad day desire help friend yet algorithms models objective people devise build personal training model ’ result may incorrect discriminatory training renders biased picture reality relevance area question personal would contraventio n fairness principle principle requires processing personal conducted respect subject ’ interests accordance might reasonably expect principle lso requires controller implement measures prevent arbitrary discriminatory treatment individual persons regulation ’ preface describes suitable mathematical statistical procedures possible measures wo uld however sufficient ensure compliance principle model must also trained using relevant correct must learn emphasise model must emphasise relating racial ethn ic origin political opinion religion belief trade union membership genetic status health status sexual orientation would lead arbitrary discriminatory treatment suspected claimed model entail unfair discriminatory results protection authority investigate whether principle fairness safeguarded processing personal investigations may include review documentation underpinning selection examination algorithm developed whether properly tested came meets principle purpose limitation many models developed using connection good causes cancer diagnosis permitted personal unrestrictedly long good cause purpose limitation principle means reason processing personal must clearly establishe indicated collected essential subject exercise control his/her purpose example claim -based discrimination levelled us system setting bail conditions sentencing system predict risk convicted perso n committing crime journal propublica studied decisions reached system concluded discriminated black defendants number blacks erroneously flagged high -offending risks twice high number whites classified company developed software disagreed propublica ’ conclusion unwilling allow criteria calculations developing algorithm examined therefore impossible convicted person general obtain decisions reached source //www.propublica.org/article/machine -biasrisk-assessments -in-criminal -sentencing 17 processing also needs fully explained subject able make informed choice whether consent yet application often requires many different types personal – infor mation cases collected purposes example possible person ’ facebook activities built algorithm determines whether obtain mortgage bank recycling may useful provide accurate analyses technically feasible previously also contravention purpose limitation principle cases previously -retrieved personal -used controlle r must consider whether purpose compatible original one case consent required basis processing must changed facebook example discussed subject must consent facebook formation bank connection mortgage applications order ensure processing conducted compliance purpose limitation principle – science purpose limitation principle highly important ensuring subject exercises control personal however exceptions principle processing example considered compatible original purpose takes place connection scientific historical statistical archival purposes interest begs question constitutes scientific ex tent application scientific university hospital environments working developing tools examples include models identify risk tax social benefit fraud image recognition software diagnoses cancer tumours really define scientific general ata protection regulation define constitutes scientific general understanding concept however must relate efforts aimed discovering knowledge 10 store norske leksikon know -how.10 gdpr ’ preface recital 159 states scientific interpreted broadly include technological demonstration basic well applied privately financed elements would indicate – cases – may considered constitute scientific applying ntelligence assess person ’ creditworthiness however said aimed gaining knowledge case defined scientific always possible differentiate application matters consideration regulation ’ preface recital 50 states following factors included ascertaining whether processing personal compatible original purpose \\uf0b7 connection original purpose purposes intended processing \\uf0b7 context ata collected \\uf0b7 subject ’ relation controller may affect subject ’ reasonable expectations regard processing \\uf0b7 nature personal \\uf0b7 consequences subject intended pr ocessing \\uf0b7 whether original processing operations ones subject appropriate safeguards list exhaustive issues relevant individual case must included appraisal 18 completed model offline clearly differentiated model developed using training tested similar model put th e training removed algorithm model process personal applied loan applicants algorithm learn anything personal currently processing consequently n develop put models develop improve continuously fed personal include models provide decision support doctors odel learns something every patient receives every scientific article reads knowledge next patient model develops continuous basis difficult differentiate deve lopment hence stops usage begins accordingly therefore difficult reach conclusion regarding extent 0f models constitute scientific limits c onstitutes scientific need reviewed protection regulations come force emphasise personal scientific governed specific rul es gdpr article 89 instances must subject appropriate safeguards secur e subject ’ freedoms safeguards must ensure technical organisational measures place protect minimisation principle particular meets minimisation often takes huge amounts personal develop hand principle minimisation requires shall adequate relevant limited necessary achieving purpose processed means controller personal necessary selected must relevant purpose challenge developing may difficult define purpose processing possible predict algorithm learn purpose may also changed machine learns develops challenges minimisation principle difficult define necessary however minimisation mo principle limiting amount detail included training model principle also stipulates proportionality restricts extent intervention subject ’ privacy personal inv olve may achieved making difficult identify individuals contained basic degree identification restricted amount nature details reveal person others pseudonymisation encryption techniques protect subject ’ identity help limit extent intervention principle also forces developers thoroughly examine intended area application model facili tate selection relevant necessary purpose furthermore developer must consider achieve objective way least invasive subject s. assessments performed need documented pre sented protection authority event inspection connection preliminary discussion protection impact assessment personal processed impacts protection must assessed likely process represent risk freedoms natural persons particularly case using cons ideration must given nature processing scope purpose context performed risk high controller limit duty bound initiate preliminary discussions ata protection authority gdpr articles 35 36 19 although difficult establish advance exact necessary relevant algo rithm – may change project – essential minimisation principle adhered means continuous assessment actual requirements protects subjects also minimises risk irrelevant leading algorithm find correlations rather significant coincidental weight attached pressure personal intensifying based analyses employed promote increased efficiency better services protection authority believes principle minimisation play major role subjects protected general confidence models retained black meets principle transparen processing protection largely safeguarding individuals decide requires controllers open personal transparent transparency achieved providing subjects process details subjects must informed whether collected subjects others gdpr articles 13 14 besides must easily available home page example written comprehensible language gdpr articles 12 shal l enable subjects exercise pursuant gdpr challenging satisfy transparency principle firstly advanced employed diffi cult understand explain secondly black makes practically impossible explain correlated weighted specific process also challenging model may reveal commercial ecrets intellectual property according gdpr ’ preface recital 63 access must avoid consideration others ’ commercial secrets organisation may nevertheless deny subj ect access relating answer find pragmatic solution cases furnishing subject needs protect interests without time disclosing trade secrets problem atical although complex difficult understand explain principle transparent processing personal applies full force discuss duty inform th e subjects general personal collected controller must always provide general \\uf0b7 identity controller \\uf0b7 controller contacted \\uf0b7 purpose processing \\uf0b7 legal basis processing \\uf0b7 categories personal processed \\uf0b7 subjects ’ inspect must also provided regarding risks rules safeguards subjects connection wit h processing well exercised addition extended duty inform apply personal collected automated decision making form automated processing moreove r cases decision taken model important clarify required decision described automated take closer look extended duty inform individual automated decisions individual automa ted decisions decisions relating individuals based machine processing example imposition fine basis image recorded automatic speed camera automated decisions defined regulated articl e 22 gdpr essentially automated individual decisions permitted exceptions apply however automated decision necessary condition entering 20 contract permitted law based exp licit consent dat subject regulation define constitutes exp licit consent opposed ordinary consent phrase indicates express gesture subject required order meet requirements regulation decision must based solely automated processing must produce legal effect similar ly significant ly affect person automated decision must based solely automated processing mean form intervention decision -making process “ intervention ” means natural person must undertaken independent assessment underlying personal authorised -examine recomm endations model produced rules governing automated decision -making circumvented fabricating intervention meant legal effect defined preface would natural understand phrase meaning automated decision must impact subject ’ duties legal set contract see examples listed fact alternative automated dec ision similarly significantly affects person defined closely assume decision must potential affect circumstances behaviour choices person subject automated decision yet difficult state precisely line drawn considerable subjective elements appraisal automated decisions applied measures must implemented protect subject ’ freedoms rightful interests subject must able demand takes final decision must appeal automated decisions involve special categories personal sensitive personal permitted subject consent ed legally warranted important aware alignment different types personal reveal sensitive individuals operating involve processing special categories personal example one study combined “ likes ” facebook simple survey predicted male users ’ sexual orientation accuracy 88 per cent moreover predicted ethnicity 95 per cent examples legal effect \\uf0b7 banned entering country \\uf0b7 satisfy requirements receiving unemployment benefit social security benefit \\uf0b7 electricity supply cut paid bills decisions similarly significantly affect person \\uf0b7 automatic rejection credit application internet \\uf0b7 electronic recruitment without intervention gdpr article 22 interpretation article 22 based latest draft article 29 working party ’ guidelines automated decision making draft based submissions 64 organisations planned publication beginning february 2018. article 29 working party consists represent atives states ’ protection authorities eea country norway observer status working party ’ statements normally carry considerable weight article 29 protection working party xx/2017 automated individual decision -maki ng profiling purposes regulation 2016/679 21 accuracy whethe r user christian muslim 82 per cent accuracy.11 study nature subject legal obligations pursuant gdpr sensitive personal processed outset connection individual automated decisions addition receiving -mentioned gen eral subjects must informed personal collected automated decision -making process relevant must also given regarding underlying logic model well significance nd anticipated impacts automated process given regarding model ’ logic cover example aspects whether decision trees weighted correlated mu st readily understood subject always necessary provide thorough explanation algorithm even include algorithm subjects must also informed automated decisions may affect insurance company employs automated decision -making 11 michael kosinski david stilwell thore graepel « private traits attributes predictable records behaviour proceedings national acad emy sciences united states america » //www.pnas.org/content/110/15/5802.full.pdf set motor insurance premiums basis policy holders ’ driving patterns inform customers possible impacts nd careless driving lead higher premiums subject must receive th e described automated processing commences enable subject lodge complaint processing consent r ight explanation automated decision subject request explanation decision reached words explanation model arrived result preface states necessary guarantees given cases automated processing hall include “ specific … … obtain explanation decision reached fter automated assessment ” recital 71 preface states subject entitled explanation model arrived result words weighted considered specific instance however explanation appear gdpr implications linguistic differences preface wording articles unclear,12 preface legally binding grant explanation regardless differences language mean controller must provide much necessary order subject exercise means decision must explained way subject able understand result explanation necessarily mean black must opened bu explanation enable subject understand particular decision reached needs 12 see example andre burt « explanation machine gdpr » //iapp.org/news/a/is -there -a-right -toexplanat ion-for-machine -learning -in-the-gdpr/ cf sandra wachter brent mittelstadt luciano floridi international privacy law forthcoming « explanation automated decision -making exist general protection regulation » vailable //papers.ssrn.com/sol3/papers.cfm abstract_id=2903469 special categories personal special categories personal include racial ethnic origin political convictions religious philosophical beliefs trade union membership well processing genetic biometric aim uniquely identifying natural person health details regarding person ’ sexual relationships sexual orientation gdpr article 4 22 change order different decision reached .13 subject must informed oppose decision either appealing requesting intervention one explanation makes decision based model ’ recommendtion sometimes automated process occurs lead automated decision instead utilises produced automated process reach decision example employing decision support tool preconditions automated decision taken therefore satisfied question therefore whether subject entitled explanation case automated decision articles gdpr statements preface regarding explanation specific decision preconditions automated decisions satisfied subject nevertheless entitled given necessary safeguard transparen cy principle also sets requirements access also gives subject obtain personal reaching decision however g rant given explanat ion decision even though explanation decision automated transparency principle requires controller give explanation similar given aut omated decis ions 13 see example sandra wachter brent mittelstadt chris russel « counterfactual explanations without pening black automated decisions gdpr » levant regulations addition gdpr regulations requirin g decision explained example sector subject administration act requires inter alia individual decisions substantiated person concerned informed regulations actual circumstances underpinning decision well main considerations decisive administration act sections 24 25 23 controlling algorithms future find decisions affecting us made may decisions regarding whether obtain loan motor insurance premium online newspaper shows us time getting difficult comprehend gain insight complex make decisions behalf dependent service providers processing appropriate manner compliance protection regulations protection authority dpa tasked supervising organisations private sector ensuring comply protection regulations algorithm bla ck supervised pa ’ supervisory competence gdpr establishes investigative authority vested dpa connection supervisory role control whether personal processed accordance regulations dpa may conduct investigation inspection shall clarify whether controller routines guidelines place designed ensure compliance regulations whether routines guidelines followed connection investigation representatives pa may ask require perform tasks might consist documentation relating organisational technical measures risk assessments protection impact assessments employee training ways approaches made subjects followed representatives may also requ ire given access premises processing equipment means well personal processed access premises processing equipment means shall granted accordance procedural rules apply nationally consulted subject personal act norway norwegian dpa proposed consideration given granting authority powers securing evidence similar currently wielded norwegian competition authority investigating organisation developing utilising bound legal constraints oth er organisation processing personal course normal inspection dpa check whether organisation basis processing whether satisfactory internal controls routines risk assessments carri ed technical organisational measures place order protect reas may particularly important control organisations utilising compliance principles described earlier report -used purposes without adequate processing basis organisations process personal need measures place ensure fair treatment subjects informed required law organisation develops may relevant control nature quantity training well applied training process organisation uses -based system may relev ant check whether tests results conducts audits ensure personal utilised unlawful discriminatory manner also relevant investigate whether system developed basis privacy design deep investigation go investigations sufficient dpa obtain documentation determine whether organisation compliance regulations organisation must able explain document cases demonstrate process personal accordance rules means organisation must know system processes personal able account organisat ion account uses personal dpa authorised impose fine temporary definitive ban processing activit ies 24 dpa suspects account given organisation wrong contains erroneous ask organisation verify details routines assessments example organisation demonstrate system processes personal may necessary example suspicion algorithm using organisation basis processing suspicion algorithm correlating lead discriminatory result dpa currently carries system controls inspection cases need dpa check taking place inside system example investigating long camera recording stored expect need control increase coming years line great er automated analyses decision -making sectors moreover personal act places greater emphasis controller ’ duty carry responsible operations internal controls less emphasis preliminary controls cond ucted dpa.14 inspect “ black ” “ ordinary ” algorithms relatively straightforward deal th ey programmed carry specific actions example income x debts obtain loan z. much -simplified example shows possible see inputs processed order obtain given result however models based deep neural networks complex low transparency making challenging control actually taking place inside system considerable knowledge based required order know look well questions ask inspection situation identify need delve deeply system advanced technological expertise required resource utilisation standpoint solution may hire external expertise cases “ deep ” control required -based system important dpa knowledge resources required discover breac hes regulations avoid algorithms reinforce social differences lead arbitrary discrimination well unlawful -use 14 see guidelines responsibilities enterprises gdpr th e norwegian dpa ’ web site norwegian //www.datatilsynet.no/regelverk -og-skjema/veiledere/virksomhetens ansvar -etter -nytt-regelverk 25 solutions recommen dations protection principle underpins applications accountability principle central gdpr places greater responsibility controller ensur ing processing conducted comp liance rules processors also bound accountability principle chapter shall present examples tools solutions help controller comply rules first discuss two requirements gdpr especially important connection application protection impact assessment dpia privacy design following look tools methods help protect privacy finally shall propose recommendations developers system suppliers organisations buying using end users authorities assess protection impact – build privacy syste protection regulations enhance individuals time duties organisations tightened two requirements especially relevant organisations using requirements privacy design dpia privacy design controller shall build privacy protection ensure protection safeguarded system ’ standard settings requirements described article 25 gdpr apply developing software ordering solutions services well developing rules require protection given due consideration stages system 15 read norwegian dpa ’ guidelines software wi th embedded privacy //www.datatilsynet.no/en/regulations -andtools/guidelines/data -protection -by-design -and-by-default/ routines daily standard settings shall protective privacy possible protection features shall embedded design stage.15 principle minimisation expressly mentioned provision relating privacy design protection impact ssessment anyone process ing personal duty assess risks involved enterprise believes planned process likely pose high risk natural persons ’ freedoms duty conduct protection impact asses sment dpia described article 35 gdpr risk assessed consideration shall given nature scope context purpose process must also taken account moreover requi rement assess impact personal privacy systematically extensively considering personal details cases automated decision making special categories personal sensitive personal large systematic large -scale monitoring areas also requires documentation showing dpia conducted impact assessment include following minimum \\uf0b7 systematic description process ts purpose justified interest protects \\uf0b7 assessment whether process necessary proportional given purpose \\uf0b7 assessment risk processing involves people ’ including privacy \\uf0b7 measures selected managing risk identified dpa shall involved preliminary discussions impact analysis reveal planned process may represent high risk subjects 26 risk reduced controller gd pr article 36 tools methods good protection rapidly developing applies tools methods help meet protection challenges posed collect ed number examples illustrate available options methods evaluated practice assessed according possible potential means technically perhaps unsuitable today concepts exciting potential future placed methods three categories \\uf0b7 methods reducing need training \\uf0b7 methods uphold protection without reducing basic dataset \\uf0b7 methods des igned avoid black issue 1. methods reducing need training one challenges pointed report often need huge amounts machine however selecting features adjusting appropriately requirement reduced selection methods help achieve generative adversarial networks16 generative adversarial networks gan generating synthetic f today gan mainly generation images also potential becoming method generating huge volumes high quality synthetic training areas satisfy need labelled large volumes without need utilise great amounts containing real personal federated learning17 form distributed federated works downloading latest version centralized 16 //papers.nips.cc/paper/5423 -generative -adversarial -nets.pdf 17 //research.googleblog.com/2017/04/federated -learning collaborative.html model client unit example mobile phone model improved locally client basis local changes model sent back server consolidated change models clients average changed improve centralized model improved centralized model may downloaded clients provides opportunity improve existing model basis large number users without share users ’ matrix capsules18 matrix capsules variant neural network require less currently norm deep advantageous lot less required machine 2. methods protect privacy without reducing basis optimal solution would one could much one wished machine without compromising privacy fie ld cryptology offers promising possibilities area differential privacy19 let us example start database contains natural persons features related persons retrieved database response contain deliberately -generated “ noise ” enabling retrieved persons database precise details specific individuals database must able give markedly different result query ind ividual person removed database overriding trends characteristics dataset change homomorphic encryption encryption method enables processing whilst still encrypted means confidentiality maintained without limiting usage possibilities dataset present homomorphic encryption limitations mean employing operate much lower rate effic iency promising however 18 //openreview.net/pdf id=hjwlfgwrb 19 //www.cis.upenn.edu/~aaroth/papers/privacybook.pdf //arxiv.org/abs/1412.7584 27 microsoft example published white paper system uses homomorphic encryption connection image recognition.20 efforts also underway standardise homomorphic encryption soluti ons.21 transfer learning22 case always necessary develop models scratch another possibility utilise existing models solve similar tasks basing processing existing models often possible achieve result less shorter time libraries containing pretrained models raird statistics norway ssb norwegian centre nsd developed system called raird23 perm carried without direct access complete dataset short system works means interface allows researchers access metadata underlying dataset dataset may examp le cancer diagnosis register containing fields age gender date place birth researcher submit queries based metadata obtain report containing aggregate olution designed prevent retrieval relating small groups individual persons type system therefore machine needed instead receiving report end result one could obtain model system 3. meth ods avoidiing b lack issue one issues mentioned lack transparency connection machine automated decision -making represents challenge using system people whose processed system developers base work machine would derive great benefit knowing takes place bonnet 20 //www.microsoft.com/en -us/research/publication/cryptonets applying -neural -networks -to-encrypted -data -with -high -throughput -andaccuracy/ 21 //homomorphicenc ryption.org/ 22 //www.cs.utexas.edu/~ml/publications/area/125/transfer_learning order quality assure improve products explainable xai 24 xai idea automated decisions made explicable people involved process often desirable explanation given outcome interesting possibilities two areas also need able control embedded probably also attractive developers employing transfer also project underway field run defense advanced projects agency darpa objective gain knowledge providing understandable explanations automated decisions sponsored oregon state university awarding amount usd 6.5 million four years topic goal create explain decisions way understandable promotes confidence using system case good grounds believing drive field forward lime25 lime approach xai model -agnostic solution produces explanations ordinary people understand case image recognition example able show parts picture relevant thinks image makes easy anyo ne comprehend basis decision 23 //raird.no/ 24 //www.darpa.mil/program/explainable -artificial -intelligence 25 //www.oreil ly.com/learning/introduction -to-local -interpretable model -agnostic -explanations -lime recommendations privacy friendly following propos e number recommendations protecting personal developing using reco mmendations develop ers recommendations meant actors pursuing main milieus universities large commercial organisations constitute important target group becaus e developing basic basis application \\uf0b7 conduct intelligent system made privacy friendly designed order make easy users comply regulations example carried solutions less training anonymisation techniques solutions explain process reach conclusions interesting include conduct system audits ensure system ’ biased especially audits third parties \\uf0b7 adopt multidisciplinary approach important put together multi -disciplinary teams consider consequences society developed also throw light considerable value society well problematical areas recommendations system suppliers recommendations meant organisations basic technologies developed others – organisations projects solutions supplied others controllers merely supplier service product recommendations als relevant milieus utilising technologies developed others \\uf0b7 get familiar gdpr – duties duties user system \\uf0b7 select models meet privacy needs buyer example types model explain reached specific result \\uf0b7 limit amount personal training relevant necessary purpose \\uf0b7 ensure document system developing meets requirement privacy design \\uf0b7 document protection requirements met documentation one requirements regulations requested customers users \\uf0b7 assist customers showing different protect personal dat example helping fulfil duty provide showing customer test audit system ensure compliance regulations internal requirements recommendations organisations purchasing using -based recommendations aimed organisations purchasing using solutions based technologies could commercial organisations \\uf0b7 carry risk assessment required carry dpia pur chase system start using well \\uf0b7 demand system order satisfies requirements privacy design \\uf0b7 conduct regular tests system ensure complies regulatory requirements exam ple avoiding latent discriminatory treatment \\uf0b7 ensure system protects users example demand limit ed processing \\uf0b7 ensure good protecting subjects righ access deletion consent legal basis processing system must also include functionality enabling consent given withdrawn \\uf0b7 consider establishing industry norms ethical guidelines protecti panel consisting external experts fields society protection provide advice legal ethical social technological challenges – opportunities – linked 29 recommendations end users recommendations aimed end users end user subject using service whose personal details processed using \\uf0b7 entitled comprehensible readily available p rocessing personal applies organisations retrieve directly retrieved sources shall know purpose legal basis whic h organisation processing \\uf0b7 consent many situations controller must obtain consent processing begin controller responsible documenting proper consent given means given voluntary specific informed unambiguous declaration approve personal processed also withdraw consent given previously \\uf0b7 access contact organisations ask whether processing details registered rule entitled copy details registered however exceptions access informati example within judicial sector \\uf0b7 rectify delete entitled ask incorrect unnecessary details rectified deleted \\uf0b7 object processing details may protest processing details concerning protest direct marketing must stopped without needing provide grounds situations may object expla ining circumstances affecting situation organisation must cease processing unless prove compelling justifiable grounds processing grounds weigh heavily interests freedoms \\uf0b7 demand limited processing opinion details incorrect processed unlawfully exercised protest processing organisation may compelled stop continue store disagreement settled \\uf0b7 portability whether contractually given consent personal processed ask details delivered organisation structured generally applicable machine -readable format recommendations authorities recommendations legislators political decision makers set terms conditions \\uf0b7 ensure sector sets good example using requires acute awareness ethical privacy consequences well expertise buyers make sure purchased privacy design meet legislative requirements \\uf0b7 allocate funds ensure processes personal compliance regulations protecting personal legal requirement also competitive advantage norwegian industry \\uf0b7 ensure enforcement authorities possess relevant expertise arrange experience knowledge sharing across sectoral boundaries \\uf0b7 ensure law keeps apace technological deve lopments applies legislation relevance personal norwegian protection authority visiting address tollbugata 3 0152 oslo norway postal address p.o 8177 dep. 0034 oslo norway postkasse datatilsynet.no telephone +47 22 39 69 00 datatilsynet.no personvernbloggen.no twitter.com/datatilsynet\",\n",
       " \"guidance auditing framework draft guidance consultation ico commissioner 's office auditing framework draft guidance consultation contents guidance ................................................................................... 4 produced guidance ................................................ 5 mean ‘ ’ ................................................................ 6 guidance relate ico work ......................... 7 guidance ................................................................... 8 ico focusing risk -based approach ........................ 8 guidance set principles ................................................. 9 legislation applies ................................................................... 10 guidance structured ........................................................ 11 accountability governance implications ofai ........... 12 approach governance risk management ............. 13 set meaningful risk appetite .................................... 13 need consider undertaking protection impact assessments ....................................................................... .. 15 understand controller/processor relationships ........ 21 -related trade -offs manage .......... 26 need ensure lawfulness fairness transparency ......................................................................................... 36 principle lawfulness fairness transparency apply ................................................................................................ 36 identify purposes lawful basis using .......... 37 need statistical accuracy ............................... 46 address risks bias discrimination ....................... 53 assess security minimisation ................ 64 security risks introduce ................................................ 64 case study security risks introduced externally maintained software steps take manage risks privacy attacks minimisation privacy -preserving techniques available case study losing track training ............................................. 66 build ................................................................... 67 types privacy attacks apply models ............................... 69 models ........................................................................................ .73 .................................................................................... 77 enable individual ........................ 86 individu al apply different stages lifecycle ....... 86 20200214 version 1.0 2 auditing framework draft guidance consultation individual relate contained model ...... 91 enable individual relating solely automated decisions legal similar effect ............................................................... 92 role oversight .................................................. 97 20200214 version 1.0 3 auditing framework draft guidance consultation guidance glance applications incr easingly permeate many aspects lives understand distinct benefits bring also risks pose freedoms individuals developed framework auditing focusing best practi ces protection compliance – whether design system implement one third party provides solid methodology audit applications ensure process personal fairly comprises • auditing tools procedures audits investigations • detailed guidance protection includes indicative risk control measures deploy process personal guidance aimed two audiences • compliance protection officers dpos general counsel risk managers ico 's auditors • specialists including machine experts scientists software developers engineers cybersecurity risk managers guidance clarifies assess risks freedoms pose appropriate measures implement mitigate protection ‘ ethics ’ overlap guid ance provide generic ethical design principles corresponds different protection principles structured follows • part one addresses accountability governance including protection impact sessments dpias • part two covers fair lawful transparent processing including lawful bases assessing improving system performance mitigating potential discrimination • part three addresses minimisation security • part four facilitate exercise individual including related automated decision -making 20200214 version 1.0 4 audi ting framework -draft guidance consultation detail • produced guidance • mean ‘ ’ • guidance relate ico work • guidance • ico focusing risk -based approach • guidance set principles • legislation applies • guidance structured produced guidance see uses everyday healthcare recruitment commerce beyond understand nefits bring organisations individuals risks ’ one three strategic priorities decided develop framework auditing compliance protection obligations framework • gives us solid methodology audit applications ensure process personal fairly lawfully transparently • ensures necessary measures place assess manage risks freedoms arise • supports work ur investigation assurance teams assessing compliance organisations using well using framework guide activity also wanted share thinking behind framework therefore two distinct outputs 1. auditing tools procedures investigation assurance teams assessing compliance organisations using 2. detailed guidance protection organisations outlines thinking also incl udes indicative risk control tables end section help organisations audit compliance guidance aims inform think constitutes best practice protection- compliant 20200214 version 1.0 5 auditing framework -draft guidance consultation guidance statutory code contains advice interpret relevant law applies recommendations good practice organisational technical measures mitigate risks individuals may cause exacerb ate penalty fail adopt good practice recommendations long find another way comply law reading – ico guidance strategy -2021 mean ‘ ’ term ‘ ’ variety meanings within community refers various methods ‘ using non -human system learn experience imitate intelligent behaviour ’ whilst context protection referred ‘ theory computer able perform tasks normally requiring visual perception speech recognition decision -making translation languages ’ however large amounts make predictions classifications individuals existed sectors like insurance since 19 th century long term ‘ ’ coined 1950s traditional forms statistical analysis statistical models calculated using pen paper later calculator modern machine techniques much easier create statistical models using computationally intensive techniques much larger multi-dimensional datasets increase complexity models combined decreasing costs creating heightened concerns risks freedoms individuals one prominent area ‘ machine ’ ml computational techniques create often complex statistical models using typically large quantities models make classifications predictions points involves ml recent interest driven ml way whether context image recognition speech -to-text classifying credit risk guidance therefore focuses protection challenges ml -based may present acknowledging kinds may differ processing large amounts personal purpose protection law applies processing context statistical models using models make predictions people guidance relevant regardless whether classify activities ml 20200214 version 1.0 6 auditing framework -draft guidance consultation umbrella term ‘ ’ becom e mainstream way organisations refer range technologies mimic thought similar technologies similar sources risk likely benefit set risk measures whether call machine complex processing something else risks controls identified helpful important differences different types example simple regression models deep neural networks refer explicitly resources see international working group protection telecommunications ’ working paper privacy external link gu idance relate ico work guidance designed complement existing ico resources including • big achine report published updated • guidance explaining decisions made produced collaboration alan turing institute explain guidance bi g report provide stron g foundatio n understanding protectio n implications f technologies note commissioner ’ foreword edition thi complicated fast -developing area considerations h ave arise n las three years n terms f risks pose individuals organisational technical measures taken address risks engageme nt stakeholders gaine additional insights nto org anisations using ground go beyo nd presented n report another significant challenge raise explainability par government ’ sector deal n collaboration turing institute produced guidance organisations best explain individuals resulted explain guidance published draft form consultation last year process finalising explain guidance light feedback stakeholders update links guidance c ompleted exp lain guidance already covers challenge explainability individuals substantia l detail guidance includes additional considerations explainability within organisation eg internal oversight compliance two pieces guidance complementary recommend reading tandem 20200214 version 1.0 7 auditing framework -draft guidance consultation reading – ico guidance big machine protection ico turing consultation explaining decisions guidance guidance guidance covers best practices protection- compliant two broad intended audiences first compliance including • protection officers • general counsel • risk managers • ico ’ auditors – words utilise guidance exercise audit functions protection legislation second specialists including • machine developers scientists • software developers engineers • cybersecurity risk managers guidance written accessible audiences parts aimed primarily either compliance roles signposted accordingly ico focusing risk -based approach taking risk -based approach means • assessing risks freedoms individuals may arise • implementing appropriate proportionate technical organisational measures mitigate risks general requirements protection law mean ignore law risks low may mean stop planned project sufficiently mitigate risks help integrate guidan ce existing risk management process organised several major risk areas risk area describe 20200214 version 1.0 8 auditing framework -draft guidance consultation • risks involved • may increase likelihood and/or impact • possible measures could identify evaluate minimise monitor control risks technical organisational measures included consider good practice wide variety contexts however since many risk controls may need adopt context- specific include exhaustive definitive list guidance covers -and-data-protection -specific risks implications risks governance accountability regardless whether using accountability measures place however adopting applications may require r e-assess existing governance risk management practices applications exacerbate existing risks introduce ones generally make risks difficult assess manage decision -makers organisation therefore reconsider organisation ’ risk appetite light existing proposed applications sections guidance deep -dives one challenge areas explores associated risks processes controls guidance set principles guidance provide generic ethical and/or design principles may overlaps ‘ ethics ’ protection proposed ethics principles already reflected protection law guidance focused protection compliance although protection dictate designers jobs process personal need comply principles protection design default may direct application developers ’ go play role processing – however also note • responsibility put place appropriate technical organisational measures designed im plement protection principles effective manner • developer personal train models controller processing protection law applies certain design choices lik ely result infringe protection one way guidance help designers engineers understand choices better design high performing whilst still protecting freedoms individuals 20200214 version 1.0 9 auditing framework -draft guidance consultatio n worth noting work focuses exclusively protection challenges introduced heightened general protection considerations addressed except far relate challenged resources read global privacy assembly ’ ‘ declaration ethics protection ’ external link ethics protection intersect context legislation applies guidance deals challenges raises protection relevant piece uk legislation protection act 2018. dpa sets uk ’ protection framework alongside general protection regu lation gdpr comprises following protection regimes • part 2 – supplements tailors gdpr uk • part 3 – sets separate regime law enforcement authorities • part 4 – sets separate regime three services guidance apply regardless part dpa applies processing however relevant differences requirements different regimes explicitly addressed text shou ld also review guidance brexit impacts protection law impacts areas ico competence protection notably freedom considered reading – ico guidance different regimes read guide protection nee detail protection brexit see faqs 20200214 10 version 1.0 auditing framework -draft guidance consultation guidance structured guidance divided several parts corresponding different protection principles part one addresses issues primarily relate accountability principle requires responsible complying protection principles demonstrating compliance sections part deal -specific implications accountability including da ta protection impact assessments dpias controller processor responsibilities assessing justifying trade -offs part two covers lawfulness fairness transparency processing personal sections covering lawful bases processing personal assessing improving system performance mitigating potential discrimination ensure fair processing part three covers principle security minimisation part four co vers facilitate exercise individuals ’ personal relating solely automated decisions particular part four covers ensure meaningful input non -automated p artly-automated decisions meaningful review solely automated decisions 20200214 11 version 1.0 auditing framework -draft guidance consultation accountability governance implications glance accountability principle makes responsible complying protection demonstrating compliance system context accountability requires • responsible complian ce system • assess mitigate risks • document demonstrate system compliant choices made consider issues part dpia system intend note legally required complete dpia process personal dpias offer opportunity consider using process personal potential risks could due complex ity mutual dependency various kinds processing typically involved supply chains also need take care understand identify controller processor relationships additionally depending designed deployed inevitably involve making trade -offs privacy competing interests need know trade -offs may manage otherwise risk fail adequately assess strike balance however also note always comply fundamental protection principles ‘ trade ’ requirement away detail • approach governa nce risk management • set meaningful risk appetite • need consider undertaking protection impac assessments • understand controller/processor relationships • -related trade -offs manage 20200214 12 version 1.0 auditi ng framework -draft guidance consultation approach governance risk management well potential make organisations efficient effective innovative however also raises significant risks freedoms individuals well compliance challenges organisations different technological approaches either exacerbate mitigate issues many others much broader specific rest guidance suggests protection implications heavily dependent specific cases population deployed overla pping regulatory requirements well social cultural political considerations increases importance embedding protection design default organisation ’ culture processes technical complexities sys tems make difficult demonstrating addressed complexities important element accountability delegate issues scientists engineering teams senior management including protectio n officers dpos accountable understanding addressing appropriately promptly addition upskilling need diverse well resourced teams support discharging responsibilities also need internal structures roles responsibilities maps training requirements policies incentives overall governance risk management strategy important underestimate initial ongoing level investment resources effort required governance risk management capabilities need proportionate particularly true adoption still initial stages associated laws regulations governance risk management best practices still developing quickly also currently developing general accountability toolkit specific provides baseline demonstratin g accountability gdpr could build approach accountability update final version guidance refer final version accountability toolkit published set meaningful risk appetite risk -based approach protection law requires comply obligations implement appropriate measures context particular circumstances – nature scope context purposes 20200214 13 version 1.0 auditing framework -draft guidan ce consultation processin g inten risks poses individuals ’ freedoms compliance consideration therefore involve assessing risks nd freedoms f individuals takin g judgements wha appropriate circumstances cases need ensure comply protection requirements applie technologie proce ss personal n conte xt specific n ature f risks pose circumstances r processing require strike appropriate balance betwee n competin g interests yo u go ensuri ng protec tion compliance may turn impact outcome processing unrealistic adop ‘ zero tolerance ’ approac h risks nd freedoms indee law n ot require – ensuring risks identified managed mitigated see ‘ trade-o ffs manage ’ manage risks individuals tha arise processi ng personal n importan yo u develop mature understanding articulatio n fundamental risks nd balance othe r interests ultimately necessary fo r • assess risks individuals ’ poses • determine need address • establish impact ensure approach fits organisation circumstances processing appropriate also risk assessment frameworks complex task take time get ultimately however give well ico fuller meaningful view risk positions adequacy compliance risk management approaches following sections deal -specific implications accountability including • undertake protection impact assessments • identify whether controller processor specific processing operations involved deployment resulting implications responsibilities • assess risks freedoms individuals address design decide system 20200214 14 version 1.0 auditing framework -draft guidance consultati • document demonstrate approach take including decision processing question need consider undertaking protection impact assessments dpias key part protection law ’ accountability protection design see dpias mere ticking compliance exercise effectively act roadmaps identify control risks freedoms pose also perfect opportunity consider demonstrate accountability decisions make design procurement need carry dpias protection law using process personal likely result high risk individuals ’ freedoms therefore triggers legal requirement undertake dpia result assessment indicates residual high risk individuals sufficiently reduce must consult ico prior starting processing addition conducting dpia may also required undertake kinds impact assessments voluntarily instance sector organisations required undertake equality impact assessments organisations voluntarily undertake ‘ algorithm impact assessments ’ reason combine exercises long assessment encompasses requirements dpia ico produced detailed guidance dpias explains required complete section sets things think carrying dpia processing person al relevant provisions legislation see articles 35 36 recitals 74 -77 84 89 -92 94 95 gdp r external link see sections 64 65 dpa external link decide whether dpia list types processing likely result high risk process personal must carry dpia case major project involves personal good practice dpia 20200214 15 version 1.0 auditing framework -draft guidance consultation read list processing operations ‘ likely result high risk ’ examples operations require dpia detail criteria high risk combination others reading – ico guidance see ‘ decide whether dpia ’ guid ance dpias assess dpia dpia needs describe nature scope context purposes processing personal -it needs make going process need detail • collect store • volume variety sensitivity • nature relationship individuals • intended outcomes individuals wider society well context lifecyc le dpia best serve purpose undertake earliest stages project feature minimum following key components describe processing dpia include • systematic description processing activity including flows stages processes automated decisions ay produce effects individuals • explanation relevant variation margins error performance system may affect fairness personal processing see ‘ statistical accuracy ’ • description scope context processing including process number subjects involved source far individuals likely expect processing dpia identify record degree involvement decision -making process stage takes place automated decisions subject intervention view implement processes ensure meaningful also detail fact decisions overturned 20200214 16 version 1.0 auditing framework -draft guidance consultation difficult describe processing activity complex system may appropriate maintain two versions assessment • first presenting thorough technical description specialist audiences • second containing high -level description processing explaining logic personal inputs relate outputs affecting individuals dpia set roles obligations controller include processors involved partly wholly outsourced external providers organisations involved also assess whether joint controllership exists article 26 gdpr collaborate dpia pro cess appropriate processor illustrate technical elements processing activity dpia reproducing processor example flow diagram processor ’ manual however generally avoid copying large sections processor ’ literature assessment relevant provisions legislation see article 35 7 recitals 84 90 94 gdpr external link need consult anyone • seek document views individuals representatives unless good reason • consult relevant internal stakeholders • consult processor one • consider seeking legal advice expertise appropriate unless good reason seek document views individuals representatives intended processing operation dpia therefore important describe processing way consu lted understand relevant provisions legislation see article 28 3 f article 35 9 gdpr external link assess necessity proportionality deployment system process personal needs driven proven ability system fulfil specific legitimate purpose 20200214 17 version 1.0 auditing framework -draft guidance consultation availability assessing necessity dpia evidence ’ accomplish purposes less intrusive way dpia also allows demonstrate processing personal system proportionate activity assessing proportionality need weigh interests using risks may pose freedoms individuals ystems need think detriment individuals could follow bias inaccuracy algorithms sets within proportionality element dpia need assess whether individuals would reasonably expect system conduct processing complement replace decision -making document dpia project might compare algorithmic accuracy side- by-side better also describe trade -offs made example statistical accuracy minimisation document methodology rationale identify assess risks dpia process help objectively identify relevant risks assign score level risk measured likelihood severity impact individuals personal deployment may pose risks individuals ’ considering sources risk dpia consider potential impact material non-material damage harm individuals instance machine may reproduce discrimination historic patterns could fall foul equalities legislation similarly stop published based analysis creator ’ personal could impact freedom expression contexts consider relevant legal frameworks beyond protection relevant provisions legislation see articles 35 7 c recitals 76 90 gdpr e xternal link identify mitigating measures identified risk consider options reduce level assessed risk examples could minimisation providing opportunities individuals opt processing 20200214 18 version 1.0 auditing framework -draft guidance consultation ask dpo advice considering ways reduce avoid risk record dpia whether chosen measure reduces eliminates risk question important dpos governance professionals involved projects earliest stages must open channels communicat ion project teams ensure identify address risks early lifecycle protection afterthought dpo ’ professional opinion come surprise eleventh hour dpia document safeguards put place ensure individuals responsible testing validation deployment monitoring adequately trained appreciation protection implications processing dpia also evidence organisational measures put place appropriate training mitigate risks associated error also document technical measures designed reduce risks security accuracy personal processed system measures introduced mitigate risks identified dpia document residual levels risk posed processing required eliminate every risk identified however assessment indicates high risk unable sufficiently reduce required consult ico go ahead processing conclude dpia record • additional measures plan take • whether risk eliminated reduced accepted • overall level ‘ residual risk ’ taking additional measures • opinion dpo one • whether need consult ico happens next although must carry dpia processing personal begins also consider ‘ live ’ document means reviewing dpia regularly undertaking reassessment appropriate eg nature scope context purpose processing risks posed individuals alter reason 20200214 19 version 1.0 auditing framework -draft guidance consultation instance depending deployment could demographics target population may shift people adjust behaviour time response processing relevant provisions legislation see articles 35 11 36 1 39 1 c recital 84 gdpr external link reading – ico guidance read guidance dpias guide gdpr including list processing operations likely result high risk dpias legally required also read detailed guidance dpia including step described may also want read relevant sections guide • lawfulness fairness transparency • lawful basis processing • minimisation • accuracy reading – protection board protection boar edpb replaced article 29 working party wp29 includes representatives protection authorities member state adopts guidelines complying requirements gdpr wp29 produced guidelines protection impact assessments wp248 rev.01 en endorsed edpb relevant guidelines include • guidelines protection officers ‘ dpos ’ wp243 rev.01 • guidelines automated individual decision -making profiling wp251 rev.01 20200214 20 version 1.0 auditing framework -draft guidance consultation understand controller/processor relationships controllership important many cases various processing operations involved may undertaken number different organisations therefore crucial determine controller joint controller processor involves multiple orga nisations first step understanding relationships identify distinct sets processing operations purposes see ‘ assess dpia organisations work need assess whether controller processor joint controller consult guidance controller/processor assista nce assessment essence • decide purposes means processing controller • process personal instruction another organisation processor • jointly determine purposes means processing another organisation joint controllers usually involves processing personal several different phases possible may controller joint controller phases processor others means processing decided controllers processors may discretion decide non-essential details means possible generalise every context following examples show kinds decisions means -related processing may indicate controller decisions non-essential details could taken processor type decisions may make us controller type decisions likely make controller include deciding • collect personal first place • purpose processing • individuals collect tell processing • long retain • respond requests made line individuals ’ 20200214 21 version 1.0 auditing framework -draft guidance consultation specifics circumstances may make controller read guidance controllers processors context type decisions made controllers include • source nature train model • target output model ie predicted classified • broad kinds ml algorithms create models eg regression models decision trees random forests neural networks • feature selection – features may model • key model parameters eg complex decision tree many models included ensemble • key evaluation metrics loss functions trade -off false positives false negatives • models continuously tested updated often using kinds ongoing performance assessed whilst non -exhaustive list constitutes decisions purpose means make decisions likely controller another organisation jointly determine joint controllers decisions processors take conversely ’ make decisions process basis agreed terms likely processor processors different obligations depending terms contract controller able take certain decisions • methods process personal • store • security measures protect • retrieve transfer delete dispose context processors may able decide depending terms contract • spe cific implementation generic ml algorithms programming language code libraries written • models stored formats serialised stored local caching 20200214 22 version 1.0 auditing framework -draft guidance consultation • measures optimise algorithms models minimise consumption computing resources eg implementing parallel processes • architectural details models deployed choice virtual machines microservices apis make kinds decisions may processor rather controller practice means provide variety services without necessarily considered controller however key remember overarching decisions processed purposes taken controller providing tools stance might provide cloud- based service consisting dedicated cloud computing environment processing storage suite common tools ml services enable clients build run models chosen process using tools infrastructure provide cloud example clients likely controllers whilst processor could therefore decide processor programming languages code libraries tools written configuration storage solutions graphical user interface cloud architecture clients controllers long take overarching decisions models want key model parameters processes evaluating testing updating models clients ’ purposes decided become controller processing providing prediction service companies provide live prediction classification services customers instance might develop models allow customers send queries via api eg ‘ objects image ’ get responses eg classification objects image case prediction service provider likely controller least processing first processing necessary create improve models po wer services means purposes processing principally decided service provider likely controller processing second processing necessary make predictions classi fications particular examples behalf 20200214 23 version 1.0 auditing framework -draft guidan ce consultation clients kind processing client likely controller processor however may even considered controller joint controller latter kind processing design services su ch way customers sufficient influence essential elements purposes processing involved prediction instance enable customers set balance false positives negatives according requirements add remove certain features model may practice joint controller exerting sufficiently strong influence essential means processing furthermore initially process behalf client part providing service process clients improve models controller processing decided undertake purposes instance recruitment consultant might process job applicants ’ behalf clients using ml system provides scores applicant also retaining improve ml system cases explicitly state purposes outset seek lawful basis see section purposes lawful bases additionally protection legislation states tha anyone acting authority controller processor shall process personal except instructions controller unless required law need consider applies particular circumstances example processor permitted process instructions controller – processes personal due role processor processing may still infringe gdpr even purposes fair compatible original controller needs agree disclose third -party controller ie sharing operation needs comply protection law relevant provisions legislation see article 29 gdpr external link see sections 60 106 dpa external link responsibilities procuring models local deployment companies sell provide free pre -trained models standalone pieces software customers install run order develop market robust system organisations would purchase third parties likely processed personal form eg train system however 20200214 24 version 1.0 auditing framework -draft guidance consulta tion integrate system processing environment developer may play part processing providers make decisions processing personal purposes training models sell controller processing however third party developers may intend process personal da ta controllers behalf processors procured models deploy model provider process personal controller processing undertake using model provider services identify processing operations controller processor ensure clients procuring services different responsibilities part considerations selecting system process personal • remember compliance protection law remains responsibility whether procure system build • check developer des igned system including whether line protection principles • result assess whether using service going help meet processing objectives well protection obligations controller writing contracts service level agreements also need remember contract might stipulate status controller processor matters protection perspective practice decides purposes essential means processing similarly provider services identify processing operations controller processor ensure clients relevant provisions legislation see articles 4 7 4 8 5 1 5 2 25 28 recital 78 gdpr external link reading – ico guidance read guidance controller/processor contracts/liabilities guide gdpr 20200214 25 version 1.0 auditing framework -draft guidance consultation resources see court justice union ’ cjeu judgment case unabhängiges landeszentrum für datens chutz uld schleswig -holstein wirtschaftsakademie schleswig- holstein gmbh see also cjeu ’ judgment case fashion id gmbh co. kg verbraucherzentrale nrw ev -related trade -offs manage must comply requirements protection law however number different values interests may times pull different directions risk -based approach protection law help navigate potential ‘ trade -offs ’ privacy one hand competing values interest using therefore need iden tify assess interests strike appropriate balance given context whilst continuing meet obligations law balance particular trade -off depends specific sectoral social context operate impact individuals however methods assess mitigate trade -offs relevant many cases following sections provide short overview notable trade -offs likely face designing procuring privacy vs statistical accuracy fairness protection context generally means handle personal ways people would reasonably expect ways unjustified adverse effects improving ‘ statistical accuracy ’ system ’ outputs one considerations ensure compliance fairness principle important note word ‘ accuracy ’ different meaning contexts protection accuracy protection one fundamental principles requiring ensure personal accurate necessary kept date accuracy generally statistical modelling refers often system guesses correct answer – many cases answers personal ‘ accuracy principle ’ applies personal process system need 100 ‘ statistically accurate ’ order comply principle however statistically accurate 20200214 26 version 1.0 auditing framework -draft guidance consultation system likely processing line fairness principle clarity guidance • term ‘ accuracy ’ refer accuracy principle protection law • term ‘ statistical accuracy ’ refer accuracy system differences ‘ protection ’ accuracy statistical accuracy see section ‘ need statistical accuracy ’ general system learns case ml models trained statistically accurate likely capture underlying statistically useful relationships features datasets example model predicting future purchases based customers ’ purchase history would tend statistically accurate customers includ ed training features added existing dataset may relevant model trying predict instance purchase histories augmented additional demographic might improve statistical accuracy model however generally speaking points collected person people whose included set greater risks individuals reading – ico guidance read guidance minimisation guide gdpr statistical accuracy discrimination discussed ‘ address risks bias discrimination ’ section lead biased discriminatory outcomes may turn pose compliance risks terms fairness princip le need implement appropriate technical measures mitigate risk considerations also include impact techniques statistical accuracy system ’ performance example reduce potential discrimination might modify credit risk model proportion positive predictions people different protected characteristics eg men women equalised may help prevent discriminatory outcomes could also resul higher number statistical errors overall also need manage 20200214 27 version 1.0 auditing framework -draft guidance consultation practice may always tension statistical accuracy avoiding discrimination example discriminatory outcomes model driven lack minority population statistical accuracy th e model could increased collecting whilst also equalising proportions correct predictions however case would face different choice -between collecting minority population interests reducing disproportionate number statistical errors face collecting due risks posed freedoms individuals explainability statistical accuracy part fairness considerations also include trade -off explainability statistical accuracy complex based deep may hard follow logic system therefore difficult adequately explain work sometimes characterised ‘ black problem ’ depending circumstances may view complex statistically accurate effective especially comes problems like image recognition may therefore think face trade -off explainability statistical accuracy however many applications simpler models perform well trade -offs explainability statistical accuracy may actually relatively small issues considered greater depth explain project guidance general point ‘ black ’ models • thoroughly considered potential impacts risks advance members team determined case organisational capacities/resources support responsible design implementation • system includes supplemental interpretability tools provide domain -appropriate level explainability explainability exposure personal commercial security providing individuals meaningful logic driven decision potentially increase risk inadvertently disclosing nee keep private – including personal also proprietary logic system –in process recent demonstrated proposed methods make ml models explainable unintentionally make easier infer personal individuals whose train model see 20200214 28 version 1.0 auditing framework -draft guidance consultation sections ‘ model inversion attacks ’ ‘ membership inference attacks ’ also highlights risk course providing explanation individuals may accidentally reveal proprietary model works however must take care conflate commercial interests protection requirements eg commercial security protection security nd instead consider extent trade -off genuinely exists stakeholder engagement far indicate risk quite low however theory least may cases need consider individuals receive explanation example interests businesses maintain trade secrets noting protection compliance ‘ traded away ’ risks areas likelihood severity subject debate investigation continue monitor review risks may update guidance accordingly manage trade- offs cases striking balance multiple trade -offs matter judgement specific case context system meant deployed whatever choices make need accountable efforts proportional risks system considering deploy poses individuals .. • identify assess existing potential trade -offs designing procuring system assess impact may individuals • consider available technical approaches minimise need trade -offs • consider techniques implement reasonable level investment effort • criteria lines accountability final trade -off decisions include robust risk -based independent approval process • appropriate take steps explain trade -offs individuals tasked reviewing outputs • review trade -offs regular basis taking account among things views individuals representatives emerging techniques best practices reduce 20200214 29 version 1.0 auditing framework -draft guidance consultation document processes outcomes auditable standard capture dpia appropriate level detail also document • considered risks individuals personal processed • methodology identifying assessing trade -offs scope reasons adopting rejecting particular technical approaches relevant • prio ritisation criteria rationale final decision • final decision fits within overall risk appetite also ready halt deployment possible achieve appropriate trade -off betw een two multiple protection requirements outsourcing third -party either buy solution third party outsource altogether need conduct independent evaluation trade -offs part due diligence process also required specify requirements procurement stage rather addressing trade -offs ex post recital 78 gdpr says producers solutions encouraged • take account protection developing designing • make sure controllers processors able fulfil protection obligations ensure authority modify deployment either directly via third party provider consider appropriate trade -offs outset risks considerations arise instance vendor may offer cv screening tool effectively scor es promising job candidates may ostensibly require lot candidate order make assessment procuring system need consider whether collecting much personal candida tes request provider modify system seek another provider see section minimisation 20200214 30 version 1.0 auditing framework -draft guidance consultation culture diversity engagement stakeholders need make significant judgement calls determining appropriate trade -offs effective risk management processes essential culture organisation also plays fundamental role undertaking kind exercise require collaboration different teams within organisation diversity incentives work collaboratively well environment staff feel encouraged voice concerns propose alternative approaches important social acceptability different contexts best practices relation trade -offs subject ongoing societal debates consultation stakeholders outside organisation including affected trade -off help understand value place different criteria assessing trade- offs worked example many cases trade -offs precisely quantifiable lead arbitrary decisions perform contextual assessments documenting justifying assumptions value different requirements specific cases one possible approach help identi fy best possible trade -offs visually represent choices different designs graph plot possible choices system could designed graph two criteria balanced – example statistical accuracy privacy – x axis statistical accuracy given precise measurement ways described section ‘ need statistical accuracy ’ privacy measures likely less exact indicative nature could include • amount personal required • sensitivity • extent might uniquely identify individual • nature scope context purpose processing • risks freedoms processing may present individuals • number individuals applied graph reveal known ‘ production -possibility frontier ’ one way help decision makers understand system design decisions may impact balance different values 20200214 31 version 1.0 auditing framework -draft guidance consultation method figure 1 visualise trade -off privacy statistical accuracy might look like presented senior decision maker responsible approving choice particular system help understand trade -offs figure 1 points graph represent different possible technical configurations system resulting different design choices ml models amount types point represents possible end result particular trade -off statistical accuracy privacy scenario figure 1 proposed achieve high statistical accuracy high privacy trade -off privacy statistical accuracy significant different case trade -offs may look different visualised figure 2 20200214 32 version 1.0 auditing framework -draft guidance consultation figure 2 scenario may easier achieve reasonable trade -off statistical accuracy privacy graph also shows cost sacrificing either privacy statistical accuracy lower middle curve c edge b example diminishing returns statistical accuracy possible intend choose system b would placing higher value statistical accuracy warranted circumstances need take account impact freedoms individuals able demonstrate processing still fair proportionate overall visual representati trade -offs also include lower limits either variable willing go figure 3 20200214 33 version 1.0 auditing framework -draft guidance consultation figure 3 scenario figure 3 possible system meets lower limits statistical accuracy privacy suggesting pursue deployment may mean looking methods dat sources reformulating problem abandoning attempt solve problem mathematical approaches minimise trade -offs cases precisely quantify elements trade -offs number mathematical co mputer science techniques known ‘ constrained optimisation ’ aim find optimal solutions minimising trade -offs technical specialist ml engineer assess viability techniques particular context instance theory differential privacy provides framework quantifying minimising trade -offs knowledge gained dataset statistical model privacy people similarly various methods exist create ml models optimise statistical accuracy also minimising mathematically defined measures discrimination 20200214 34 version 1.0 auditing framework -draft guidance consultation approaches provide theoretical guarantees hard meaningfully put practice many cases values like privacy fairness difficult meaningfully quantify example differential privacy may able measure likelihood individual uniquely identified particular dataset sensitivity identification therefore always supplement methods qualitative holistic approach inability preci sely quantify values stake mean avoid assessing justifying trade -off altogether still need choices example controls risk statement inadequate inappropriate trade -off analysis decisions lea incorrectly prioritise one criterion another important criteria preventative • clearly document purpose model important criteria model specification • ensure specification signed ppropriate management • senior management review various models trade -off analysis approve particular model • systematically review trade -off options provide justification specific model selected • ensure reviews completed action taken result • complete training ensure system designers date latest techniques detective • periodic review trade -off given available since date deployment • periodically re- analyse trade-offs corrective • select appropriate model include thorough justification change • retrain system developers 20200214 35 version 1.0 auditing framework -draft guidance consultation need ensure lawfulness fairness transparency glance process personal must ensure lawful fair transparent compliance principle may challenging context process personal various stages variety purposes risk fail appropriately distinguish distinct processing operation identify appropriate lawful basi could lead failure comply protection principle lawfulness section presents considerations help find appropriate lawful basis various kinds personal processing involved creatin g using ensure processing fair detail • principles lawfulness fairness transparency apply • identify purposes lawful basis using • need statistical accuracy • address risks bias discrimination principle lawfulness fairness transparency apply first deployment involve processing personal different ways different purposes must identify purposes appropriate lawful basis order comply w ith principle lawfulness second system infer people order processing fair need ensure • system sufficiently statistically accurate avoids discrimination • consider impact individuals ’ reasonable expectations 20200214 36 version 1.0 auditing framework -draft guidance consu ltation finally need transparent process personal system comply principle transparency core issues regarding transparency principle addressed explain guidance discussed n detail identify purposes lawful basis using consider deciding lawful bases whenever processing personal – whether train system make predictions using existing one – must appropriate lawful basis different lawful bases may apply depending particular circumstances however lawful bases may likely appropriate training deployment others sa time must remember • responsibility decide lawful basis applies processing • must always choose lawful basis closely reflects true nature relationship individual purpose processing • make determination start processing • document decision • swap lawful bases later date without good reason • must include lawful basis privacy notice along purposes • processing special categories need lawful basis additional condition processing reading – ico guidance read guidance lawful basis processing guide gdpr distinguish p urposes deployment many cases determining purpose lawful basis make sense separate training deployment distinct separate purposes different circumstances risks therefore consider whether different lawful bases apply deployment example need 20200214 37 version 1.0 auditing framework -draft guidance consu ltation • system trained general -purpose task deploy different contexts different purposes instance facial recognition system could trained recognise faces functionality could multiple purposes preventing crime authentication tagging friends social network applications might require different lawful basis • cases implement system third party processing personal undertaken developer different purpose th intend system therefore may need identify different lawful basis • processing personal purposes training model may directly affect individuals model deployed may aut omatically make decisions legal significant effects means provisions automated decision making apply result different range available lawful bases may apply training deployment stages following sections -related considerations gdpr ’ lawful bases consider part 3 dpa stage rely consent consent may appropriate lawful basis cases direct relationship individuals whose want process training deploying model however must ensure consent freely given specific informed unambiguous involves affirmative act part individuals advan tage consent lead trust buy -in individuals using service providing individuals control also factor dpias however consent apply individuals must genuine choice abo ut whether may implications depending intend – difficult ensure collect valid consent complicated processing operations example things want difficult ensure consent genuinely specific informed key individuals understand using personal consented instance want collect wide range features explore different models predict variety outcomes consent may appropriate lawful basis provided inform individuals activities obtain valid consent 20200214 38 version 1.0 auditing framework -draft guidance consultation consent may also appropriate lawful basis individual ’ deployment system eg purposes personalising service making prediction recommendation however aware consent valid individuals must also able withdraw consent easily gave relying consent basis processing wi th system deployment eg drive personalised ready accommodate withdrawal consent processing relevant provisions gdpr see articles 4 11 6 1 7 8 9 2 recitals 32 38 40 42 43 171 external link reading – ico guidance read guid ance consent guide gdpr reading -european protection b oard protection board edpb replaced article 29 working party wp29 includes representatives protection authorities member state adopts guidelines complying requirements gdpr wp29 adopted guidelines consent edpb endorsed may 2018. rely performance contract lawful basis applies processi ng using objectively necessary deliver contractual service relevant individual take steps prior entering contract individual ’ request eg provide -derived quote service less intrusive way processing provide service processing practice objectively necessary performance contract rely lawful basis processing furthermore even appropriate ground system may appropriate ground processing personal train system system perform well enough without trained individual ’ personal performance f contract depend processing 20200214 39 version 1.0 auditing framework -draft guidance consultation similarly even performance contract lawful basis processing personal provide quote prior contract mean also using train system also note unlikely able rely basis processing personal purposes ‘ service improvement ’ system cases collection personal service details users engage service functions within hat service objectively necessary provision contract service delivered without processing conversely process personal purposes personalising may regarded necessa ry performance contract – cases whether processing regarded ‘ intrinsic ’ service depends • nature service • expectations individuals • whether provide service without processing ie personalisation means system integral service consider alternative lawful basis relevant provisions legislation see article 6 1 b recital 44 gdpr external link reading – ico guidance read guidance contracts guide gdpr rea ding – protection board protection board edpb replaced article 29 working party wp29 includes representatives protection authorities member state adopts guidelines complyi ng requirements gdpr edpb published guidelines 2/2019 processing personal article 6 1 b context online services november 20200214 40 version 1.0 auditing framework -draft guidance consultation rely legal obligation task vital interests examples system process personal may legal obligation purposes detecting anti -money laundering part 7 proceeds crime act similarly organis ation uses part exercise official authority perform task interest set law necessary processing personal involved may based grounds limited number cases processing p ersonal system might based protecting vital interests individuals example emergency medical diagnosis patients otherwise incapable providing consent eg processing fmri scan unconscious patien diagnostic system however unlikely vital interests could also provide basis training system would rarely directly immediately result protecting vital interests individuals even mo dels eventually built might later save lives training potentially life -saving would better rely lawful bases relevant provisions legislation see article 6 1 c recitals 41 45 gdpr provisions using legal obligation external link see article 6 1 e 6 3 recitals 41 45 50 gdpr provisions using interests see article 6 1 article 9 2 c recital 46 gdpr provisions using vital interests external link see sections 7 8 schedule 1 paras 6 7 protection act external link reading – ico guidance read guidance legal obligation vital interests task guide gdpr rely legitimate interests depending circumstances could base processing personal training ongoing legitimate interests lawful basis 20200214 41 version 1.0 auditing framework -draft guidance consultation important note legitimate interests flexible lawful basis processing always appropriate example way intend people ’ would unexpected cause unnecessary harm also means taking additional responsibility considering protecting people ’ interests additionally authority rely legitimate interests processing legitimate reason performing tasks authority three elements legitimate interests lawful basis help think ‘ three -part test ’ need • identify legitimate interest ‘ pur pose test ’ • show processing necessary achieve ‘ necessity test ’ • balance individual ’ interests freedoms ‘ balancing test ’ wide range interests constitute ‘ legitimate inter ests ’ protection law third parties well commercial societal interests however key understanding legitimate interests may flexible comes additional responsibilities requires assess impact processing individuals able demonstrate compelling benefit processing address document considerations part legitimate interests assessment li example organisation seeks rely legitimate interests processing personal purposes training machine model legitimate interests may allow organisation room experiment different variables model however part legitimate interests assessment organisation demonstrate range variables models intends reasonable approach achieving outcome best achieve properly defining purposes justifying type collected – allow organisation work necessity balancing aspects lia example mere p ossibility might useful prediction sufficient organisation demonstrate processing necessary building model 20200214 42 version 1.0 auditing framework -draft guidance consultation relevant provisions legislation see gdpr article 6 1 f recitals 47 -49 external link reading – ico guidance read guidance legitimate interests guide gdpr also published lawful basis assessment tool help decide basis appropriate well legitimate interests template word special category criminal offences intend proces special category criminal offences need ensure comply requirements articles 9 10 gdpr well dpa 2018. special category personal needs protection sensitive order process need lawful basis article 6 well separate condition article 9 although linked number conditions also require meet additional requirements safeguards set schedule 1 dpa 2018. must • determine document condition processing start • ensure appropriate policy document place required • complete dpia processing likely high risk criminal offences need lawful basis article 6 gdpr either lawful official authority article 10. dpa sets specific conditions provide lawful authority also process type official authority ie processing official capacity special category must determine condition processing identify official authority start also document 20200214 43 version 1.0 auditing framework -draft guidance consultation relevant provisions legislation see articles 9 10 gdpr external link reading – ico guida nce read guidance special category criminal offence guide gdpr impact article 22 gdpr protection law applies automated individual decision making profiling article 22 gdpr additional rules protect individuals carrying solely automated decision- making legal similarly significant effects may application context eg using system make kinds decisions however carry type decision- making decision • necessary entry performance contract • authorised law applies • based individual ’ explicit consent therefore identify processing falls article 22 make sure • give individuals informatio n processing • introduce simple ways request intervention challenge decision • carry regular checks make sure working intended reading – ico guidance read guidance related automated decision maki ng including profiling guide gdpr 20200214 44 version 1.0 auditing framework -draft guidan ce consultation example controls risk statement reliance inappropriate lawful basis processing results potential failure fulfil necessary requirements non -compliance dp legislation preventative • ensure system developers completed training associated competency assessments • document training key stakeholders relevant personnel identified eg senior management risk managers audit • thoroughly assess lawful basis processing dpia • consult dp specialists within model design workforce • ensure requirement dpia documented developers provided guidance assessment criteria • complete legitimate interests assessment reliance legitimate interests lawful basis detective • monitor individual requests complaints indiv iduals including action taken result individual level boarder analysis • conduct periodic dpia review ensure remains accurate date • periodically assess model usage ensure purpose remains necessit legitimate interests li still valid • conduct periodic review records processing ensure validity lawful basis corrective • implement corrective measures system order satisfy original lawful basis • select lawful basis associated actions example carrying legitimate interests assessment obtaining consent • retrain system developers individuals involved assessment lawful bases 20200214 45 version 1.0 auditing framework -draft guidance consultation need statistical accuracy statistical accuracy refers proportion answers system gets correct incorrect section explains controls implement ensure sufficiently statistically accurate ensure personal process complies fairness principle difference ‘ accuracy ’ protection law nd ‘ statistical accuracy ’ said section ‘ trade -offs manage ’ accuracy slightly different meanings protection contexts protection accuracy one fundamental principles requires take reasonable steps make sure personal process ‘ incorrect misleading matter fact ’ necessary corrected deleted without undue delay accuracy refers often sys tem guesses correct answer many contexts answers system provides personal instance system might infer someone ’ demographic interests behaviour social network protectio n ’ accuracy principle applies personal whether individual input system output system however mean system needs 100 statistically accurate order comply accuracy principle many cases outputs system intended treated factual individual instead intended represent statistically informed guess something may b e true individual future order avoid personal misinterpreted factual ensure records indicate statistically informed guesses rather facts records also clude provenance system generate inference also record becomes inference based inaccurate system generate statistically flawed way may affected quality inference similarly processing incorrect inference may impact individual may request inclusion additional record countering incorrect infe rence helps ensure decisions taken basis potentially incorrect inference informed evidence may wrong 20200214 46 version 1.0 auditing framework -draft guidance consultation gdpr mentions statistical accuracy context profiling automated decision making recital 71. states organisations put place ‘ appropriate mathematical statistical procedures ’ profiling individuals part technical measures ensure factors may result inaccuracies personal corrected risk errors minimised system make inferences people need ensur e system sufficiently statistically accurate purposes mean every inference correct need factor possibility incorrect impact may decisions may take basis failure could mean processing compliant fairness principle may also impact compliance minimisation principle personal – including inferences – must adequate relevant purpose system therefore needs sufficiently statistically accurate ensure personal generated processed lawfully fairly however overall statistical accuracy particularly useful measure usually needs broken different measures important measure prioritise ones see next section relevant provisions legislation see gdpr articles 5 1 22 recital 71 external link reading – ico guidance read guidance accuracy guide gdpr well guidance rectification erasure reading – protection board protection board edpb replaced article 29 working party wp29 includes representatives protection authorities member state adopts guidelines complying requirements gdpr wp29 published guidelines automated decision making profiling 2017. edpb endorse guidelines may 20200214 47 version 1.0 auditing framework -draft guidance consultation define prioritise different statistical accuracy measures statistical accuracy general measure closely system ’ predictions match correct labels defined test example system classify emails spam spam simple measure statistical accuracy number emails correctly classified spam spam proportion emails analysed however measure could misleading instance 90 emails received inbox spam could create 90 ccurate classifier simply labelling everything spam would defeat purpose classifier genuine email would get reason alternative measures assess good system measures shoul reflect balance two different kinds errors • false positive ‘ type ’ error cases system incorrectly labels positive eg emails classified spam genuine • false negative ‘ type ii ’ error cases system incorrectly labels negative actually positive eg emails classified genuine actually spam important strike balance two types errors mor e useful measures reflect two types errors including • precision percentage cases identified positive fact positive also called ‘ positive predictive value ’ instance nine 10 emails classified spam actually spam precision system 90 • recall sensitivity percentage cases fact positive identified instance 10 100 emails actually spam system identifies seven recall 70 trade -offs precision recall assessed using measures ‘ f -1 ’ statistic -see link place importance finding many positive cases possible maximising recall may come cost false positives lowering precision addition may important differences consequences false positives false negatives individuals 20200214 48 version 1.0 auditing framework -draft guidance consultation example cv filtering system selecting qualified candidates interview produces false positive unqualified candidate invited interview wasting employer applicant ’ time unnecessarily produces false negative qualified candidate miss employment opportunity organisation miss good candidate prioritise avoiding certain kinds error based severity nature risks general statistical accuracy measure depends possible compare performance system ’ outputs ‘ ground truth ’ ie checking results system real world instance medical diagnostic tool designed detect malignant tumours could evaluated high quality test containing known patient outcomes areas ground truth may unattainable could high -quality test exists trying predict classify subjective eg whether social media post offensive risk statistica l accuracy misconstrued situations seen highly statistically accurate even though reflecting average set labellers thought rather objective truth avoid record indicate outputs intended reflect objective facts decisions taken basis personal reflect limitations also example must take account accuracy principle – see guidance accuracy principle refers accuracy opinions finally statistical accuracy measure usually measured test real life situations applie changing populations system statistically accurate existing population ’ eg customers last year may continue perform well change characteristics population population system applied future behaviours may change either accord adapting response system system may become less statistically accurate time phenomenon ref erred machine ‘ concept model drift ’ various methods exist detecting instance measure distance classification errors time increasingly frequent errors may suggest drift regularly assess drift retrain model necessary part accountability decide document 20200214 49 version 1.0 auditing framework -draft guidance consultation appropriate thresholds determining whether model needs retrained based nature scope context purposes processing risks poses example model scoring cvs part recruitment exercise kinds skills candidates need particular job likely change every two years anticipate assessing need re- train fresh least often application domains main features ’ change often eg recognising handwritten digits anticipate less drift need assess based circumstances reading – ico guidance see guidance accuracy principle guide gdpr resources see ‘ define prioritise different statistical accuracy measures ’ see ‘ concept drift overview ’ explanation concept drift always think carefully start whether appropriate automate prediction decision- making process include assessing effectiveness system making statistically accurate predictions individuals whose personal processes assess merits using particular system light consideration effectiveness making accurate therefore valuable predictions demonstrate sufficient level statistical accuracy decide adopt system comply protection principles • ensure functions individuals responsible testing validation deployment monitoring adequately trained understand associated statistical accuracy requirements measures • make sure clearly labelled inferences predictions claimed factual • ensure managed trade -offs nd reasonable expectations 20200214 50 version 1.0 auditing framework -draft guidance consultation • adopt common terminology staff discuss statistical accuracy performance measures including limitations adverse impact individuals else part obligation implement protection design default consider statistical accuracy appropriate measures evaluate design phase test measures throughout lifecycle deployment implement monitoring frequency proportional impact incorrect output may individuals higher impact frequently monitor report also review statistical accuracy measures regularly mitigate risk concept drift change policy procedures take account outset statistical accuracy also important consideration outsource system third party either fully partially purchase solution external vendor cases examine test claims made third parties part procurement process similarly agree regular updates reviews statistical accuracy guard changing population concept model drift provider services ensure designed way allow organisations fulfil protection obligations finally vast quantity personal may hold process part likely put pressure pre -existing non-ai processes identify necessary rectify/delete inaccurate personal whether input training/test therefore need rev iew governance practices ensure remain fit purpose example controls risk statement inaccurate output decisions made could lead unfair negative outcomes individuals failure meet fairness principle preventative • put place governance framework describes personal ongoing training testing evaluation system service correct accurate relevant representative complete up-to-date possible 20200214 51 version 1.0 auditing framework -draft guidance consultation • provide training key stakeholders document relevant personnel identified eg senior management risk managers audit • document access management controls segregation duties deployment ensure changes affecting statistical accuracy made signed authorised persons maintain evidence controls monitored • document levels approval authority development/use maintain evi dence appropriate approval • dpia include thorough assessment impact/importance different errors • maintain documented policies processes dealing third parties evidence due diligence completed particular procuring services ensure meet statistical accuracy requirements allow regular re-testing • maintain document policy process performing pre implementation testing changes prior go -live maintain ev idence testing completed prior deployment system results test detective • post -implementation testing document results testing action taken result • monitor output reports/performa nce expectations • conduct review sample decisions statistical accuracy including sample selected criteria • document individual requests complaints regarding statistically inaccurate outputs individuals particular relating article 22 including action taken result individual level broader analysis • ensure continuous oversight third -party suppliers/processors including regularl reviewing performance expectations adherence contractual requirements • test system set confirm outcome reached corrective • retrain system eg improving input different balance false p ositives negatives using different algorithm • retrain system developers relation discriminatory model performance • change decision made assess whether individuals could impacted inaccu racy 20200214 52 version 1.0 auditing fr amework draft guidance consultation ddress risk bias discrimination system learn may unbalanced and/o r reflect discrimination may produce output discriminatory effects people based gender race age health religion disability sexual orientation r characteristics fact learn guarant ee outputs lead discriminatory effects trai n test w ell w ay designed ight lead system treat certain groups less favourably protectio n law ntended balance righ protection personal function society processing personal leads discrimination bias impact fairne ss processing poses compliance issues fairness principle wel l risks individuals ’ freedoms – including non-d iscrimination furthermore gdpr specificall notes organisations hould take measure prevent ‘ discriminatory effect natural persons ’ additionally uk ’ ant i-discrimination legal framework notably uk equality act sits longside protection law app lies range organisations thi include government departments service providers employers education providers transport providers association membership bodies well providers publi c functions gives individuals protection fr om discrimination whether generated automate decision- making system combination two section w e explor e means practice context focusing machine ml system cl assify r make prediction abo ut individuals ay lead discrimination also explore f technical nd organisational measures hat yo u ado pt manage r isk ight n system lead discrimination let ’ take hypothetical scenario example bank develops system calculate credit risk potential customers bank system approve reject loan applications system trained large dataset containing range previous borrowers occupation income age whether repaid loan testing bank wants check possible gender bias finds system tends give women lower credit scores 20200214 53 version 1.0 auditing framework -draft guidance consultation two main reasons might one imbalanced training proportion different genders training may balanced example training may include greater proportion male borrowers past fewer women applied loans therefore bank ’ enough women algorithm generate statistical model designed best fit trained tested men -represented training model pay attention statistical relationships predict repayment rates men less statistical patterns predict repayment rates women might different put another way statistically ‘ less important ’ model may systematically predict lower loan repayment rates women even women training dataset average likely repay loans men issues apply population under- represented training example facial recognition model trained disproportionate number faces belonging particular ethnicity gender eg white men perform better recognising individuals group worse others another reason training may reflect past discrimination instance past loan applications wome n rejected frequently men due prejudice model based training likely reproduce pattern discrimination certain domains discrimination historically significant problem likely experience problem acutely police stop -and-search young black men recruitment traditionally male roles issues occur even training contain protected characteristics like gender race variety features training often closely correlated protected characteristics eg occupation ‘ proxy variables ’ enable model reproduce patterns discrimination associated characteristics ev en designers intend problems occur statistical model however likely occur include greater number features may identify complex combinations features proxies protected characteristics many modern ml methods powerful traditional statistical approaches better 20200214 54 version 1.0 auditing framework -draft guidance consultation uncovering non -linear patterns high dimensional however also include patterns reflect discrimination technical approaches mitigate discrimination risk ml models discrimination b roader problem realistically ‘ ’ various approaches mitigate -driven discrimination computer scientists others developing different mathematical techniques measure ml models treat individuals different groups potentially discriminatory ways field often referred algorithmic ‘ fairness ’ many techniques early stages may market- ready basic approac hes developers take measure mitigate potential discrimination resulting cases imbalanced training may possible balance adding removing under/overrepresented subsets population eg adding points loan applications women alternatively could train separate models example one men another women design perform well possible sub-group however cases creating different models different protected classes could violation non -discrimination law eg different car insurance premiums men women cases training reflects past discrimination could either modify change process modify model training order techniques effective need choose one mathematical ‘ fairness ’ measures measure results measur es grouped three broad categories • anti-classification • outcome error parity • equal calibration anti-classification model fair excludes protected characteristics making classification prediction anti classification approaches also try identify exclude proxies protected characteristics eg attendance single -sex school impractical removing possible proxies may leave predictively useful features also ften hard know whether particular variable combination variables proxy protected characteristic without collection analysis 20200214 55 version 1.0 auditing framework -draft guidance consultation outcome error parity compares members different protected groups treated model outcome parity model fair gives equal numbers positive negative outcomes different groups error parity model fair gives equal numbers errors different groups error parity broken parity false positives false negatives see section 2.3 statistical accuracy details equal calibration – calibration measures closely model ’ estimation likelihood something happening matches actual frequency event happening according ‘ equal calibration ’ model fair equally calibrated members different pro tected groups instance classification model sorts loan applicants low medium high chance repayment equal proportions male female applicants actually repay within risk category ’ mean equal proportions men women across different risk categories instance women actually higher repayment rates men may women men low risk category unfortunately different measures may often incompatible therefore need consider conflicts carefully selecting particular approach es example • equal calibration incompatible outcome error parity except rare cases actual distribution outcomes equal different protected groups • attempting achieve outcome parity removing protected characteristics required anti -classification measures may result algorithm finding using irrelevant proxies order create model equalises outcomes may unfair process special category assess address discrimination techniques discussed require access dataset containing personal representative sample population person represented need labels protected characteristics interest outlined equality act 2010. cou ld dataset containing protected characteristics test system performs protected group also potentially -train model performs fairly kind analysis need ensure appropriate lawful basis process purposes different protection considerations depending kinds discrimination testing testing system discriminatory impact age sex gender special protection conditions processing protected characteristics 20200214 56 version 1.0 auditing framework -draft guidance consultation classifi ed ‘ special category ’ protection law still need consider • broader questions lawfulness fairness risks processing poses whole • possibility either special category anyway becoming processing ie processing involves analysing inferring health genetic status also note dealing personal results specific technical processing physical physiological behavioural characteristics individual allows confirms individual ’ unique identificat ion biometric biometric purpose uniquely identifying individual also special category system uses biometric testing mitigating discrimination pur pose confirming identity individuals within dataset making kind decision relation biometric come article 9. still regarded biometric gdpr special category similarly personal allow confirm individual ’ unique identification biometric special category however protected characteristics outlined equality act classified special category include race religion belief sexual orientation may also include disability pregnancy gender reassignment far may reveal person ’ health similarly civil partnerships recently available -sex couples indicates someone civil partnership may indirectly reveal sexual orientation testing system discriminatory impact basis characteri stics likely need process special category order lawfully addition lawful basis article 6 need meet one conditions article 9 gdpr also require additional basis authorisation uk law found schedule 1 dpa 2018. conditions processing special category appropriate depends individual circumstances example using special category ssess discrimination identify promote maintain equality opportunity organisation using cv scoring system assist recruitment decisions needs test whether system discriminating religious philosophical belie fs 20200214 57 version 1.0 auditing framewor k -draft guidance consultation collects religious beliefs sample job applicants order assess whether system indeed producing disproportionately negative outcomes erroneous predictions organisation relies substantial interest condition article 9 2 g equality opportunity treatment condition schedule 1 8 dpa 20 18. provision identify keep review existence absence equality opportunity treatment certain protected groups view enabling equality promoted maintained example using special category assess discrimination purposes university researcher investigating whether facial recognition available market perform differently faces people different racial ethnic origin part project order researcher assigns racial labels existing dataset faces system tested thereby processing special category dat a. rely archiving statistics condition article 9 2 j read schedule 1 paragraph 4 dpa 2018. finally protected characteristics using assess improve potentially discriminatory origin ally processed different purpose consider • whether purpose compatible original purpose • obtain fresh consent required example initially collected basis consent even purpose compatible still need collect fresh consent purpose • purpose incompatible ask consent relevant provisions legislation see article 9 recitals 51 56 gdpr external link see schedule 1 dpa exte rnal link reading – ico guidance read guidance purpose limitation special category guide gdpr 20200214 58 version 1.0 auditing framework -draft guidance con sultation special category discrimination automated decision -making using special category assess potential discriminatory impacts usually constitute automated decision- making protection law involve directly making decisions individuals similarly -training discriminatory model diverse population order reduce discriminatory effects involve directly making decisions individuals therefore classed decision legal similarly significant effect however cases simply -training model diverse training set may enough sufficiently mitigate discriminatory impact rather trying make model fair ignoring protected characteristics making prediction approaches directly include characteristics making classification order ensure members potentially disadvantaged groups protected instance using system sort job applicants rather attempting create model ignores pers ’ disability may effective include disability status order ensure system discriminate including disability status input automated decision could mean system likely discriminate people disability factor effect condition features make prediction approach amounts making decisions individuals solely automated way significant effects using special category prohibited gdpr unless explicit consent individual meet one substantial interest conditions laid schedule 1 dpa need carefully assess whi ch conditions schedule 1 may apply example equality opportunity monitoring provision mentioned relied contexts processing carried purposes decisions particular individual therefore approaches lawful based different substantial interest condition schedule 1. accidentally infer special category many contexts non -protected characteris tics postcode live proxies protected characteristic like race recent advances machine ‘ deep ’ made even easier detect patterns world reflected seemi ngly unrelated unfortunately also includes detecting patterns discrimination using complex combinations features might correlated protected characteristics non -obvious ways 20200214 59 version 1.0 auditing framework -draft guidance consultation instance system score job applications assist decision maker recruitment decisions might trained examples previously successful candidates contained application may include protected characteristics like race disability mental health however examples employees train model discriminated grounds eg systematically rated performance reviews algorithm may learn reproduce discrimination inferring characteristics proxy contained job application despite designer never intending eve n ’ protected characteristics model possible may inadvertently model detected patterns discrimination based protected characteristics reproducing outputs described protected characteristics also special category special category defined personal ‘ reveals concerns ’ special categories model learns particular combinations features ufficiently revealing special category model may processing special category stated guidance special category profiling intention inferring special category special catego ry irrespective whether inferences incorrect furthermore reasons stated may also situations model infers special category intermediate step another non-special -category inference may able tell model looking went model outputs produces may high accuracy even though intend using machine personal proactively assess chances model might inferring protected characteristics and/or special category order make predictions actively monitor possibility throughout lifecycle system potentially inferred characteristics special category ensure appropriate article 9 condition processing noted model make legal similarly significant decisions solely automated way lawful person ’ consent meet substantial interest condition appropriate provision schedule 1 reading – ico guidance read guidance special category 20200214 60 version 1.0 auditing framework -draft guidance consultation mitigate risks appropriate approach managing risk discriminatory outcomes ml depend particular domain context operating determine document approach bias discrimination mitigation beginning application lifecycle take account put place appropriate safeguards technical measures design build phase establishing policies good practices procurement lawful processing high -quality training test important especially enough internally whether procured internally externally satisfy representative population apply ml system although reasons stated sufficient ensure fairness example high street bank operating uk training could compared recent census senior management responsible signing -off chosen approach manage discrimination risk accountable compliance protection law able leverage expertise leads internal external subject matter experts accountable senior leaders still need sufficient understanding limitations advantages different approaches also true dpos senior staff oversight functions expected provide ongoing advice guidance appropriateness measures safeguards put place mitigate discrimination risk many cases choosing betwe en different risk management approaches requires trade- offs see section ‘ -related trade-offs manage ’ includes choosing safeguards different protected characteristics groups need document approach choose trade- offs driven technical approaches always obvious non technical staff scientists highlight explain proacti vely business owners well staff responsibility risk management protection compliance technical leads also proactive seeking domain -specific knowledge including known proxies protected characteristics inform algorithmic ‘ fairness ’ approaches undertake robust testing anti -discrimination measures monitor ml system ’ performance ongoing basis risk management policies clearly set process person responsible final validation ml system deployment appropriate update 20200214 61 version 1.0 auditing framework -draft guidance consultation discrimination monitoring purposes organisational policies set variance tolerances selected key performance metrics well escalation variance investigation procedures also clearly set variance limits ml system stop replacing traditional decision -making consider running concurrently period time investigate significant difference type de cisions eg loan acceptance rejection different protected groups two differences system predicted perform practice beyond requirements protection law diverse workforce powerful tool identifying managing bias discrimination organisation generally finally area best practice technical approaches continue develop invest time res ources ensure continue follow best practice staff remain appropriately trained ongoing basis cases may actually provide opportunity uncover address existing discrimination traditional decision -making proce sses allow address underlying discriminatory practices resources equality act external link charter fundamental external link example controls risk statement discriminatory output decisions made could lead statistically inaccurate /unfair decisions individuals certain groups preventative • put place governance framework describes personal ongoing training testing evaluation system correct accura te relevant representative complete -todate possible • ensure developers completed training associated competency assessments identify address bias discrimination • provide training key stakeholders document relevant personnel identified eg senior management risk managers audit 20200214 62 version 1.0 auditing framework -draft guidance consultation • document access management controls segregation duties deployment ensure changes affecting statistical accuracy made signed authorised persons maintain evidence controls monitored • document levels approval authority development/use maintain evidence appropriate approval • dpia include thorough assessment risk f discrimination mitigants controls place prevent • maintain documented policies processes dealing third parties evidence due diligence completed • maintain documented process cross- section peer review system design maintain evidence review completed • maintain documented policy process performing pre implementation testing changes prior go -live maintain evidence testing completed results test • document levels approval attestation diversity representation training test prior within system maintain evidence appropriate approval detective • regularly monitor algorithmic fairness using appropriate measures • document levels approval attestation diversity representation training test prior within system maintain evidence appropriate approval • regularly review model performance recent corrective • add remove overrepresented groups including thorough analysis justification • retrain model fairness constraints • retrain model designers relation discriminatory model performance 20200214 63 version 1.0 auditing framework -draft guidance consultation assess security minimisation glance exacerbate known security risks make difficult manage also present challenges compliance minimisation principle exacerbate known security risks make difficult manage two secu rity risks increase • potential loss misuse large amounts personal often required train • potential software vulnerabilities introduced result introduction -relate code infrastructure default standard practices developing deploying involve processing large amounts risk fails comply minimisation principle number techniques exist enable minimisation effective deployment detail • security risks introduce • types privacy attacks apply models • steps take manage risks privacy attacks models • minimisation privacy -preserving techniques available security risks introduce must process personal manner ensures appropriate levels security unauthorised unlawful processing accidental loss destruction damage section way adversely affect security making known risks worse challenging control security requirements “ one -size-fits-all ” approach security appropriate security measures adopt depend level type risks arise specific processing activities 20200214 64 version 1.0 auditing framework -draft guidance consultation using process personal important implications security risk profile need assess manage carefully implications may triggered introduction types risks eg adversarial attacks machine models see section x reading – ico guidance read guidance security guide gdpr ico/ncsc security outcomes general security protection law security key component auditing framework also central work regulator ico planning expand general security guidance take account additional requirements set gdpr guidance ai- specific cover range topics relevant organisations using including software supply chain security increasing open-source software ’ different security compared ‘ traditional ’ technologies unique characteristics mean compliance protection law ’ security requirements challenging established technologies technological perspective technological perspective introduce kinds complexity found traditional may using depending circumstances also likely rely heavily third party code and/or relationships suppliers also existing need integrated several existing components also intricately connected complexity may make difficult identify manage security risks may increase others risk outages perspective people involved building deploying likely wider range backgrounds usual including traditional software engineering administration scientists statisticians well domain experts security practices expectations may vary significantly may less understanding broader security compliance requirements well protection law specifically security personal may always key priority especially someone previously building applications non -personal capacity 20200214 65 version 1.0 auditing framework -draft guidance consultation complications arise common practices process personal securely science engineering still part compliance security principle ensure actively monitor take account state -ofthe-art security practices using personal context possible list known security risks might exacerbated process personal impact security depends • way built deployed • complexity organisation deployin g • strength maturity existing risk management capabilities • nature scope context purposes processing personal system risks posed individuals result following hypothetical scenar ios intended raise awareness known security risks challenges exacerbate key message review risk management practices ensuring personal secure context case study losing track training ml require large sets training testing copied imported original context processing shared stored variety formats places including third parties make difficult keep track manage example organisation decides system offered third -party recruiter part hiring process effective organisation needs share imilar previous hiring decisions eg sales manager recruiter previously organisation entirely manual cv scanning process led sharing personal eg candidates ’ cvs involve transfer large q uantities personal organisation recruiter organisation must ensure appropriate lawful basis processing beyond sharing additional could involve creating multiple copies different formats stored different locations see require important security governance considerations 20200214 66 version 1.0 auditing framework -draft guidance consultation • organisation may need copy hr recruitment separate database system examine select relevant vacancies recruitment firm working • selected subsets need saved exported files transferred recruiter compressed form • upon receipt recruiter could upload files remote location eg cloud • cloud files may loaded programming environment cleaned building system • ready likely saved file later time • organisation recruiter time copied stored different places increased risk personal breach including unauthorised processing loss destruction damage example copies training need shared managed necessary deleted line security policies many recruitment firms already governance security policies place may longer fit -for-purpose adopted reviewed necessary updated circumstance technical teams record document movements storing personal one location another help apply appropriate security risk controls monitor effectiveness audit trails also necessary satisfy accountability documentation requirements addition delete intermediate files containing personal soon longer required eg compressed versions files created transfer depending likelihood severity risk individuals may also need apply de -identification techniq ues training extracted source shared internally externally example may need remove certain features apply privacy enhancing technologies pets sharing another organisation case study security risks introduced externally maintained software build organisations build entirely in- house cases design building running provided least part third parties organisation may always contractual relationship 20200214 67 version 1.0 auditing framework -draft guidance consultation even hire ml engineers may still rely significantly third-party frameworks code libraries many popular ml frameworks open source using third- party open source code valid option developing software components system scratch requires large investment time resources many organisations afford especially compared open source tools would n ot benefit rich ecosystem contributors services built around existing frameworks however one important drawback standard ml frameworks often depend pieces software already installed system give sense risks involved recent study found popular ml frameworks include 887,000 lines code rely 137 external dependencies therefore implementing require changes organisation ’ software stack possibly hardware may introduce additional security risks example recruiter hires ml engineer build automated cv filtering system using python- based ml framework ml framework depends number specialist open- source programming libraries needed downloaded recruiter ’ system one libraries contains software function convert raw training format required train ml model later discovered function security vulnerability due unsafe default configuration attacker introduced executed malicious code remotely system disguising training far -fetched example january vulnerability discovered ‘ numpy ’ popular library python programming language many machine develo pers circumstance whether built house externally combination need assess security risks well ensuring security code developed in- house need assess security externally maintained code frameworks many respects standard requirements maintaining code managing security risks apply applications example • external code security measures inclu de subscribing security advisories notified vulnerabilities • internal code security measures include adhering coding standards instituting source code review processes 20200214 68 version 1.0 auditing framework -draft guidance consultation whatever approach need ensure staff appropriate skills knowledge address security risks additionally develop ml mitigate security risks associated third party code separating ml environment rest infrastructure possible two ways achieve • using ‘ virtual machines ’ ‘ containers ’ -emulations computer system run inside isolated rest system pre- configured specifically ml tasks recruitment example ml engineer virtual machine vulnerability could contained • many ml developed using programming languages well -developed scientific machine uses like python necessarily secure however possible train ml model using one programming language eg python deployment convert model another language eg java makes making insecure coding less likely return recruitment example another way ml engineer could mitigated risk malicious attack cv filtering model would convert model different programming language prior deployment reading – ico guidance read report protecting personal online services mistakes others although written report ’ area may still assist ico developing security guidance include additional recommendations oversight review externally maintained source code protection perspective well implications security protection design resources guidance national cyber security centre ncsc maintaining code repositories may also assist types privacy attacks apply models personal people system trained might inadvertently revealed outputs system 20200214 69 version 1.0 auditing framework -draft guidance consultation normally assumed personal individuals whose train system inferred simply observing predictions system returns response inputs however types privacy attacks ml models suggest sometimes possible update two kinds privacy attacks – ‘ model inversion ’ ‘ membership inference ’ model inversion attacks model inversion attack attackers already access personal belonging specific individuals included training infer personal individuals observing inputs outputs ml model attackers learn goes beyond generic inferences individuals similar characteristics figure 1. illustration model inversion membership inference attacks reproduced veale et al 'algorithms remember model inversion attacks protection law example one – model inversion attack early demonstration kind attack concerned medical model designed predict correct dosage anticoagulant using patient including genetic biomarkers proved attacker access demographic individuals included training could infer genetic biomarkers model despite access underlying training 20200214 70 version 1.0 auditing framework -draft guidance consultation example two – model inversion attack another recent example demonstrates attackers could reconstruct images faces facial recognition frt system trained recognise frt often designed allow third parties query model model given image person whose face recognises model returns best guess name person associated c onfidence rate attackers could probe model submitting many different randomly generated face images observing names confidence scores returned model could reconstruct face images associated individuals inc luded training reconstructed face images imperfect researchers found could matched reviewers individuals training 95 accuracy see figure 2 figure 2. face image recovered using model inversion attack corresponding training set image fredriksen et al. 'model inversion attacks exploit conﬁdence ’ resources ‘ algorithms remember model inversion attacks protection law ’ simple demographics often identify people uniquely ‘ model inversion attacks exploit confidence informatio n basic countermeasures ’ membership inference attacks membership inference attacks allow malicious actors deduce whether given individual present training ml model however 20200214 71 version 1.0 auditing framework -draft guidance consultation unlike model inversion ’ necessarily learn additional personal individual instance hospital records train model predicts patient discharged attackers could model combination particular individual already work part training would reveal individual ’ training set practice would reveal visited one hospitals generated training period collected similar earlier frt example membership inference attacks exploit confidence scores provided alongside model ’ prediction individual training model disproportionately confident prediction tha person seen allows attacker infer person training gravity consequences models ’ vulnerability membership inference depend sensitive revealing membership migh model trained large number people drawn general population membership inference attacks pose less risk model trained vulnerable sensitive population eg patients dementia hiv merel revealing someone part population may serious privacy risk black white attacks important distinction ‘ black ’ ‘ white ’ attacks models two approaches correspond differe nt operational models white attacks attacker complete access model inspect underlying code properties although training example providers give third parties entire pre trained model allow run locally white attacks enable additional gathered – type model parameters – could help attacker inferring personal model black attacks atta cker ability query model observe relationships inputs outputs example many providers enable third parties access functionality ml model online send queries containing input receive model ’ response examples highlighted black attacks white black attacks performed providers ’ customers anyone else either authorised unauthorised access either model que ry response functionality respectively 20200214 72 version 1.0 auditing framework -draft guidance consultation models include training design model inversion membership inferences show models inadvertently contain personal also note certain kinds ml models actually contain parts training raw form within design instance ‘ support vector machines ’ svms ‘ k-nearest neighbours ’ knn models contain training model cases training personal access model means organisation purchasing model already access subset personal contained training without exert efforts providers ml models third parties procuring aware may co ntain personal way unlike model inversion membership inference personal contained models like attack vector personal contained models would design easily retrievable third party storing using models therefore constitutes processing personal standard protection provisions apply steps take manage risks privacy attacks models train models provide others assess whether models may contain personal risk revealing attacked take appropriate steps mitigate risks assess whether training contains identified identifiabl e personal individuals either directly may access model assess means may reasonably likely light vulnerabilities described rapidly developing area hould stay up- to-date state art methods attack mitigation security ml researchers still working understand factors make ml models less vulnerable kinds attacks design effective protections mitigation strategies one possible cause ml models vulnerable privacy attacks known ‘ overfitting ’ model pays much attention details training effectively almost remembering parti cular examples training rather general patterns model inversion membership inference attacks exploit avoiding overfitting help mitigating risk privacy attacks also ensuring model able make good inferences examples ’ seen however avoiding overfitting 20200214 73 version 1.0 auditing framework -draft guidance consultation completely eliminate risks even models overfitted training still vulnerable privacy attacks cases confidence provided ml system exploited frt example risk could mitigated providing end user would need balanced need genuine end users know whether rely output depend particular case context going provide whole model others via application programming interface api would subject white -box attacks way api ’ users would direct access model however might still subjected black attacks mit igate risk could monitor queries api ’ users order detect whether suspiciously may indicate privacy attack would require prompt investigation potential suspension blocking particular user account measures may become part common real- time monitoring techniques protect security threats ‘ rate -limiting ’ reducing number queries performed particular user given time limit model going provided whole third party rather merely accessible via api need consider risk ‘ white ’ attacks model provider less easily able monitor model deployment thereby assess mitigate risk privacy attacks however remain responsible ensuring personal train models exposed result way clients deployed model may able fully assess risk without collaborating clients understand particular deployment contexts associated threat models part procurement policy sufficient sharing party perform respective assessments necessary cases ml model providers clients joint controllers therefore need perform joint risk assessment cases model actually contains examples training default svms knns mentioned transfer personal treat adversarial examples main protection concerns involve accidentally revealing personal potential novel security risks ‘ adversarial examples ’ examples fed ml model deliberately modified reliably misclassified images 20200214 74 version 1.0 auditing framework -draft guidance consultation manipulated even real -world modifications stickers placed surface item examples include pictures turtles classi fied guns road signs stickers would instantly recognise ‘ stop ’ image recognition model adversarial examples concerning security perspective might themselve raise protection concerns ’ involve personal security principle refers security personal – protecting unauthorised processing however adversarial attacks ’ necessarily involve unauthorised processing personal compromise system however may cases adversarial examples risk freedoms individuals instance attacks demonstrated facial recognition slightly distorting face image one individual adversary trick facial recognition system misclassifying another even though would still recognise distorted image correct individual would raise concern system ’ statistical accuracy especially system make legal similarly significant decisions individuals may also need consider risk adversarial examples part obligations nis directive 2018. ico competent authority ‘ relevant service providers ’ nis include online search engines online marketplaces cloud computing services ‘ nis incident ’ includes incidents compromise stored network related services provide likely include cloud computing services even adversarial attack involve personal may still nis incident therefore within ico ’ remit reading – ico guidance nis regulations including whether qualify relevant service provider read guide nis example controls risk statement infrastructure architecture increases likelihood unauthorised access alteration destruction personal preventative • subscribe security advisories receive alerts vulnerabilities • comply assess system external security certifications schemes 20200214 75 version 1.0 auditing framework -draft guidance consultation • subject software quality review one individuals view read parts source code least one reviewers must author code • document policy process separation environment rest network infrastructure evidence separation adhered happened • approach asset management ensure coordinate approach optimisation costs risks service/performance sustainability • document contracts third parties role responsibilities third parties • document policies processes dealing third parties evidence due diligence security completed • document policy processes breach reporting escalation • adhere policy process • model governance policy • assess secure implementations trained model mplement appropriate post pre-deployment • processes place review latest privacy enhancing techniques assess technique 's applicability context implement appropriate • document dpia including thorough assessment security risks mitigants controls reduce likelihood impact attack • api access policy place monitors volume patterns requests identify report sus picious activity • ensure staff trained understand breach reporting policy procedures follow detective • monitor api requests detect suspicious requests take action result • regularly test assess evaluate effectiveness security measures put place eg techniques penetration testing • monitor complaints monitoring take action result including broader analysis identify individuals may impacted corrective • evidence changes made system design including analysis justification reduce risk future attacks 20200214 76 version 1.0 auditing framework -draft guidance consultation minimisation privacy -preserving techniques available considerations minimisation principle need make minimisation principle requires identify minimum amount personal ou need fulfil purpose process example article 5 1 c gdpr says quote ‘ 1 personal shall adequate relevant limited necessary relation purposes hey processed minimisation ’ however generally require large amounts first glance may therefore difficult see comply minimisation principle – yet using part processing still required whilst may appear challenging practice may case minimisation principle mean either ‘ process personal ’ ‘ process ’ going break law ’ key process personal need purpose go determining ‘ adequate relevant limited ’ therefore going specific circumstances existing guidance minimisation deta ils steps take context ‘ adequate relevant limited ’ therefore also case specific however number techniques adopt order develop process need still remaining functional section explore relevant techniques supervised machine ml currently common type within organisations individuals acc ountable risk management compliance need aware techniques exist able discuss assess different approaches technical staff example default approach scientists designing nd building might involve collecting using much possible without thinking ways could achieve purposes less must therefore implement risk management practices designed ensure minimis ation relevant minimisation techniques 20200214 77 version 1.0 auditing framework -draft guidance consultation fully considered design phase similarly buy and/or implement operated third parties considerations form part procurement process due diligence also aware may help comply principle minimisation techniques described eliminate kinds risk also techniques require compromise comply minim isation requirements others may need balance minimisation compliance utility objectives eg making statistically accurate non -discriminatory ml models see trade -offs section detail first step take towards compliance minimisation understand map ml processes personal might relevant provisions legislation see article 5 1 c recital 39 article 16 rectification article 17 erasure gdpr external link reading – ico guidance read guidance minimisation principle guide gdpr process personal supervised ml models supervised ml algorithms trained identify patterns create models datasets ‘ training ’ include past examples type instances model asked classify predict specifically training contains ‘ target ’ variable ie thing model aiming predict classify several ‘ predictor ’ variables ie input make prediction instance training bank ’ credit risk ml model predictor variables might include age income occupation location previous customers target variable whether customers repaid loan trained ml classify make predictions based containing examples system never seen query sent ml model containing predictor variables instance e g customer ’ age income occupation etc. model responds best guess target variable instance eg whether customer default loan 20200214 78 version 1.0 auditing framework -draft guidance consultation supervised ml approaches therefore two main phases 1. training phase training develop models based past examples 2. inference phase model make prediction classification instances model make predictions classifications individual people likely personal training inference phases techniques minimise personal designing ml applications designing building ml applications scientists generally assume training testing operating system aggregated centralised way held full original form single entity multiple places throughout system ’ lifecycle however personal need consider whether necessary process purpose achieve outcome processing less personal definition minimisation principle requires number techniques exist help minimise amount personal need process minimise personal training stage explained training phase involves applying algorithm dataset containing set features individual generate prediction classificati however features included dataset necessarily relevant purpose example financial demographic features useful predict credit risk therefore need assess features – therefore wh – relevant purpose process variety standard feature selection methods scientists select features usefu l inclusion model methods good practice science also go way towards meeting minimisation principle also discussed ico ’ previous report big fact might later process found useful making predictions enough establish need keep purpose retroactively collection retention must collect personal -chance might useful 20200214 79 version 1.0 auditing framework -draft guidance consultation future although may able hold foreseeable event may occur – able reading – ico guidance read report big machine protection privacy -enhancing methods consider also range techniques enhancing privacy minimise personal processed training phase including • perturbation adding ‘ noise ’ • federated techniques involve modifying training reduce extent traced back specific individuals retaining purposes training well -performing models apply types privacy- enhancing techniques training already collected possible however apply collecting personal part mitigating risks individuals large datasets pose measure effectiveness privacy -enhancing techniques balancing privacy individuals utility ml system mathematically using methods differential privacy differential privacy way measure whe ther model created ml algorithm significantly depends particular individual train mathematically rigorous theory meaningfully implementing differential privacy practice still challenging monitor developments methods assess whether provide meaningful minimisation particular context attempting implement • perturbation modification could involve changing values points belonging individuals random – known ‘ perturbing ’ adding ‘ noise ’ – way preserves statistical properties features generally speaking choose much noise inject obvious consequences much still learn ‘ noisy ’ instance smartphone predictive text based words users previously typed rather always collecting user ’ actual keystrokes system could designed create ‘ noisy ’ ie false 20200214 80 version 1.0 auditing framework -draft guidance consultation words random means makes substantially less certain words ‘ noise ’ words actually typed specific user although would less accurate individual level provided system enough users could still observe patterns train ml model aggregate level noise inject less learn cases may able inject sufficient noise render pseudonymous way provides meaningful level protection • federated related privacy- preserving technique federated allows multiple different parties train models ‘ local ’ models combine patterns models identified known ‘ gradients ’ single mo accurate ‘ global ’ model without share training federated relatively several large -scale applications include auto correction predictive text models across smartphones also medical involving analysis across multiple patient databases sharing gradient derived locally trained model presents lower privacy risk sharing training gradient still reveal personal informatio n individuals derived especially model complex lot fine -grained variables therefore still need assess risk -identification case federated participating organisations may co nsidered joint controllers even though ’ access ’ reading controllership read section controller/processo r rela tionships see ‘ rappor randomised aggregatable privacy preserving ordinal responses ’ example perturbation minimise personal inference sta ge make prediction classification individual ml models usually require full set predictor variables person included query training phase number techniques minimise personal and/or mitigate risks posed inference stage including • converting personal less ‘ readable ’ formats • making inferences locally • privacy- preserving query approaches 20200214 81 version 1.0 auditing framework -draft guidance consultation consider approaches • converting personal less “ readable ” formats many cases process converting format allows classified model go way towards minimising raw personal usually first converted abstract format purposes prediction instance -readable words normally translated series numbers called ‘ feature vector ’ means deploy model may need process -interpretable version personal contained query example conversion happens user ’ device however fact longer easily human- interpretable imply converted longer personal consider facial recognition frt example order facial recognition model work images faces classified converted ‘ faceprints ’ mathematical representations geometric properties underlying faces – eg distance person ’ nose upper lip rather sending facial images servers photos could converted faceprints directly individuals ’ device captures sending model querying faceprints would less easily identifiable humans face photos however faceprints still personal indeed biometric therefore much identifiable within context facial recognition models – purposes uniquely identifying individual would special category protection law • making inferences locally another way mitigate risks involved sharing predictor variables host ml model device query generated already collects stores individual ’ personal example ml model could installed user ’ device make inferences ‘ locally ’ rather hosted cloud server instance models predicting news co ntent user might interested could run locally smartphone user opens news app day ’ news sent phone local model would select relevant stories show user based user personal ha bits profile tracked stored device shared provider app store constraint ml models need sufficiently small computationally efficient run user ’ hardware however recent advances purpose -built hardware smartphones embedded devices mean increasingly viable option 20200214 82 version 1.0 auditing framework -draft guidance consultation important note local processing necessarily scope protection law even personal involved training processed user ’ device organisation creates distributes model still controller far determines means purposes processing similarly personal user ’ device subsequently accessed third party activity would constitute ‘ processing ’ • privacy-preserving query approaches feasible deploy model locally privacy- enhancing techniques exist minimise revealed query sent ml model allow one party retrieve prediction classificat ion without revealing party running model simple terms allow get answer without fully reveal question reading see ‘ privad practical privacy online advertising ’ external link ‘ targeted advertising handset privacy security challenges ’ external link proof concept examples making inferences locally see ‘ tapas trustworthy privacy -aware participatory sensing ’ example privacy -preserving query approaches anonymisation role conceptual technical similarities minimisation anonymisation cases applying privacy- preserving techniques means certain ml rendered ps eudonymous anonymous however note pseudonymisation essentially security risk reduction technique protection law still applies personal undergone pseudonymisation contrast ‘ anonymous ’ means question longer personal protection law apply reading ico currently developing guidance anonymisation take account recent developments technique field storing limiting training sometimes may necessary retain training order -train model instance modelling approaches become available debugging however whe model established unlikely re-trained modified training may longer needed 20200214 83 version 1.0 auditing framework -draft guidance consu ltation model designed last 12 months ’ worth retention policy specify older 12 months deleted reading union agency network security enisa number publications pets including reports external link example controls risk statement developers properly assess adequacy necessity relevance personal resulting noncompliance minimisation principle preventative • document levels approval authority development/use including personal sets included within model evidence appropriate approval • review personal relevance stage model includ ing detailed justification retention confirmation irrelevant removed deleted • separate different stages lifecycle based conditions minimisation principle • document retention policy schedule evidence schedule adhered personal deleted line schedule retention outside schedule justified approved • carry independent review model input output specifically regarding relevance personal inputs • document dpia including thorough assessment pets considered considered appropriate detective • periodic review features within model check ar e still relevant eg testing fewer features see results achieved view reducing amount personal processed • monitor individual requests complaints individuals including action taken result individual level boarder analysis • periodically assess whether model remains compliant minimization processes third parties corrective 20200214 84 version 1.0 auditing framework -draft guidance consultation • remove delete non -required features • select less invasive model including thorough justification change • remove erase training longer required eg longer predictively useful • implement appropriate pets 20200214 85 version 1.0 auditing framework -draft guidance consultation enable individual glance way developed deployed means personal often managed processed unusual ways may make harder understand individual apply challenging implement effective mechanisms individuals exercise detail • individual apply different stages lifecycle • individual relate personal contained model • enable individual relating solely automated decisions legal similar effect • role oversight individual apply different stages lifecycle protection law individuals number relating personal within apply wherever personal various points deployment lifecycle system therefore covers personal • contained training • make prediction deployment result prediction • might contained model section describes considerations may encounter developing deploying attempti ng comply individual access rectification erasure restriction processing portability object referred articles 13 -21 gdpr cover detail discusses general challenges facilitating context appropriate mentions challenges specific individuals solely automated decisions affect legal similarly significant ways discussed detail 20200214 86 version 1.0 auditing framework -draft guidance consultation ‘ role oversight ’ raise particular challenges using enable individual requests training creating using ml models invariably need obtain train models instance retailer creating model predict consumer purchases based past transactions needs large dataset customer transactions train model identifying individuals training potential challenge enabling typically training includes relevant predictions past transactions demographics location contact details unique customer identifiers training also typically subjected various measures make amena ble ml algorithms however detailed timeline customer ’ purchases might transformed summary peaks troughs transaction history process transforming prior using training statistical model nstance transforming numbers values 0 1 often referred ‘ pre -processing ’ create confusion regarding terminology protection ‘ processing ’ refers operation set operations performed personal ‘ pre -processing ’ machine terminology still ‘ processing ’ protection terminology therefore protection still applies processes involve converting personal one form another pot entially less detailed form may make training potentially much harder link particular named individual however protection law necessarily considered sufficient take scope therefore still nee consider responding individuals ’ requests exercise even lacks associated identifiers contact details transformed pre -processing training may still considered personal ‘ single ’ individual relates combination may process even associated customer ’ name instance training purchase pre diction model might include pattern purchases unique one customer example customer provide list recent purchases part request organisation may able identify portion training hat relates individual 20200214 87 version 1.0 auditing framework draft guidance consultation kinds circumstances obliged respond individual ’ request assuming taken reasonable measures verify identity exceptions apply consult guidance determining personal identifiability • access regard requests access rectification erasure training manifestly unfounded excessive may harder fulfil motivation requesting may unclear comparison access requests might typically receive collect maintain additional personal enable identify individuals within training sole purposes complying gdpr per article 11 may times therefore able identify individual training individual provide additional would enable identification therefore fulfil request • rectification rectification may also apply f personal train system steps take respect rectification depend process well nature scope context purpose processing case training system one purpose processing may find general patterns large datasets context individual inaccuracies training likely affect performance model since one point among many compared personal might take action individual example may think important rectify incorrectly recorded customer delivery address rectify incorrect address training rationale likely tha former could result failed delivery latter would barely affect overall accuracy model however practice rectification allow disregard requests think less important fo r purposes • erasure may also receive requests erasure personal contained within training note whilst erasure still need consider erasure request receive u nless processing basis legal obligation task unlikely lawful bases training – see section lawful bases 20200214 88 version 1.0 auditing framework -draft guidance consultation erasure individual ’ personal training unlikely affect ability fulfil purposes training system therefore unlikely justification fulfilling request erase personal training dataset complying request erase training entail erasing ml models based unless models contain infer situations cover section • portability individuals portability ‘ provided ’ controller lawful basis processing consent contract ‘ provided ’ includes individual consciously input form also behavioural observational gathered process using service cases traini ng model eg demographic spending habits counts ‘ provided ’ individual portability would therefore apply cases processing based consent contract however discussed pre -processing methods usually applied significantly change original form something effectively analysed machine algorithms transformation significant resulting may longer count ‘ provided ’ case would subject portability although still constitute personal protection still apply eg access however original form pre -processed derived still subject portability provided individual consent contract processed automated means • informed inform individuals personal going train system cases may obtained training individual therefore opportunity inform time cases provide individual wi th specified article 14 within reasonable period one month latest since using individual ’ purposes training system normally constitute making solely automated decision legal similar ly significant effects need provide decisions taking however still need comply main transparency requirements 20200214 89 version 1.0 auditing framework -draft guidance consulta tion reasons stated may difficult identify communicate individuals whose personal contained training instance training may stripped personal identifiers contact addresses still remaining personal cases may impossible involve disproportionate effort provide directly individual therefore instead take appropriate measures protect individual ’ freedoms legitimate interests instance could provide explaining obtained train system enable individ ual requests outputs typically deployed outputs system stored profile individual take action instance product offers customer sees website might driven output predictive model stored profile constitutes personal subject access rectification erasure whereas individual inaccuracies training may negligible effect inaccur ate output model could directly affect individual requests rectification model outputs personal inputs based therefore likely made requests rectification training however said predictions inaccurate intended prediction scores opposed statements fact personal inaccurate rectification apply personal resulting analysis provided subject portability means outputs models predictions classifications individuals scope portability cases features tra model may result previous analysis personal instance credit score result statistical analysis based individual ’ financial might feature ml model cases credit score included within scope portability even features reading – ico guidance read guidance individual guide gdpr including • informed • access • erasure • rectification 20200214 90 version 1.0 auditing framework -draft guidance consultation • portability individual relate contained model addition inputs outputs model cases personal might also contained model explained section 3.2 could happen two reasons design accident fulfil requests regarding models contain design personal included models design certain types models support vector machines svms contain key examples training order help distinguish examples deployment cases small set individual examples contained somewhere internal logic model training set typ ically contains hundreds thousands examples small percentage ends directly model therefore chances one relevant individuals makes request small remains possible dependin g particular programming library ml model implemented may built -in function easily retrieve examples cases might practically possible respond individual ’ request enable using models contain personal design implement way allows easy retrieval examples request access could fulfil without altering model request rectification erasure may possible without -training model either rectified without erased deleting model altogether well -organised model management system deployment pipeline accommodating requests -training redeploying models accordingly prohibitively costly fulfil requests regarding contained models accident aside svms models hat contain examples training design models might ‘ leak ’ personal accident cases unauthorised parties may able recover elements training infer analysing way model behaves access rectification erasure may difficult impossible exercise fulfil scenarios unless individual presents evidence personal could inferred model may 20200214 91 version 1.0 auditing framework -draft guidance consultation able determine whether personal inferred therefore whether request basis regularly proactively evaluate possibility personal inferred models light state -of-the-art minimise risk accidental disclosure enable individual relating olely automated decisions legal similar effect protection requires implement suitable safeguards processing personal make solely automated decisions legal similarly significant impact individuals afeguards include individuals • obtain intervention • express point view • contest decision made • obtain explanation logic decision processing involving solely automated decision making falls part 2 dpa safeguards differ gdpr lawful basis processing requirement authorisation law processing involving solel automated decision making falls part 3 dpa applicable safeguards depend regulations provided particular law authorising automated decision- making although individual request consider decision take decision based solely automated processing safeguards token gestures guidance published protection board edpb states intervention involve review decision quote “ must carried someone appropriate authority capability change decision ” review also include quote “ thorough assessment relevant including additional provided subject. ” 20200214 92 version 1.0 auditing framework -draft guidance consultation conditions intervention qualifies meaningful similar apply render decision non- solely automated see previous section however key difference solely automated contexts intervention required case -by-case basis safeguard individual ’ whereas system qualify solely automated meaningful intervention required every decision could relating automated decisions particular issue type complexity involved making solely automated decisions affect nature severity risk people ’ protection raise different considerations well compliance risk management challenges basic automate relatively small number explicitly written rules unlikely considered eg set clearly expressed ‘ -then ’ rules determine customer ’ eligibility product however resulting decisions could still constitute automated decision -making within meaning protection law also relatively easy reviewer identify rectify mistake decision challenged individual system ’ high interpretability however based ml may complex present challenges meaningful review ml make predictions classifications people based patterns even highly statistically accurate occasionally reach wrong decision individual case errors may easy reviewer identify understand fix every challenge fro individual result decision overturned expect many could two particular reasons may case ml • individual ‘ outlier ’ ie circumstances substantially different considered training build system ml model trained enough similar individuals make incorrect predictions classifications • assumptions design challe nged eg continuous variable age might broken ‘ binned ’ discrete age ranges like 20 -39 part modelling process finer-grained ‘ bins ’ may result different model substantially different predictions peopl e different ages validity pre -processing design choices may come question result individual ’ challenge 20200214 93 version 1.0 auditing framework -draft guidance consultation steps take fulfil related automated decision making • consider system requirements necessary support meaningful review design phase particularly interpretabilit requirements effective user- interface design support reviews interventions • design deliver appropriate training support reviewers • give staff appropriate authority incentives support address escalate individuals ’ concerns necessary override system ’ decision however additional requirements considerations aware ico ’ explain guidance looks extent complex might affect ability provide meaningful explanations individuals however complex also impact effectiveness mandatory safeguards system comp lex explain may also complex meaningfully contest intervene review put alternative point view instance system uses hundreds features complex nonlinear model make prediction may difficult individual determine variables correlations object therefore safeguards around solely automated mutually supportive designed holistically individual mind logic system explanations decisions give individuals necessary context decide whether grounds would like request intervention cases insufficient explanations may prompt ndividuals resort unnecessarily requests intervention expression views contests likely happen individuals ’ feel sufficient understanding decision reached process indivi duals exercise simple user friendly example communicate result solely automated decision communicated website page contain link allowing individual contact member staff intervene without undue delays complications also required keep record decisions made system part accountability documentation obligations also include whe ther individual requested intervention expressed 20200214 94 version 1.0 auditing framework -draft guidance consultation views contested decision whether changed decision result monitor analyse decisions regularly changed response individuals exercising consider amend accordingly system based ml might involve including corrected decisions fresh training similar mistakes less likely happen future substantially may identify need collect better training fill gaps led erroneous decision modify model -building process ie changing feature selection addition compliance requirement also opportunity improve performance turn build individuals ’ trust however grave frequent mistakes identified need take immediate steps understand rectify underlying issues necessary suspend automated system also trade- offs -in-the-loop may entail either terms erosion privacy reviewers need consider additional personal order validate reject generated output possible reintroduction biases end automated process reading – ico guidance read guidance documentation guide gdpr reading – protection board protection board edpb replaced article 29 working party wp29 includes representatives f rom protection authorities member state adopts guidelines complying requirements gdpr wp29 published guidelines automated decision making profiling february 2018. edpb endorsed guidelines may 2018. explain logic involved ai- driven automated decision individuals also meaningful logic involved -driven automated decision detail comply please see recent explain guidance produced collaboration alan turing institute turing 20200214 95 version 1.0 auditing framework -draft guidance consultation example controls risk statement infrastructure architecture inhibits ability recognise respond act upon individual requests preventative • ensure developers receive training includes requirement consider individual ir offset • set document levels approval authority development/use including consideration ir model maintain evidence appropriate approval • implement document policy process dealing ir requests processing pipeline particular defining circumstances would ’ respond eg train versus output impact individual • provide individual training 'customer facing individuals including escalate complex requests • dpia include thorough flow mapping • consider indexing making searchable using common identifiers part system design ir requests anticipated • fully automated decision making ensure humans may required investigate decision skil ls tools autonomy investigate override decision • ensure subjects informed processing purposes training system profiling cases training another organisation direct relationship dat subject informing directly would involve disproportionate effort ensure make publicly available organisations source processes place inform subjects processing • ensure individuals given means provide additional order identified within • maintain documented policies processes dealing third parties particular roles responsibilities controller processor detective • conduct peer reviews ensure actions completed required 20200214 96 version 1.0 auditing framework -draft guidance consultation • conduct periodic review sample ir requests ensure accurate complete declined justification eg manifestly unfounded appropriate • systematically monitor time taken respond requests order identify potentially complex • submit ‘ dummy ’ ir requests test process measure outcomes corrective • re-design system storage indexing training • consider additional ata could help identify subjects case request • correct inaccurate personal contextualise inferred misleading matter fact • delete personal required • re-train employees responsible fo r identification execution ir requests • retrain humans reassessment resource requirements eg humans pressured make x decisions time • redesign eg simplification inclusion warnings pop-ups • select appropriate model including thorough justification change • re-review overturn decisions eg one rogue reviewer action taken result including broader assessment impacts individuals role oversight inform legal similarly significant decisions individuals risk decisions made without appropriate oversight infringes article 22 gdpr mitigate risk ensure people assigned provide oversight remain engaged critical able challenge system ’ outputs wherever appropriate difference solely automated partly automated decision making two ways • automated decision making adm system makes decision automatically 20200214 97 version 1.0 auditing framework -draft guidance consultation • decision -support system supports decision maker deliberation example could system automatically approves rejects financial loan merely provide additional support loan officer deci ding whether grant loan application whether fully automated -driven decision making generally less risky -supported decision making depends specific circumstances therefore need evaluate based ow n context regardless merits automated decisions treated differently decisions protection law specifically article 22 gdpr restricts fully automated decisions legal similarly significant effects individuals limited set lawful bases requires certain safeguards place contrast decision support tools support decision -making subject conditions result restrict ions safeguards automated decision -making arguably carries higher risk decision- making even though may cases mitigate risks decision -making decide support decision -making aware decision fall outside scope article 22 ‘ rubber- stamped ’ input needs meaningful degree quality review intervention final decision made individual key factors determining whether system automated decision-making merely decision -support ensuring input meaningful situations responsibility using system senior leaders scientists business owners oversight functions among others expected play role ensuring applications designed built intended deploying desi gned decision support tools therefore intended outside scope article 22 aware existing guidance issues ico edpb key considerations • reviewers must involved checking system ’ recommendation apply automated recommendation individual routine fashion • reviewers ’ involvement must token gesture actual ‘ meaningful ’ influence decisio n 20200214 98 version 1.0 auditing framework -draft guidance consultation including ‘ authority competence ’ go recommendation • reviewers must ‘ weigh -up ’ ‘ interpret ’ recommendation consider available input also take account additional factors relevant provisions legislation see gdpr article 22 recital 71 external link see dpa sections 14 49 50 external link reading – ico guidance read guidance au tomated decision making profiling guide gdpr reading – protection board protection board edpb replaced article 29 working party wp29 includes representatives protection authorities member state adopts guidelines complying requirements gdpr wp29 published guidelines automated decision making profiling february 2018. edpb endorsed guidelines may 2018. additional risk factors need consider meaningfulness input automated decision -making system however basi c may however complex two additional factors could potentially cause system intended decision -support inadvertently fail ensure meaningful input therefore fall scope article 22. th ey • automation bias • lack interpretability ‘ automation bias mean models based mathematics people tend think objective trust output regardless accurate 20200214 99 version 1.0 auditing framework -draft guidance consultation terms automation bias automation -induced complacency describe users routinely rely output generated decisio n-support system stop using judgement stop questioning whether output might wrong ‘ lack interpretability ’ mean types may outputs difficult reviewer interpret examp le rely complex high dimensional ‘ deep ’ models outputs easily interpretable explanation tools available reliable risk able meaningfully assess output system factor decision -making meaningful reviews possible reviewer may start agree system ’ recommendations without judgement challenge means resulting decisions effectively ‘ solely automated ’ distinguish solely non -solely automated yes take view intended system beginning specify document clearly whether using support enhance decision -making make solely automated decisions senior management review sign -off intended system making sure line organisation ’ risk appetite means senior management needs solid understanding key risk implications associated option ready equipped provide appropriate degree challenge must also ensure lines accountability effective risk manageme nt policies place outset intended support decisions policies specifically address additional risk factors automation bias lack interpretability possible • may know advance whether partly fully automated application meet needs best • believe fully automated system fully achieve intended outcome processing may carry risks individuals partly automated system cases risk management policies dpias clearly reflec include risk controls option throughout system ’ lifecycle 20200214 100 version 1.0 auditing framework -draft guidance consultation address risks automation bias may think address automation bias chiefly improving effectiveness training monitoring reviewers training key component effective risk management controls mitigate automation bias place start project including scoping design phases well deployment design build phase relevant parts organisation eg business owners scientists oversight functions work together develop design requirements support meaningful review outset must think features expect system consider additional factors revi ewers take account finalising decision instance system could consider quantitatively measurable properties like many years ’ experience job applicant reviewer qualitatively assesses aspect application eg written communication reviewers access system arguably taking account additional factors means review may sufficiently meaningful decision may end considered ‘ solely automated ’ necessary consider capture additional factors consideration reviewers example might interact directly person decisi gather charge designing front -end interface system must understand needs thought process behaviours reviewers enable effectively intervene may therefore helpfu l consult test options reviewers early however features also depend available type model selected system building choices need test confirm assumptions made design phase system trained built address risks interpretability also consider interpretability design phase however interpretability challenging define terms measured different ways example reviewer • predict system ’ outputs change given different inputs • identify important inputs contributing particular output • identify output might wrong 20200214 101 version 1.0 auditing framework -draft guidance consultation important define document interpretability means measure specific context system wish personal system process interpretable others instance models small number -interpretable features eg age weight likely b e easier interpret models large number features relationship input features model ’ output also either simple complicated simple rules set conditions certain inferences made case decision trees easier interpret similarly linear relationships value output increases proportional input may easier interpret relationships non -linear output value proportional input non-monotonic output value may increase decrease input increases one approach address low interpretability 'local explanations using methods like local interpretable model -agnosti c explanation lime provides explanation specific output rather model general limes simpler surrogate model summarise relationships input output pairs similar system try ing interpret addition summaries individual predictions limes sometimes help detect errors eg see specific part image led model classify incorrectly however represent logic underlying system outputs misleading misused therefore assess whether context lime similar approaches help decision maker meaningfully interpret system output many statistical models also designed provide confidence score alongside output could help reviewer decision -making lower confidence score indicates reviewer needs input final decision see ‘ need statistical accuracy ’ assessing interpretability requirements part design phase allowing develop explanation tools part system required risk management policies establish robust risk based independent approval process processing operation uses also set clearly responsible testing final validatio n system deployed individuals accountable negative impact interpretability 20200214 102 version 1.0 auditing framework -draft guidance consultation effectiveness reviews provide sign -off line adopted risk management policy train staff address risks training staff pivotal ensuring system considered non solely automated starting point train retrain reviewers • understand system works limitations • anticipate system may misleading wrong • healthy level scepticism system ’ output given sense often system could wrong • understand expertise meant complement system provide list factors take account • provide meaningful explanations either rejecting accepting system ’ output – decision responsible also escalation policy place order training effective important • reviewers authorit override output generated system confident penalised authority confidence created policies training alone supportive organisational culture also crucial • training programme kept date line technological developments changes processes reviewers offered ‘ refresher ’ training intervals appropriate focussed training reviewers however worth noting also consider whether function requires additional training provide effective oversight eg risk internal audit monitoring undertake analysis many times reviewer accepted rejected system ’ output key part effective risk monitoring system risk monitoring reports flag reviewers routinely agreeing system ’ outputs demonstrate genuinely assessed decisions may effectively classed solely automated gdpr 20200214 103 version 1.0 auditing framework -draft guidance consultation need controls place keep risk within target levels outcomes go beyond target levels processes swiftly assess compliance take action necessary might include temporarily increasing scrutiny ensuring appropriate lawful basis safeguards case decision -making effectively become fully automated reading – ico guidance read explain draft guidance methods explaining interpreting example controls risk statement incorrectly classified fully automated result lack meaningful oversight potential non -compliance dp legislation preventative • implement document policy process classification relation article 22 including level approval authority maintain evidence decision -making process appropriate sign-off approval • provide training humans employed provide meaningful oversight including ability challenge system decision provide independent review • ensure system developers understood skills experience ability overseers designing system • include pre -implementation testing assessment oversight ensure meaningful • set document levels approval authority development/use particular relation model complexity ensure reviewers interpret challenge maintain evidence appropriate approval • conduct document analysis time expected meaningfully review detective • conduct post -implementation testing document results testing action taken result • test sample decisions ensure making decision document tests including sample selected criteria 20200214 104 version 1.0 auditing framework -draft guidance consultation • monitor decisions made compare decisions document action taken result performance goes outside defined tolerances • conduct document ‘ mystery shopping ’ exercises periodically provide deliberately misleading disagree ensure input meaningful • monitor individual requests complaints individuals particular relating article 22 including action taken result individual level boarder analysis • conduct periodic assessment confidence overturning outcome • monitor individuals ’ performance identify outliers action taken result corrective • re-train decision makers reassess resource requirements eg humans pressured make many decisions short space time • re-design eg simplification inclusion warnings pop-ups • select appropriate model include thorough justification change • re-review overturn decisions eg one rogue reviewer action taken result including broader assessment impacts individuals 20200214 105 version 1.0\",\n",
       " 'opinion ethics commission opinion ethics commission overview executive summary ..................................................................................... 12 introduction ................................................................................................ 33 ethical legal principles .......................................................................... 39 technical foundations ................................................................................. 49 multi-level governance complex ecosystems ................................... 67 ............................................................................................................. 79 algorithmic ................................................................................... 159 path ......................................................................................... 225 appendix .................................................................................................... 229a b c e f g executive summary ..................................................................................... 12 1. general ethical legal principles ............................................................................... 14 2. ............................................................................................................... 16 3. algorithmic .............................................................................................. 24 4. path .................................................................................................. 32 introduction ................................................................................................ 33 guiding motifs ....................................................................................................... 34 1. mission basic understanding .................................................................................. 35 2. working method .................................................................................................. 36 3. objectives scope report ................................................................................ 37 ethical legal principles .......................................................................... 39 1. fundamental value agency ......................................................................... 40 2. relationship ethics law .............................................................................. 41 3. general ethical legal principles ............................................................................... 43 3.1 dignity ............................................................................................... 43 3.2 self-determination ........................................................................................... 43 3.3 privacy ....................................................................................................... 45 3.4 security ...................................................................................................... 45 3.5 democracy ................................................................................................... 46 3.6 justice solidarity .......................................................................................... 46 3.7 sustainability ................................................................................................. 47 technical foundations ................................................................................. 49 1. status quo ................................................................................................... 51 2. system elements ............................................................................................. 52 2.1 .................................................................................................... 52 2.1.1 definition properties ....................................................................... 52 2.1.2 management ...................................................................................... 53 2.1.3 big small ................................................................................. 53 2.2 processing ......................................................................................... 54 2.2.1 algorithms ............................................................................................. 54 2.2.2 statistical inference .................................................................................... 55 2.2.3 machine ....................................................................................... 57 2.2.4 ................................................................................... 59 2.2.5 algorithmic ............................................................................... 62 2.3 software ................................................................................................ 62 2.4 hardware ............................................................................................... 63 2.5 system architecture ...................................................................................... 63a b ctable contents multi-level governance complex ecosystems ................................... 67 1. general role state .......................................................................................... 69 2. corporate self-regulation corporate responsibility .................................................... 70 3. education boosting skills critical reflection ........................................................... 72 4. technological developments ethical design ................................................................... 74 5. .......................................................................................................... 75 6. standardisation ................................................................................................... 76 7. two governance perspectives perspective algorithms perspective ............................... 77 ............................................................................................................. 79 1. general standards governance ........................................................................ 81 1.1 foresighted responsibility ................................................................................ 81 1.2 respect parties involved .............................................................. 82 1.3 sharing good ............................................................. 82 1.4 fit-for-purpose quality .............................................................................. 83 1.5 risk-adequate level security ............................................................... 83 1.6 interest-oriented transparency ........................................................................... 83 2. corresponding obligations .................................................................... 85 2.1 general principles obligations .......................................................... 85 2.2 clarification general principles reference typical scenarios ................................... 87 2.2.1 scenarios involving desistance ............................................................ 87 2.2.2 scenarios involving access .................................................................. 90 2.2.3 scenarios involving rectification .................................................................... 92 2.2.4 scenarios involving economic share ............................................................. 93 2.3 collective aspects obligations ...................................................... 94 3. standards personal ........................................................................ 95 3.1 personal relating legal entities ........................................................... 95 3.2 self-determination challenge tackled legal system whole ........................ 95 3.2.1 cooperative relationship applicable legal regimes ...................................... 95 3.2.2 risk-adequate interpretation applicable legal framework ...................................... 96 3.2.3. need clarify tighten applicable legal framework ................................... 99 3.2.4 uniform market-related supervisory activities ...................................................... 103 3.3 personal asset ................................................................................ 104 3.3.1 commercialisation personal ............................................................... 104 3.3.2. ownership issue financial compensation ............................................ 104 3.3.3. counter-performance ..................................................................... 105 3.3.4 basis personalised risk assessments ................................................. 106 3.3.5 reputational capital ....................................................................... 107 3.3.6 tradeable items ........................................................................... 108d e 3.4 inheritance ............................................................................. 110 3.4.1 precedence living wills ......................................................................... 110 3.4.2 role intermediaries ......................................................................... 110 3.4.3 post-mortem protection ..................................................................... 111 3.5 special groups subjects .......................................................................... 112 3.5.1 employees ....................................................................................... 112 3.5.2 patients .......................................................................................... 113 3.5.3 minors ........................................................................................... 114 3.5.4 vulnerable care-dependent persons ..................................................... 115 3.6 protection technical design ...................................................................... 116 3.6.1 privacy-friendly design products services .................................................... 116 3.6.2 privacy-friendly product ............................................................. 120 summary important recommendations action ................................................ 121 4. improving controlled access personal ................................................................ 124 4.1 enabling uses personal ............................................................... 124 4.1.1 preliminary considerations ........................................................................ 124 4.1.2 legal clarity certainty ......................................................................... 125 4.1.3 consent processes sensitive ............................................................... 126 4.1.4 legal protection discrimination ............................................................ 128 4.2 anonymisation pseudonymisation synthetic .................................................... 129 4.2.1 procedures standards presumption rules ...................................................... 131 4.2.2 ban de-anonymisation ......................................................................... 132 4.2.3 synthetic .................................................................................... 132 4.3 controlled access management trust schemes ................................ 133 4.3.1 privacy management tools pmt personal management pims ............ 133 4.3.2 need regulation pmt/pims ................................................................. 133 4.3.3 pmt/pims potential interface economy ......................................... 135 4.4 access portability ..................................................................... 136 4.4.1 promotion portability ...................................................................... 136 4.4.2 scope portability extended ...................................... 137 4.4.3 portability interoperability interconnectivity ........................................... 137 4.5 crowdsensing good ....................................................................... 138 summary important recommendations action ................................................ 139 5. debates around access non-personal ................................................................. 141 5.1 appropriate access macroeconomic asset ....................................................... 141 5.2 creation necessary framework conditions ......................................................... 142 5.2.1 awareness raising skills .................................................................. 142 5.2.2 building infrastructures needed data-based economy ...................................... 142 5.2.3 sustainable strategic economic policy ......................................................... 144 5.2.4 improved industrial property protection ........................................................... 144 5.2.5 partnerships ................................................................................. 145 5.3 access existing value creation ............................................................ 145 5.3.1 context .......................................................................................... 145 5.3.2 presence contractual relationship ............................................................. 146 5.3.3 absence contractual relationship .............................................................. 147 5.3.4 sector-specific access .................................................................. 147 5.4 open sector ........................................................................... 148 5.4.1 preliminary considerations ........................................................................ 148 5.4.2 legal framework infrastructures ............................................................... 149 5.4.3 state ’ duty protection ..................................................................... 150 5.5 open private sector .......................................................................... 151 5.5.1 platforms ............................................................................ 151 5.5.2 additional incentives voluntary sharing .................................................... 151 5.5.3 statutory access ....................................................................... 152 5.5.4 role competition law ........................................................................... 153 5.6 access public-sector b2g public-interest purposes ......................................... 154 summary important recommendations action ................................................ 155 algorithmic ................................................................................... 159 1. characteristics algorithmic ....................................................................... 160 2. general standards algorithmic ................................................................... 163 2.1 human-centred design ................................................................................. 163 2.2 compatibility core societal values ................................................................... 164 2.3 sustainability design algorithmic ............................................... 165 2.4 high level quality performance ................................................................... 165 2.5 guarantee robustness security ................................................................... 166 2.6 minimising bias discrimination prerequisite fair decisions ..................................... 167 2.7 transparent explainable comprehensible ................................................... 169 2.8 accountability structures .......................................................................... 171 2.9 result responsibility-guided consideration .............................................................. 171 3. recommendation risk-adapted regulatory approach .................................................... 173 3.1 system criticality system requirements .............................................................. 173 3.2 criticality pyramid ...................................................................................... 177 3.3 regulation algorithmic enshrining horizontal requirements formed sectoral instruments ................................................................................. 180 summary important recommendations action ................................................. 183f 4. instruments obligations controllers subjects ....................................... 185 4.1 transparency requirements ............................................................................. 185 4.1.1 mandatory labelling “ ” ......................................................................... 185 4.1.2 duties provide duties provide explanation access “ ” “ ” ...................................................... 185 4.1.3 risk impact assessment ........................................................................... 188 4.1.4 duty draw documentation keep logs ..................................................... 190 4.2 requirements algorithmic ............................................................. 190 4.2.1 general quality requirements algorithmic .............................................. 190 4.2.2 special protective measures algorithmic context decision-making .......................................................................... 191 4.2.3 appropriate algorithmic inferences ....................................................... 193 4.2.4 legal protection discrimination ............................................................ 193 4.2.5 preventive official licensing procedures high-risk algorithmic ........................... 195 summary important recommendations action ................................................. 196 5. institutions ................................................................................................ 198 5.1 regulatory powers specialist expertise ............................................................... 198 5.1.1 distribution supervisory tasks within sectoral network oversight authorities ................ 198 5.1.2 definition oversight powers according tasks involved ...................................... 199 5.1.3 criticality-adapted extent oversight ............................................................. 200 5.2 corporate self-regulation co-regulation ............................................................. 201 5.2.1 self-regulation self-certification ............................................................... 201 5.2.2 creation code conduct ..................................................................... 202 5.2.3 quality seals algorithmic .............................................................. 203 5.2.4 contact persons algorithmic companies authorities .............................. 203 5.2.5 involvement civil society stakeholders .......................................................... 203 5.3 technical standardisation ............................................................................... 203 5.4 institutional legal protection particular associations file action ......................... 204 summary important recommendations action ................................................. 205 6. special topic algorithmic media intermediaries .............................................. 207 6.1 relevance democratic process example social networks .................................... 207 6.2 diversity media intermediaries example social networks ....................................... 208 6.3 labelling obligation social bots ...................................................................... 209 6.4 measures combat fake news .......................................................................... 210 6.5 transparency obligations news aggregators ........................................................... 210 summary important recommendations action ................................................. 211 7. algorithmic state bodies .................................................................. 212 7.1 opportunities risks involved algorithmic state bodies ......................... 212 7.2 algorithmic law-making ..................................................................... 212 7.3 algorithmic dispensation justice ....................................................... 213 7.4 algorithmic administration ............................................................ 214 7.5 algorithmic security law ............................................................... 214 7.6 transparency requirements algorithmic state actors ............................. 215 7.7 risk involved automated total enforcement ........................................................ 217 summary important recommendations action ................................................. 218 8. liability algorithmic ............................................................................. 219 8.1 significance ............................................................................................ 219 8.2 harm caused algorithmic .......................................................... 219 8.2.1 liability “ electronic person ” ................................................................ 219 8.2.2 vicarious liability “ autonomous ” ...................................................... 219 8.2.3 strict liability ..................................................................................... 220 8.2.4 product security product liability .............................................................. 221 8.3 need reassessment liability law .................................................................. 222 summary important recommendations action ................................................. 224 path ......................................................................................... 225 appendix .................................................................................................... 229 1. federal government ’ key questions ethics commission ...................................... 230 2. members ethics commission .................................................................... 234g 12 executive summar executive summary 13 executive summar society experiencing profound changes brought digitalisation innovative data-based technologies may benefit us individual wider societal levels well potentially boosting economic productivity promoting sustainability catalysing huge strides forward terms scientific progress time however digitalisation poses risks fundamental freedoms raises wide range ethical legal questions centring around two wider issues role want technologies play design want ensure transformation serves good society whole society elected political representatives must engage debate shape data-based technologies including germany ’ federal government set ethics commission datenethikkommission 18 july 2018. given one-year mandate develop ethical benchmarks guidelines well specific recommendations action aiming protecting individual preserving social cohesion safeguarding promoting prosperity age starting point federal government presented ethics commission number key questions clustered around three main topics algorithm-based decision-making adm opinion ethics commission however merely one among many possible variants algorithmic system much common terms ethical legal questions raises mind ethics commission structured work two different headings algorithmic broader sense .in preparing opinion ethics commission inspired following guiding motifs ●ensuring human-centred value-oriented design ●fostering skills critical reflection world ●enhancing protection individual freedom self- determination integrity ●fostering responsible utilisation compatible good ●introducing risk-adapted regulation effective oversight algorithmic ●safeguarding promoting democracy social cohesion ●aligning strategies sustainability goals ●strengthening sovereignty germany europe 1 general ethical legal principles humans morally responsible actions escaping moral dimension humans responsible goals pursue means pursue reasons dimension societal conditionality action must always taken account designing technologically shaped future time notion serve humans rather humans subservient taken incontrovertible fact germany ’ constitutional system founded understanding nature adheres tradition europe ’ cultural intellectual history technologies altered ethical framework – terms basic values freedoms enshrined german constitution charter fundamental union yet challenges facing mean need reassert values freedoms perform balancing exercises mind ethics commission believes following ethical legal principles precepts viewed indispensable socially accepted benchmarks action.human dignity dignity principle presupposes uncon ditional value every prohibiting practices total monitoring individual humiliation deception manipulation exclusion self-determination self-determination fundamental expression freedom encompasses notion informational self-determination term “ self- determination ” express idea selfdetermined player society privacy privacy intended preserve individual ’ freedom integrity personal identity potential threats privacy include wholesale collection evaluation even intimate topics security principle security relates physical emotional safety humans also environmental protection involves preservation vitally important assets guaranteeing security entails compliance stringent requirements e. g. relation human/machine interaction system resilience attacks misuse 15 executive summar democracy technologies systemic relevance flourishing democracy make possible shape forms political participation also foster emergence threats manipulation radicalisation justice solidarity view vast amounts power accumulated using technologies threats exclusion discrimination safeguarding equitable access distributive justice urgent task digitalisation foster participation society thereby promote social cohesion sustainability developments also serve sustainable technologies contribute towards achieving economic ecological social sustainability goals ethics equated one-to-one basis law words everything relevant ethical perspective enshrined legislation conversely provisions law motivated purely pragmatic considerations nevertheless law must times heedful potential ethical implications legal provisions force well living ethical standards ethics commission holds view regulation necessary replaced ethical principles particularly true issues heightened implications fundamental require central decisions made democratically elected legislator regulation also essential basis building system citizens companies institutions trust transformation society guided ethical principles time regulation must unduly inhibit technological social innovation dynamic market growth overly rigid laws attempt regulate every last detail situation may place stranglehold progress increase red tape extent innovation german companies longer keep pace rate technological international stage yet legislation one range tools lend tangible shape ethical principles synergistic various governance instruments different levels multi-level governance vital view complexity dynamism ecosystems instruments include legislative measures standardisation also various forms co- regulation self-regulation technological design moreover function governance instruments applies business models options steering economy governance broader sense also encompasses policy-making decisions fields education important consider aforesaid governance instruments national context also particular international contexts view ethics commission key questions presented federal government belong one two different perspectives questions concentrate mainly “ perspective ” questions primarily focused algorithmic “ algorithms perspective ” two perspectives regarded competing views two sides coin instead represent two different ethical discourses complement contingent upon different ethical discourses typically also reflected different governance instruments including different acts legislation perspective focuses machine basis algorithmically shaped decisions plethora purposes perspective considers primarily view origin potential impact processing may certain parties involved subject well society large ethical legal point view important identify standards governance typically however parties involved enforce others play even significant role central distinction context personal non-personal since determines whether provisions protection law apply general standards governance opinion ethics commission responsible governance must guided following ethics principles ●foresighted responsibility possible future cumulative effects network effects effects technological developments changing actor constellations must taken account gauging potential impact collecting processing forwarding individuals general ●respect parties involved parties involved generation – whether subjects different role – may relation must respected ●data sharing good non-rivalrous resource duplicated parallel many different individuals many different purposes thereby furthering good ●fit-for-purpose quality responsible includes ensuring high level quality fit relevant purpose ●risk-adequate level security vulnerable external attacks difficult recover gone astray standard security applied must therefore commensurate potential risk inherent situation question ●interest-oriented transparency controllers must prepared account data-related activities requires appropriate documentation transparency necessary corresponding liability regime place 2 17 executive summar corresponding obligations self-determined navigation society parties must able enforce certain data-related others first foremost among relating individual ’ personal derive informational selfdetermination enshrined fundamental guaranteed applicable protection law self-determination society also includes self-determined economic exploitation one ’ includes selfdetermined manage ment non-personal non-personal generated one ’ devices ethics commission takes view principle self-determination society also applies companies legal entities – least extent – groups persons collectives often generated contributions different parties acting different roles – subject owner data- generating device yet another role opinion ethics commission contributions generation lead exclusive ownership rather dataspecific co-determination participation turn may lead corresponding obligations part parties extent individual entitled kind shape take depends following general factors nature scope party ’ contribution generation b weight party ’ legitimate interest granted c weight possibly conflicting interests part party third parties taking account potential compensation arrangements e. g. protective measures remuneration interests general e balance power parties involved.data may allow holders pursue number different goals particular following ●requiring controller desist require erasure ●requiring controller rectify ●requiring controller grant access full portability ●requiring economic share profits derived help type desistance rectification access economic share exists separate set conditions defining e. g. counts party ’ legitimate interest granted determining whether party require desistance particular key considerations include potential harm associated said circumstances party question contributed generating potential harm may also relevant request made rectify benchmark lower respect party requests access graded spectrum interests count legitimate interest granted access particularly relevant within existing value creation narrowly defined conditions may party independent claim economic share profits derived others granted subjects ’ general protection regulation gdpr particularly important manifestation aimed specifically protecting natural persons pertain also extent standardised manifestation given hinge qualification personal considering principles ethics commission wishes submit following key recommendations action 18 executive summar standards personal 1 ethics commission recommends measures taken ethically indefensible uses examples uses include total surveillance profiling poses threat personal integrity targeted exploitation vulnerabilities addictive designs dark patterns methods influencing political elections incompatible principle democracy vendor lock-in systematic consumer detriment many practices involve trading personal 2 protection law well branches legal system including general private law unfair commercial practices law already provide range instruments prevent ethically indefensible uses however spite widespread impact enormous potential harm little done date terms harnessing power instruments particularly market giants various factors contributing enforcement gap must tackled systematically 3 well steps make front-line players e. g. supervisory authorities aware existing options urgent need legislative framework force fleshed clearly strengthened certain areas examples recommended measures include blacklisting data-specific unfair contract terms fleshing data-specific contractual duties fiduciary nature data-specific torts blacklisting certain data-specific unfair commercial practices introduction much detailed legislative framework profiling scoring trading 4 order allow supervisory authorities take action effectively authorities need significantly better material resources attempts made strengthen formalise cooperation different protection authorities germany thereby ensuring uniform coherent application protection law attempts fail consideration given centralisation market-related supervisory activities within federal-level authority granted broad mandate cooperates closely specialist supervisory authorities authorities land level remain responsible supervisory activities relating sector however 5 ethics commission believes “ ownership ” i.e exclusive modelled ownership tangible assets intellectual property would solve problems currently facing would create problems instead recommends refraining recognition also advises granting subjects copyrightlike economic exploitation respect personal might managed collective societies 6 ethics commission also argues referred “ counter-performance ” provided exchange service even though term sums issue nutshell helped raise awareness among general regardless protection authorities court justice ultimately take regard prohibition gdpr “ tying ” “ bundling ” consent provision service ethics commission believes consumers must offered reasonable alternatives releasing commercial e. g. appropriately designed pay options 19 executive summar 7 stringent requirements limitations imposed personalised risk assessment e. g. “ black ” premiums certain insurance schemes particular processing may intrude intimate areas private life must causal relationship risk difference individual prices charged basis personalised nonpersonalised risk assessments exceed certain percentages determined also stringent requirements respect transparency nondiscrimination protection third parties 8 ethics commission advises federal government consider issues falling heading “ inheritance ” settled federal court justice ’ ruling ephemeral spoken word replaced many situations communications recorded less entirety possibility records handed deceased ’ heirs adds whole dimension privacy risk range mitigating measures taken including imposition obligations service providers quality assurance standards estate planning services national regulations post-mortem protection 9 ethics commission recommends federal government invite social partners work towards common legislative provisions adopted view stepping protection employee based examples best practices existing collective agreements concerns individuals non-standard forms employment also taken account process 10 view benefits could gained digitalising healthcare ethics commission recommends swift expansion infrastructures sector expansion range quality digitalised healthcare services include measures better allow patients exercise informational self-determination measures could taken respect include introduction roll-out electronic health record building participatory process involves relevant stakeholders procedures reviewing assessing medical apps insurer-funded consumer-funded health markets 11 ethics commission calls action significant enforcement gap exists regard statutory protection children young people sphere particular attention paid mandatory provision technologies including effective identity management default settings guarantee reliable protection children young people also familyfriendly i.e neither demand much parents guardians allow even encourage excessive surveillance home environment 12 standards guidelines handling personal vulnerable care-dependent persons introduced provide greater legal certainty professionals care sector time consideration given clarifying relevant legal provisions living wills may also include dispositions regard future processing personal far processing require care-dependent person ’ consent e. g. dementia patients provide legally valid consent 20 executive summar 13 ethics commission believes number binding requirements introduced ensure privacy-friendly design products services principles privacy design privacy default gdpr imposes controllers already put practice upstream manufacturers service pro viders requirements would particularly important regard consumer equipment con text standardised icons also introduced consumers able take informed purchase decisions 14 action must also taken number different levels provide manufacturers adequate incentives implement features privacy-friendly design cludes effective legal remedies pursued parties along entire distribution chain ensure also manufacturers held accountable inadequate application principles privacy design privacy default consideration also given particular requirements built tender specifications procure ment guidelines bodies conditions funding programmes applies privacy-friendly product including training algorithmic 15 debates protection tend quite rightly centre around natural persons important ignore fact companies legal persons must also granted protection almost limitless ability pool together individual pieces means obtaining comprehensive picture company ’ internal operating procedures passed competitors negotiating partners parties interested takeover bid poses variety threats – inter alia sovereignty germany europe – view significant volumes flow third countries many ethics commission ’ recommendations action therefore also apply mutatis mutandis basis legal persons ethics commission believes action must taken federal govern ment step level data-related protection afforded companies .improving controlled access personal 16 ethics commission identifies enormous potential purposes serve interest e. g. improve healthcare provision protection law currently stands acknowledges potential principle granting far-reaching privileges processing personal purposes uncertainty persists however particular regards scope so-called privilege secondary scope counts “ ” context product ethics commission believes appropriate clarifications law necessary rectify situation 17 fragmentation research-specific protection law within germany among member states represents potential obstacle datadriven ethics commission therefore recommends research-specific regulations harmonised federal land level different legal within introducing notification requirement research- specific national law could also bring improvement could establishment clearing house cross-border projects 18 case involving particularly sensitive categories personal e. g. health guidelines produced researchers obtain consent legally compliant manner innovative consent models promoted explicitly recognised law potential options include roll-out consent assistants recognition so-called meta consent alongside endeavours clarify scope privilege secondary 21 executive summar 19 ethics commission supports principle move towards “ healthcare system ” healthcare provision continuously improved making systematic quality-oriented health generated day-to-day basis keeping principles evidence-based medicine progress made direction however greater efforts must made time protect subjects significant potential discrimination exists sensitive categories might involve prohibiting exploitation beyond defined range purposes 20 procedures standards anonymisation pseudonymisation central efforts improve controlled access formerly personal legal presumption compliance standard achieved longer qualify personal “ appropriate safeguards ” provided respect subject ’ would improve legal certainty long way measures accompanied rules – pain criminal penalty – prohibit de-anonymisation anonymised e. g. becomes available would allow re-identification subjects reversal pseudonymisation absence narrowly defined grounds also field synthetic shows enormous promise funding funnelled area 21 fundamentally speaking ethics commission believes innovative management trust schemes hold great potential provided designed robust suited real-life applications compliant protection law broad spectrum models falls heading ranging dashboards perform purely technical function privacy manage ment tools pmt comprehensive consent management services personal man agement services pims underlying aim empower individuals take control personal overburdening decisions beyond capabilities ethics commission recom mends field management trust schemes identified funding priority also wishes make adequate protection legitimate interests parties involved require additional regulatory meas ures level regulatory measures would need secure central functions without operators since scope action would otherwise limited hand also necessary protect individuals parties assume acting interests reality prioritising financial aims interests others event feasible method protection found trust schemes could serve vitally important mediators protection interests economy interests 22 far portability enshrined article 20 gdpr concerned ethics commission recommends industry-specific codes conduct standards formats adopted given underlying purpose article 20 gdpr make straightforward change provider also allow providers access easily important evaluate carefully market impact existing portability analyse potential mech anisms prevented small number providers increase yet market power findings evaluation available expansion scope example cover provided subject real- time porting would seem premature advisable 23 certain sectors example messenger services social networks interoperability interconnectivity obligations might help reduce market entry barriers providers obligations designed asymmetric basis i.e stringency regulation increase step company ’ market share interoperability interconnectivity obligations would also prerequisite building strengthening within europe certain basic services society 22 executive summar debates around access nonpersonal 24 access companies appropriate non- personal appropriate quality key factor growth economy order benefit enhanced access however stakeholders must sufficient degree data-awareness skills necessary make also access proves disproportionately advantageous stakeholders already built largest reserves best infrastructures hand ethics commission therefore wishes stress factors referred always receive due attention discussing whether improve access keeping asisa principle awareness – skills – infrastructures – stocks – access 25 ethics commission therefore supports efforts already initiated level promote improve infrastructures broadest sense term e. g. platforms standards application program ming interfaces elements model contracts support centre recommends federal govern ment efforts continue matched corresponding efforts national level would also advisable set ombudsman ’ office federal level provide assistance support relation nego tiation access agreements dispute settlement 26 ethics commission ascribes enormous impor tance holistically conceived sustainable strategic economic policy outlines effective methods preventing exodus innovative companies acquisition third-country compa nies also excessive dependence third-country infrastructures e. g. server capacities balance must struck context much-needed international cooperation networking one hand resolute assumption responsibility sustaina -ble security prosperity europe backdrop ever-evolving global power dynamic 27 also perspective boosting economy ethics commission see benefit introducing exclusive “ ownership ” “ producer ” instead recommends affording limited third-party effects contractual agreements e. g. restrictions utilisation onward transfer recipient third-party effects could modelled regime protection trade secrets ethics commission also recommends adoption legislative solutions enabling companies cooperate example using trust schemes without running afoul anti-trust law “ partnerships ” 28 accumulated existing value creation e. g. production distribution chains often enormous commercial significance inside outside value creation system many cases however provisions access appear contractual agreements concluded within value creation system unfair and/or inefficient lacking entirely certain cases contractual agreement efforts must therefore made raise awareness among businesses sectors far outside commonly perceived “ economy ” provide practical guidance support e. g. model contracts 29 ethics commission furthermore recommends cautious adaptations current legislative framework first stage process make explicit reference section 311 german civil code bürgerliches gesetzbuch bgb special relationship exists party contributed generation value creation system controller clarifying parties may certain quasi-contractual duties fiduciary nature duties normally include duty enter negotiations fair efficient 23 executive summar access arrangements consideration also given whether additional steps taken could range blacklisting particular contract terms also b2b transactions formulating default provisions contracts introducing sector- specific access 30 ethics commission believes open govern ment ogd concepts hold enormous potential recommends concepts built promoted also recommends series measures promote shift mindset among authorities something yet fully taken place make easier practice share basis ogd concepts measures include establishment relevant infrastructures e. g. platforms also harmonisation improvement existing legal framework currently fragmented sometimes inconsistent 31 nevertheless ethics commission identifies degree tension efforts promote ogd relying principles “ open default ” “ open purposes ” efforts enhance protection protection trade secrets legally enshrined concepts “ privacy default ” ethics commission submits cases doubt priority given duty protecting individuals companies entrusted state often without given choice matter e. g. tax state must deliver duty implementing range different measures may include technical well legal safeguards misuse 32 particular would beneficial develop standard licences model terms conditions public- sector sharing arrangements make mandatory least sector-specific basis standard licenses model terms conditions include clearly defined safeguards third parties affected access arrangement provision also made way ultimately harms interests also still greater accumulation market power part big players would likely undermine competition taxpayer pay twice 33 regards open-data concepts private sector priority given promoting supporting voluntary data-sharing arrangements consideration must given improvement infrastructures e. g. platforms also broad range potential incentives might include certain privileges context tax breaks procurement funding programmes licensing procedures statutory access corresponding obligations grant access considered fall-back options measures fail deliver desired outcomes 34 generally speaking ethics commission believes cautious approach taken introduction statutory access ideally developed sector-by-sector basis sectors level demand analysed include media mobility energy sectors case statutory access even disclosure obligation introduced full impact assessment needs carried examining weighing possible implications include implications protection protection trade secrets investment decisions distribution market power well strategic interests german companies compared companies third countries 35 ethics commission recommends considering enhanced obligations private enterprises grant access interest public-sector purposes business-to-government b2g cautious sector-specific approach however recommended respect well algorithmic algorithms perspective focuses architecture data-driven algorithmic dynamics ’ impacts individuals society ethical legal discourse area typically centres around relationship humans machines particular automation outsourcing increasingly complex operational decision- making processes “ autonomous ” enabled algorithms perspective differs perspective processed system might connection whatsoever persons affected particular individuals may suffer ethically indefensible implications even e. g. train algorithmic system nonpersonal current debates “ algorithmic oversight ” liability central importance respect general standards algorithmic ethics commission distinguishes three different levels algorithmic involvement decision-making based distribution tasks machine specific case question algorithm-based decisions decisions based either whole part obtained using algorithmic calculations b algorithm-driven decisions decisions shaped outputs algorithmic way ’ factual decision-making abilities capacity self-determination restricted c algorithm-determined decisions trigger consequences automatically provision made decision individual case.in opinion ethics commission following principles observed ensure responsible algorithmic ●human-centred design must centred around uses affected decisions must prioritise fundamental freedoms basic needs physical emotional well-being skills ●compatibility core societal values process system design must take account system ’ impact society whole particular effects democratic process citizen- centred nature state action competition future work sovereignty germany europe ●sustainability considerations relating availability skills participation environmental protection sustainable resource management sustainable economic activity becoming increasingly important factors design algorithmic ●quality performance algorithmic must work correctly reliably goals pursued help achieved ●robustness security robust secure system design involves making system secure external threats also protecting humans environment negative impacts may emanate system ●minimisation bias discrimination decision- making patterns upon algorithmic based must source systematic bias cause discriminatory decisions 3 25 executive summar ●transparent explainable comprehensible vitally important ensure users algorithmic understand function explain control also parties affected decision provided sufficient exercise properly challenge decision necessary ●clear accountability structures questions allo cation responsibility accountability including possible liability arising algorithmic must unambiguously resolved system criticality level criticality algorithmic system dictates specific requirements must meet particular regard transparency oversight system criticality determined assessing algorithmic system ’ potential harm basis two-pronged investigation likelihood harm occur severity harm severity harm could potentially sustained example result mistaken decision depends significance legally protected interests affected privacy fundamental life physical integrity prohibition discrimination level potential harm suffered individuals including non-material harm loss utility hard calculate monetary terms number individuals affected total figure harm potentially sustained overall harm sustained society whole may go well beyond straightforward summation harm suffered individuals likelihood harm sustained also influenced properties system question particular role algorithmic system components decision-making process complexity decision effects decision reversibility effects severity likelihood predicted harm may also contingent whether algorithmic operated state private enterprises particularly business context market power wielded system ’ operator.in conclusion ethics commission wishes make following recommendations action basis principles risk-adapted regulatory approach 36 ethics commission recommends adopting risk-adapted regulatory approach algorithmic principle underlying approach follows greater potential harm stringent requirements farreaching intervention means regulatory instruments assessing potential harm sociotechnical system whole must considered words components algorithmic application including people involved phase – example training – implementation application environment evaluation adjustment measures 37 ethics commission recommends potential algorithmic harm individuals and/ society determined uniformly basis universally applicable model purpose legislator develop criteria-based assessment scheme tool determining criticality algorithmic scheme based general ethical legal principles presented ethics commission 38 among things regulatory instruments requirements apply algorithmic include corrective oversight mechanisms specifications transparency explainability comprehensibility ’ results rules allocation responsibility liability using 26 executive summar 39 ethics commission believes useful first stage determining potential harm algorithmic distinguish five levels criticality applications fall lowest levels level 1 associated zero negligible potential harm unnecessary carry special oversight impose requirements general quality requirements apply products irrespective whether incorporate algorithmic 40 applications fall level 2 associated potential harm regulated as-needs basis regulatory instruments connection may include ex-post controls obligation produce publish appropriate risk assessment obligation disclose supervisory bodies also enhanced transparency obligations access individuals affected 41 addition introduction licensing procedures may justified applications fall level 3 associated regular significant potential harm applications fall level 4 associated serious potential harm ethics commission believes applications subject enhanced oversight transparency obligations may extend way publication factors influence algorithmic calculations weightings pool algorithmic decision-making model option “ always-on ” regulatory oversight via live interface system may also required 42 finally complete partial ban imposed applications untenable potential harm level 5 43 ethics commission believes measures proposed implemented regulation algorithmic enshrining general horizontal requirements regulation algorithmic eu-asr horizontal regulation incorporate fundamental requirements algorithmic sytems ethics commission developed particular group together general substantive rules – informed concept system criticality – admissibility design algorithmic transparency individuals affected organisational technical safeguards supervisory institutions structures horizontal instrument fleshed sectoral instruments member state level concept system criticality serving guiding framework 44 process drafting eu-asr recommended incorporate debate best demarcate respective scopes regulation gdpr number factors taken account respect firstly algorithmic may pose specific risks individuals groups even involve processing personal risks may relate assets ownership bodily integrity discrimination secondly regulatory framework introduced future horizontal regulation algorithmic may need flexible risk-adapted current protection regime 27 executive summar instruments 45 ethics commission recommends introduction mandatory labelling scheme algorithmic enhanced criticality level 2 upwards mandatory scheme kind would oblige operators make whether i.e extent algorithmic regardless system criticality operators always obliged comply mandatory labelling scheme risk confusion machine might prove problematic ethical point view 46 individual affected decision able exercise “ meaningful logic involved well scope intended consequences ” algorithmic system cf gdpr respect fully automated also situations involve kind profiling regardless whether decision taken basis later line also expanded future apply algorithm-based decisions differing levels access decisions according system criticality measures may require clarification certain legislative provisions widening regulatory scope level 47 certain cases may appropriate ask operator algorithmic system provide individual explanation decision taken addition general explanation logic procedure scope system main objective provide individuals affected decision comprehensible relevant concrete ethics commission therefore welcomes work carried banner “ explainable ” efforts improve explainability algorithmic particular self-learning recommends federal government fund area 48 view fact certain sectors society whole may affected well individual members also particular parties individually affected algorithmic system entitled access certain types likely kind would granted primarily journalistic purposes order take due account operator ’ interests would need accompanied adequate protective measures ethics commission believes consideration also given granting unconditional access certain circumstances particular algorithmic serious potential harm level 4 state 49 appropriate reasonable impose legal requirement operators algorithmic least potential harm level 2 upwards produce publish proper risk assessment assessment kind also cover processing non-personal well risks fall heading protection particular appraise risks posed respect self- determination privacy bodily integrity personal integrity assets ownership discrimination encompass underlying logic model also methods gauging quality fairness model accuracy example bias rates statistical error overall certain sub-groups exhibited system forecasting/category formation 28 executive summar 50 provide controllers processors greater legal clarity work must done terms fleshing requirements document log sets models level granularity retention periods intended purposes addition operators sensitive applications obliged future document log program runs software may cause lasting harm sets models described way comprehensible employees supervisory institutions carrying oversight measures regards origin sets way pre-processed example optimisation goals pursued using models 51 system operators required standard- setting guarantee minimum level quality technical mathematical-procedural perspective procedural criteria imposed must ensure algorithmically derived results obtained correct lawful manner purpose quality criteria could imposed particular regards corrective control mechanisms quality system security example would appropriate impose quality criteria relationship algorithmic processing outcomes obtain outcomes 52 ethics commission believes necessary first step clarify flesh greater detail scope legal consequences article 22 gdpr relation algorithmic context decision-making second step ethics commission recommends introduction additional protective mechanisms algorithm-based algorithm-driven decision-making since influence real-life settings may almost significant algorithm-determined applications prohibitory principle followed date article 22 gdpr replaced flexible risk-adapted regulatory framework provides adequate guarantees regards protection individuals particular profiling concerned options individuals take action mistakes made jeopardised 53 consideration given expanding scope anti-discrimination legislation cover specific situations individual discriminated basis automated analysis automated decision-making procedure addition legislator take effective steps prevent discrimination basis group characteristics qualify protected characteristics law discrimination often currently qualify indirect discrimination basis protected characteristic 54 case algorithmic regular significant level 3 even serious potential harm level 4 would useful – supplement existing regulations – covered licensing procedures preliminary checks carried supervisory institutions interests preventing harm individuals affected certain sections population society whole institutions 55 ethics commission recommends federal government expand realign competencies existing supervisory institutions structures necessary set ones official supervisory tasks powers primarily entrusted sectoral supervisory authorities already built wealth expert knowledge relevant sector ensuring competent authorities financial technical resources need particularly important factor respect 29 executive summar 56 ethics commission also recommends federal government set national centre competence algorithmic centre act repository technical regulatory expertise assist sectoral supervisory authorities task monitoring algorithmic ensure compliance law 57 ethics commission believes initiatives involving technical statistical quality standards test procedures audits differentiated according critical application areas necessary worthy support test procedures kind – provided designed adequately meaningful reliable secure – may make vital contribution future auditability algorithmic 58 opinion ethics commission particular attention paid innovative forms coregulation self-regulation alongside complement forms state regulation recommends federal government examine various models co-regulation self-regulation potentially useful solution certain situations 59 ethics commission believes option worth considering might require operators law inspired “ comply explain ” regulatory model sign declaration confirming willingness comply algorithmic accountability code independ ent commission equal representation – must free state influence – could set develop code kind would apply binding basis operators algorithmic appropriate involvement civil society representatives drafting code must guaranteed 60 voluntary mandatory evidence protective measures form specific quality seal may also serve guarantee consumers algorithmic system question reliable time providing incentive developers operators develop reliable 61 ethics commission takes view companies authorities operating critical algorithmic obliged future appoint contact person way companies specific currently obliged appoint protection officer communications authorities routed contact person also subject duty cooperation 62 ensure official audits algorithmic take due account interests civil society companies affected suitable advisory boards set within sectoral supervisory authorities 63 opinion ethics commission technical standards adopted accredited standardisation organisations generally useful measure occupying intermediate state regulation purely private self-regulation therefore recommends federal government engage appropriate efforts towards adoption standards 64 system granting competitors competition associations consumer associations file action important feature german legal landscape many years could play key role civil society oversight algorithmic particular private kind could allow civil 30 executive summar society players legitimate mandate enforce compliance legal provisions area contract law fair trading law anti-discrimination law without needing rely authorities take action without needing wait individuals authorise special topic algorithmic media intermediaries 65 given specific risks posed media intermediaries act gatekeepers democracy ethics commission recommends options examined countering risks also regard influencing legislation → see recommendation 43 whole gamut risk mitigation measures considered extending ex-ante controls e.g form licensing procedure 66 national legislator constitutional obligation protect democratic system dangers free democratic pluralistic formation opinions may created providers act gatekeepers establishing binding normative framework media ethics commission believes small number operators concerned obliged algorithmic allow users least additional option access unbiased balanced selection posts embodies pluralism opinion 67 federal government consider measures take due account risks typically encountered media sector respect media intermediaries also respect providers act gatekeepers whose associated lower potential harm measures might include mechanisms enhancing transparency example ensuring available technical procedures select rank news stories introducing labelling obligations social bots establishing post countering responses timelines algorithmic state bodies 68 state must interests citizens make best available technologies including algorithmic must also exercise particular prudence actions view obligation preserve fundamental act role model general rule therefore algorithmic authorities assessed basis criticality model particularly sensitive entailing least comprehensive risk assessment 69 areas law-making dispensation justice algorithmic may peripheral tasks particular algorithmic must undermine functional independence courts democratic process way contrast enormous potential exists algorithmic connection administrative tasks particular relating provision services benefits legislator take due account fact giving green light greater number partially fully automated administrative procedures cautious consideration therefore given expanding scope section 35a german administrative procedures act verwaltungsverfahrensgesetz vwvfg couched overly restrictive terms corresponding provisions statutory law measures must accompanied adequate steps protect citizens 31 executive summar 70 decisions taken state basis algorithmic must still transparent must still possible provide justifications may necessary clarify expand existing legislation freedom transparency order achieve goals furthermore algorithmic negate principle decisions made authorities must generally justified individually contrary principle may impose limits overly complex algorithmic finally greater priority accorded opensource solutions since latter may significantly enhance transparency government actions 71 ethical point view general non-compliance rules regulations time however automated “ total ” enforcement law raises number different ethical concerns general rule therefore designed way override technical enforcement specific case balance struck potential transgression automated perhaps preventive enforcement measure must times meet requirements proportionality principle liability algorithmic 72 liability damages alongside criminal responsibility administrative sanctions vital component ethically sound regulatory framework algorithmic already apparent today algorithmic pose challenges liability law currently stands inter alia complexity dynamism growing “ autonomy ” ethics commission therefore recommends current provisions liability law undergo in-depth checks necessary revisions scope checks revisions restricted basis narrowly defined technological features machine 73 proposal future system legal personality would granted high-autonomy algorithmic would liable damages “ electronic person ” pursued far concept protagonists based purported equivalence machine ethically indefensible far boils introducing type company company law fact solve pertinent problems 74 way contrast harm caused autonomous way functionally equivalent employment auxiliaries operator ’ liability making correspond otherwise existing vicarious liability regime principal auxiliaries cf particular section 278 german civil code example bank uses autonomous system check creditworthiness customers liable towards least extent would employee perform task 75 debate currently stands appears highly likely appropriate amendments need made product liability directive dates back 1980s connection established product safety standards addition certain changes may need made rules relating fault-based liability and/ bases strict liability may need introduced case necessary determine liability regime appropriate particular types products services exact shape regime take depending criticality relevant algorithmic system consideration also given innovative liability concepts currently developed level path ethics commission examined great many different questions course work discussions questions raised ones turn alone serve indicate opinion serve one many building blocks larger edifice debate ethics law continue many years come ethics commission takes view important remember ethics law democracy must serve shaping force change broader sense specifically field achieve goal interdisciplinary discourse politics society required care must taken ensure rules regulations adopted open enough retain regulatory clout ability adapt even face fast-paced changes technologies business models rules regulations must enforced effectively means appropriate instruments procedures structures latter must make possible intervene promptly response infringements undesirable developments.in global contest future technologies germany europe confronted value models society cultures differ widely ethics commission supports “ path ” followed date defining feature technologies consistent alignment values fundamental particular enshrined union ’ charter fundamental council europe ’ convention protection fundamental freedoms ethics commission believes state particular responsibility develop enforce ethical benchmarks sphere reflect value system order deliver promise citizens must act political economic strength global stage excessive dependence others turns nation rule taker rather rule maker resulting citizens nation subject resulting citizens nation subject requirements imposed players elsewhere world private corporations part exempt democratic legitimacy oversight embarking efforts safeguard sovereignty germany europe long term therefore politically far-sighted necessity also expression ethical responsibility 4 part introduction 34 part ntroduction guiding motifs society experiencing profound changes brought digitalisation innovative data-based technologies may benefit us individual wider societal levels well potentially boosting economic productivity promoting sustainability catalysing huge strides forward terms scientific progress time however digitalisation poses risks fundamental freedoms raises wide range ethical legal questions centring around two wider issues role want technologies play design want ensure transformation serves good society whole society elected political representatives must engage debate shape data-based technologies including germany ’ federal government set ethics commission datenethikkommission 18 july 2018. given one-year mandate develop ethical benchmarks guidelines well specific recommendations action aiming protecting individual preserving social cohesion safeguarding promoting prosperity age starting point federal government presented ethics commission number key questions clustered around three main topics algorithm-based decision-making adm opinion ethics commission however merely one among many possible variants algorithmic system much common terms ethical legal questions raises mind ethics commission structured work two different headings algorithmic broader sense .in preparing opinion ethics commission inspired following guiding motifs ●ensuring human-centred value-oriented design ●fostering skills critical reflection world ●enhancing protection individual freedom self- determination integrity ●fostering responsible utilisation compatible good ●introducing risk-adapted regulation effective oversight algorithmic ●safeguarding promoting democracy social cohesion ●aligning strategies sustainability goals ●strengthening sovereignty germany europe 35 1. mission basic understanding 1. mission basic understanding society experiencing profound changes brought digitalisation innovative data-based technologies may benefit us individual wider societal levels well potentially boosting economic productivity promoting sustainability catalysing huge strides forward terms scientific progress cases already happened transformation offers tremendous opportunities countries particular germany closely networked high-tech economy means german companies coming increasing competitive pressure international market time already becoming apparent digitalisation poses risks fundamental freedoms raises wide range ethical legal questions centring around two wider issues role want technologies play design want ensure transformation serves good individuals society whole society elected political representatives must engage debate shape design data-based technologies including 18 july federal government set ethics commission datenethikkommission named 16 members → see annex 2 christiane wendehorst christiane woopen appointed co-spokespersons ethics commission given one-year mandate develop ethical benchmarks guidelines aiming protecting individual preserving social cohesion safeguarding promoting prosperity age also asked put forward specific recommendations action suggestions possible legislation view allowing ethical guidelines observed implemented supervised starting point federal government presented ethics commission number key questions → see annex 1 clustered around three main topics algorithm-based decision-making ii iii context “ ” understood ethics commission catch-all term technologies related applications based methods involve machine processing potentially large heterogeneous sets complex procedure mimics results obtained procedure may applied automated way important methods underpinning one aspect much wider computer science landscape include sub-symbolic pattern recognition machine computer-based knowledge representation knowledge engineering turn encompasses heuristic search methods inference techniques action planning ethics commission however believes would wrong restrict ethical legal debate alone merely one among many possible variants algorithmic system thus represents subset field types algorithmic share number features may give rise ethical problems meaning regulations focused alone would tackle part problem feature self-learning foreground brings specific challenges due consideration must given risk assessment stage time however many features besides self-learning require special attention following arguments therefore relate algorithmic kinds applications rarely based single algorithm examining algorithms isolation rarely meaningful ethical appraisal must based sociotechnical system whole words components algorithmic application including people involved phase – example training – implementation application environment evaluation adjustment measures 36 part ntroduction 2. working method september september ethics commission met monthly basis discussed examples cases technologies range different sectors analysed terms involved ethical legal issues raised findings obtained work fundamental debates made possible identify overarching topics questions starting point ethical appraisal framework drafting specific recommendations future political legislative action early october response policy paper federal government ethics commission put forward two specific recommendations points included strategy recommendations taken federal government november ethics commission issued another recommendation calling roll-out electronic health record building participatory process.1 ethics commission involved two conferences first took place 7 february federal ministry justice consumer protection bundesministerium der justiz und für verbraucherschutz centred around issue “ selfdetermination external determination age ” second – international round table title “ towards ethical shaping future ” – held 9 may federal ministry interior building community bundesministerium des innern für bau und heimat events allowed ethics commission engage in-depth discussions experts stakeholders well members interested citizens.2 1 documents available ethics commission ’ website www.datenethikkommission.de 2 conferences including video recordings found ethics commission ’ website www.datenethikkommission.de .on 14 november federal government ’ digitalklausur exchange views took place federal chancellor members federal government two co-spokespersons ethics commission ad-hoc discussions also held individual members federal government addition ethics commission organised expert hearings consultation meetings institutions bodies working related topics including study commission “ ” commission experts competition law 4.0 federal government ’ council advisory council consumer affairs many one defining features ethics commission work advisory activities fully independent free external political influence viewpoints outlined report reflect either personal opinions expressed ethics commission ’ individual members opinions emerged internal discussions within institutional members ethics commission adopted recommendations report consensus 37 3. objectives scope report 3. objectives scope report goal pursued ethics commission publishing report ethical legal framework order confront challenges posed technologies main concern ensure fundamental conditions place free democratic basic order preserved potential exists leveraged sustainability-oriented goals achieved social market economy flourish given increase volume personal collected automated methods process different purposes one main priorities ethics commission reconcile need protect individual ’ fundamental freedoms – including self-determination integrity – need promote progress prosperity safeguarding democracy shaping society fit future protecting individuals misuse discrimination guaranteeing security parties involved tasks fall squarely within remit state governed rule law effective regulations must adopted institutions set purpose time however state must facilitate emergence innovative business models safeguard future prosperity everyone.the ethics commission believes digitalisation – particular rapidly increasing availability complex algorithmic including – holds enormous potential technical social innovation achievement un ’ sustainable goals promising avenues action include promoting health humanising world work designing sustainable cities communities providing decent education implementing effective climate protection measures time however must forget major risks may face individuals society whole free democratic basic order connection extensive technologies risks include possibility high- granularity profiling using techniques online tracking voice analysis remote job interviews even diagnosis pathological mental conditions basis social media posts potential profiles exploited purpose controlling manipulating people either small individual pricing larger manipulating democratic opinion-building processes “ micro- targeting ” potential discrimination different social groups ability delegate responsibility machines factors mind ethics commission believes must actively shape future way realise potentials avoiding risks ethics commission advocates multi-step approach achieving goals first step ethical reflection value activity environment shaped reaffirmation key ethical principles precepts upon society founded → part b view ethics commission key questions divided questions concentrate mainly “ perspective ” questions primarily focused algorithmic “ algorithms perspective ” two perspectives represent ethical discourses complement contingent upon also reflected different governance instruments → part 38 part ntroduction section devoted perspective → part e ethics commission outlines general ethical principles governance → e 1 particular ethical principles governing obligations → e 2 serve basis series specific recommendations action regarding access → e 3 5 section devoted algorithms perspective → part f ethics commission sets general ethical requirements design algorithmic → f 2 risk-adapted regulation → f 3 instruments institutions would required implement regulations kind examined detail summarised recommendations legislator → f 4 8 shared basic understanding technical parameters relationships → part c serves essential foundation considerations kind report ends plea federal government follow “ path ” → part g per mission ethics commission ’ recommendations targeted primarily german federal government associated institutions certain points however target audience widened include stakeholders example länder municipalities institutions enterprises federal government always secondary target audience recommendations given underlying recommendation encourage support stakeholders efforts recommendations also viewed context institutions rules put place international level context developments arenas cases ethics commission suggests recommendation implemented international level interpreted recommendation german federal government make vigorous future-oriented contribution debate taking place within europe across globe ethical legal principlespart b 40 part b e thical legal principles 1. fundamental value agency given fast-paced technologies including self-learning algorithmic “ ” incorporate certain functions outperform abilities humans elementary question raised whether agency poses ethically relevant value transcends considerations effectiveness efficiency inherently preferable functioning machine question pressing momentum internal logic international competition part dictated solely goal maximising economic efficiency agency derives basic value moral significance provide reasons one ’ actions decide whether perform must bear responsibility actions taking action individuals develop realise full potential accordance capabilities preferences understanding meaningful life dimension meaning lends value activity could never claimed functioning technical ever means achieving goal humans set even – hypothetically speaking – humans decide algorithmic could set goals allowing would goal set humans technical may therefore component activity may even ethically required certain cases never possible technical replace moral dimension agency completely agency drive develop living characterised multi-dimensional nature although conceptions man espoused different cultures different faiths vary significantly incorporate dimension living moral responsibility despite differences respective answers embrace question meaning life whereas technical merely function must weigh many different criteria identifying cases preference given activity algorithmic basic principle higher level effectivity prioritised regard performance certain limited functions effectiveness rule supreme must place material restrictions ability humans take action form self-development must take second place basic ethical dimension meaningful flourishing life individual member society example even possible cared effectively robot another care robot allowed replace element attention affection person needing care time however robots perform care-related tasks alongside humans may deemed expedient makes situation significantly safer person receiving care yet effectiveness gains technical must take back seat entail intrusion privacy personal integrity individual example force employee modify work processes order maximise effectiveness people must allowed retain subjectivity rather morphing objects “ acted upon ” machines humans morally responsible actions escaping moral dimension humans responsible goals pursue means pursue reasons dimension must always taken account designing technologically shaped future time notion serve humans rather humans subservient taken incontrovertible fact germany ’ constitutional system founded understanding adheres tradition europe ’ cultural intellectual history 41 b 2. relationship ethics law 2. relationship ethics law exponential technical developments relating collection deployment algorithmic increasingly shaping life every individual aspects social coexistence developments give rise far-reaching profound questions answers questions must guided fundamental legal ethical principles democratic society undertakes uphold benchmarks guiding principles underpinning processes society shapes shape various sectors – economy education spaces healthcare finance transport energy – fundamentally ethical nature although liberal characterised high degree moral pluralism common ethical framework nevertheless established constitutional law especially fundamental far relationship state individual concerned significance ethical legal framework relation individual case event conflict differing values fundamental always clear-cut yet relativise binding nature fundamental importance ethical foundation community instead merely goes prove crucial importance open ongoing debate future shape society serves basis democratic decision-making processes acknowledge possibility different answers within framework constitution ethics equated one-to-one basis law words everything relevant ethical perspective enshrined legislation conversely provisions law motivated purely pragmatic considerations ethically imperative nevertheless legislation must times heedful potential ethical implications must live ethical standards – least requirements outlined constitutional law.the ethics commission holds view regulation necessary replaced ethical principles guidelines cases constitutionally developed principle materiality requires enactment form parliamentary legislation democratically legitimate rules enforced anyone internet governance also governance society algorithmic including become increasingly normal feature daily lives lead together society must also develop enforce rules govern calls ongoing debate also – particularly cases fundamental threat – parliamentary debate legislative initiatives given past experiences law enforcement internet sphere view experience power tends accumulated hands large corporations certain sectors markets dominated technologies systematic move away enforceable rules towards voluntary regulation would appear mistake time regulation must unduly inhibit technological social innovation dynamic market growth overly rigid laws attempt regulate every last detail situation may place stranglehold progress increase red tape extent innovative processes germany longer keep pace rate technological international stage hand regulatory frameworks must protect fundamental freedoms create legal certainty essential first stage building system within citizens companies institutions trust fact transformation society guided ethical principles addition “ toolbox-like ” nature legal system options regulating matters many different levels ranging acts ordinances codes self-governance options voluntary obligations makes suitable creating framework adaptable keep technological progress 42 part b e thical legal principles however need guidance goes far beyond regulatory sphere mind many different stakeholders – professional groups companies advisory boards national regional international level – responded manifold upheavals drafting ethical codes sets guiding ethical principles cases ensuing debate ethics commission welcomes diversity stakeholders taking action number voices heard discussion process digitalisation shaped ethical way since highlights indispensability debate everyone take responsibility flourishing future lives together keeping mission assigned coalition agreement ethics commission based recommendations “ framework develop policy deal algorithms innovations ” precepts constitutional law also cross-cutting ethical principles apply differing degrees areas society principles briefly outlined below.1 1 following approach ethics commission adhering basic principles endorsed group ethics science technologies ege opinion ege statement robotics “ autonomous ” available /ec.europa.eu/research/ege/pdf/ege_ai_statement_2018.pdf 43 b 3. general ethical legal principles 3. general ethical legal principles 3.1 dignity dignity ethical viewpoint synonymous unconditional value every enshrined “ fundamental constitutional principle ” constitutional order foundational supreme importance follows principle dignity every individual merits respect regardless attributes achievements protecting value inherent every need acquired also implies beings ranked classifying system across various spheres life activities “ super scoring ” labelled like object price treated accordingly fact individual rather pattern made points must also borne mind times situations behaviour measured measurements processed algorithmic algorithmic must therefore always designed way cater ’ claim individuality acknowledging dignity involves recognising humans must always “ superior ” i.e must completely irrevocably subordinated technical opportunities configuration intervention may localised different levels specific application principle sovereignty action must upheld humans hold responsibility human/machine interactions must regarded defective beings need optimised perfected machine instead algorithmic aims realising ideas objectives effectively rapidly fewer errors.protecting dignity also involves ensuring relational misled nature relationship example would wrong systematically deceived thinking speaking another actually bot psychological integrity individual particularly important factor protecting dignity rules datadriven manipulative purposes particularly draw comprehensive highly granular personality profiles also rules algorithmic discriminate systematically individuals groups example “ downgrading ” preventing using certain services ethically untenable reasons systematically misleading participate democratic discourse 3.2 self-determination opportunity self-determination inextricably linked dignity humans express freedom determining life goals way lead lives basis determining developing enacting essence self society takes freedom seriously must put place framework within citizens develop freely respect ’ freedom despite differences example people lead self-determined life develop freedom technical must restrict control avenues action without ethically meaningful reason self-determination must viewed solely individualistic lens – humans relational beings whose life unfolds social interactions others basis manifold reciprocal links influences rules govern interactions shaped time cultural socionormative framework serves basis life together society also shaped law democratic society especially imbalances power prevail 44 part b e thical legal principles third parties collected individual difficult becomes individual act unselfconsciously social situations even reinvent completely steps must taken ensure collection evaluation practices result personal social profiles routinely created multiple locations thereby “ cementing ” particular version individual selfdetermination therefore also encompasses develop alter one ’ identity possibility starting one ’ life afresh self-determination thus also includes individual ’ decide perceived prevent misrepresentations another vital aspect self-determination people must allowed assume responsibility must justice task responsibility always lies – institutionally enshrined necessary – never machine even technical system apply inferences based automated evaluations i.e whether loan granted responsibility developing using system ethically sound manner must lie humans important manifestation self-determination informational self-determination includes individual ’ determine collect personal may purpose informational self-determination allows individual protect freedom action privacy extent deems important also determine personality wants perceived treated public.in era digitalisation special importance individuals self-determined actors society goes beyond informational self-determination term self-determination refers encompasses skills needed individual determine basis interacting environment unfold personality interactive way certain circumstances may also include self-determined economic exploitation individual ’ assets self-determined governance non-personal example generated operating certain devices self-determination always goes hand hand accountability ethics commission takes view businesses legal persons also entitled self-determination legal persons invoke concept dignity granted article 1 paragraph 1 german basic law grundgesetz gg protected framework general personality therefore barred referring associated core area personality enjoys protection article 2 paragraph 1 basic law conjunction article 19 paragraph 3 basic law however grant legal persons protected personality also incorporates informational self-determination ability consumers take self-determined action conscious consumption decisions vital prerequisite optimum resource allocation maximisation good macroeconomic level erosion skills needed consumers exercise self-determination example excessive decision-making assistants associated habituation effects raises ethical questions regarding external determination freedom individuals take decisions also regarding ability small number market-dominant firms exert control society 45 b 3. general ethical legal principles 3.3 privacy protection dignity self-determination closely materially linked protection privacy individual ’ determine may access personal relating purpose may informational self-determination see section 3.2 justified supreme ethical importance ability prevent intrusions one ’ private sphere also appear certainty one ’ privacy protected efforts protect dignity must include legislative measures regulate responsible personal aspect privacy need preserve integrity individual ’ personal identity example integrity may violated algorithmic system – using collected entirely different purposes – “ calculates ” personality individual together preferences proclivities system operator uses calculations purposes regardless even contrary individual ’ given different spheres society shaped data-driven technologies important us increase amount attention pay many people willing make personal available semi-public receive certain products services return wish contribute good merely telling think twice disclosing personal effective instead effective regulations must adopted people rely fact responsibly steps taken prevent ethically unacceptable uses 3.4 security algorithmic also give rise crucial security questions context may promote jeopardise user security security relevant ethical legal perspective role plays protecting high-ranking values individual ’ physical mental health privacy security peace free equal democratic elections security relate collecting using means concept also bearing protection privacy major scandals hit headlines recent years made privacy breaches personal manipulative purposes far-reaching – sometimes political – consequences consideration must also given physical emotional safety individual operates uses algorithmic system stringent requirements apply respect e.g connection human/machine interactions robot carer example must ensured neither person receiving care person providing care suffer harm terms physical mental integrity algorithmic may also impact environmental safety malfunctions algorithmically controlled infrastructures e.g traffic energy water supply infrastructures may cause enormous amounts damage algorithmic may also innately unsafe causing malfunctions even functioning gateways malicious attacks manipulation even beyond inherent system vulnerabilities kind must forgotten algorithmic system could misused harmful purposes 46 part b e thical legal principles 3.5 democracy technologies complex manner systemically relevant fundamental particular freedom expression informational self-determination confidentiality telecommunications freedom assembly association freedom occupation property democracy safeguarding diversity open societal debate free equal elections example social media sites serve low-threshold opportunity every citizen participate debates shape future principle welcomed time however risk may manipulation radicalisation state take decisive action counter risks adopting rules setting institutions capable preventing undesirable developments misuse also undeniable fact rise internet accompanied economic decline journalism privately funded plurality yet electronic sphere way considered valid replacement role played journalism democracy namely “ fourth estate ” “ watchdog democracy ” – i.e instance exercises control power claim truth basis systematic independent investigations criticism certain circumstances powerful media intermediaries playing gatekeeper function may exert controlling influence democratic formation posing significant threat democracy – based ethical considerations provisions constitutional law – must countered legislative means.education training must also play prominent role safeguarding free democratic basic order since influence wide variety ways participation citizens shaping society – process critical fundamental importance democracy citizens ’ understanding appraisal socially relevant interrelationships developments – ultimately – level confidence future shaped founded values education training must impart technical mathematical skills also skills fields ethics law economics social sciences 3.6 justice solidarity observance principles justice society institutions another fundamental factor allows us live together peace prosperity freedom democracy placed enormous influence – economic clout societal sway results former – hands small number large companies raised questions fair economic order availability large volumes digitalisation processes e.g workplace healthcare sector raises questions relating equitable access distributive justice however example relation income provision healthcare developments may mean scarce resources distributed fairly may also mean individual groups people suffer disadvantage discrimination also close link justice opportunities participation stronger participatory processes also supported tools play important role promoting social innovations time technology-induced social upheavals finally questions justice arise connection situations algorithmic – particular self-learning algorithmic – means individuals groups people suffer discrimination justifying reason 47 b 3. general ethical legal principles assignment responsibility accountability indispensable feature democratic state rule law adequate level transparency explainability essential prerequisite auditing algorithmic appropriately basis real potential harm opportunities seeking legal recourse necessary holding another party accountable i.e liable must also available certain conditions world stands today access resources via internet fundamental requirement thus also social participation part provision remit state obliged ensure citizens access up-to-date internet infrastructure anywhere country adequate extent using either mobile connection part educational remit must provide citizens skills needed self-determined navigation world accurate appraisal opportunities risks internet opportunities participation promote social cohesion also based fundamental attitude societal solidarity integration latter institutional framework technologies may strengthen solidarity may also weaken destroy algorithmic certain spheres society insurance sector provision opportunities social participation care must taken avoid systematic weakening solidarity may cases caused subtle effects example perfectly possible data-driven differentiation unequal treatment appears plausible justified individual cases lead overall reduction solidarity certain groups people may particularly reliant society ’ support.3.7 sustainability technologies offer huge potential terms efficient resource management innovative business models economic aspect generally attracts lion ’ share attention general debates topic date however less interest shown question whether technologies also contribute economic sustainability consideration must also given issues relating ecological social sustainability un adopted 17 sustainable goals relating economic social ecological aspects apply un member states achieved 2030. technologies may make easier aim pursued international telecommunication union itu “ good ” initiative example similarly german advisory council global change wissenschaftliche beirat der bundesregierung globale umweltveränderungen recently outlined vision ai-based highly granular network environmental sensors would allow unprecedented “ comprehensive real-time monitoring natural earth condition ” vital building future sustainability policy yet technologies conserve resources also consume example ever-rising demand electricity reliance products certain rare earth elements available limited quantities certain countries rare-earth mining causes enormous damage environment raises questions regard sustainable economic ecological also questions international justice concerning natural resources global responsibility future generations 48 part b e thical legal principles knowledge skills also resources whose sustainability must safeguarded technologies concomitant reduction tasks need performed humans mean individuals gain certain skills lose competences debate must held responsibility towards next generation measures required preserve develop certain skills avenues independent action noted elsewhere opinion need regular comprehensive technological impact assessments assessments must also consider sustainability technologies various manifestations incumbent upon legislator ensure responsibility sustainability incorporated rules govern economy algorithmic example introduction obligation disclose entire energy footprint energy-hungry blockchain system pursuit sustainability goals set united nations particular investments economy algorithmic allocating government funding priority given economic gains short-term nature algorithmic purposes recording monitoring environmental impacts developments optimising reducing energy resource consumption addition done promote sustainability-oriented social innovations foster social creativity participation part c technical foundations part c echnical foundations 50 data-intensive applications lasting impact living working environment economy scientific endeavours society well permanently tethered smartphones search engines daily basis rely recommendation software send text voice messages family friends regulate temperature home remotely allow navigation devices guide us one place another able series technological developments occurred past decades fundamental technical concepts underpinning developments described aim provide comprehensive account highlight key points basis identifying resulting problems starting points potential governance approaches c 1. status quo 51 1. status quo entirely fields application opened thanks improved performance miniaturisation physical components hardware store process along continual enhancements wired wireless connectivity smartphones tablets wearables gradually infiltrating workplaces homes along sensors actuators cases “ autonomous ” robots many locations internet “ always ” thanks mobile access making possible – e. g. combination various sensors smartphones geolocators gyrosensors cameras microphones etc – input text also upload image video audio recordings internet time almost anywhere penetration makes possible communicate social networking sites also link devices internet things iot become impossible draw dividing line analogue worlds former contains components transfer latter becoming ever widely available analogue world bringing two closer closer together creating hybrid world .data volumes increasing exponentially thanks comprehensive arrays sensors iot falling price storage capacity specialised tools needed process large volumes time accumulation much together availability high-performance hardware promoted widespread machine procedures achieved impressive results example field speech image recognition speech recognition video processing seen huge leaps forward terms performance potential boundaries reality computer-generated become blurred happens people longer sure whether talking speech bot whether watching normal video recording “ deep fake ” i.e synthesised image saying things real person never actually said part c echnical foundations 52 2. system elements 2.1 2.1.1 definition properties keeping ethics commission ’ mission report concentrates machine-readable made stream binary electrical impulses may transient signals exist instant e. g. control impulse technical system persistent stored medium multifaceted word “ ” umbrella term encompasses enormous range manifestations example categorised basis type e. g. binary nominal ordinal metric textual process generate e. g. survey sensor sector collected e. g. financial weather function system e. g. login training categorised basis level processing yet processed referred “ raw ” processed referred “ structured ” “ unstructured ” depending level structuring normalisation function input system output system output may turn function input another system also represent assets multimedia units cryptocurrency distinction enormous legal significance personal non-personal data.the terms “ ” “ ” always synonymous make sense binary electrical impulses form basis i.e “ ” necessary know context semantics meaning one possible context would origin generated signal – knowing precise sensor emitted signal example term “ semantics ” refers contained certain sequence binary signals example “ 4 ” appears survey may equally well represent number children household number tubes toothpaste bought past six months potential sources context semantics include metadata domain tables ontologies identifiers technical specifications supplement values whenever term “ ” remainder report familiarity context semantics always implied varying quality purpose – accurately contained therein – reflect reality accurately possible example done assigning attributes exhibited entities real life correct entities world objects also many types intended express likelihood something happening reality either future types intended construct hypothetical reality others relation reality whatsoever cases pool may contain errors distinction made errors cases expected unsuitable achieving specific goal example performing particular analysis e. g. insufficiently granular outdated incomplete way quality decisive importance data-driven since even perfect algorithm deliver high-quality results receives poor input i.e inaccurate inadequate quality value relevant quality dimensions quality level depend specific see figure 1 c 2. system elements 53 2.1.2 management pre-existing entity – created process collecting preparing processing involves many different decisions implications future example potential might gained may irretrievably lost stored without context semantics careful management necessary avoid situations kind collating different sources vital ensure collation possible technical semantic perspective “ interoperability ” different sources must mapped way reflects semantics cases interoperability particularly important efforts made achieve standardisation technical specifications formats descriptive metadata etc. reference play important role respect i.e standardised schemes ontologies fall remit national international institutions e. g. international classification diseases icd published 1 doug laney 3d management controlling volume velocity variety meta group inc. 2001.2.1.3 big small term “ big ” refer separate type instead methodological approach identification relationships laney1 famously “ three vs ” – volume velocity variety – define approach still incipient stages large volumes varied potentially variety sources generated high velocity often real time special technologies needed process large volumes rapidly changing vary terms nature quality analysis large sets “ big ” particularly well suited situations necessary identify promising large number potential correlations field medical example helpful start big methods identify number likely candidates long list environmental factors might increase risk disease going perform costly high-precision experiments studies investigate candidates specific problem associated approach initially shows correlations rather causalities completely unsuitable candidates may therefore identified shape perfect colour brilliant surface glossy taste — blemishes —shape — colour — surface — taste intense blemishes noneuse photographuse strawberry mousse figure 1 example different use-specific quality requirements part c echnical foundations 54 many areas volumes available never large enough allow analysis using big methods example client base small medium-sized company may never exceed 200 customers number political parties one country rarely reaches three figures suitable “ small ” analytical methods also extract great deal knowledge quantity matters instead decisive factor availability suitable tools make possible combine adequately high quality quantities sufficient task hand basis effective analysis 2.2 processing 2.2.1 algorithms protection point view term “ processing ” refers entire sequence actions generation extraction storage transformation actual article 4 2 gdpr way contrast mathematical technical sciences mainly deploy term refer following arguments based latter two understandings term method processing follows ipo input processing output model – enter system input processed leave output form internal processing within ipo system based algorithm words operational processing sequence specifies procedure series different processing steps aim achieving desired result successive transformations inputs algorithms around since time euclid specified method easily calculating greatest common divisor two natural numbers word “ algorithm ” derived name arabian mathematician al-khwarizmi formerly latinised “ algorithmi ” published collection calculation rules solving algebraic equations 830 ad thereabouts.it hard overestimate importance term “ algorithm ” modern computer science solve particular problem processing algorithm must implemented correctly also productively presumes knowledge algorithm many cases however algorithm ultimately deliver desired result yet known first important task find suitable algorithm many situations practical relevance processing specifications derived directly i.e deduced specialist knowledge known models legislative provisions situations understanding context yet sophisticated enough allow described using less simple mathematical formulae framework understanding absent various strategies applied identify algorithm include random chance trial error data-based inference latter approach follows principle induction – attempt made infer general rule individual cases i.e general rule found solve question assumed suitable algorithm worth remembering may well several suitable rules furthermore result process induction may necessarily correct result inferred individual cases may partially wholly inaccurate c 2. system elements 55 2.2.2 statistical inference central concern statistics drawing inferences statistical inference procedures applied sets investigate problems lack known inherent logic importantly however also problems random chance forms integral part process modelled examples would estimating probability rain following day identifying highprobability prospects particular product many different statistical inference methods choose among starting various forms regression linear regression logistic regression regularisation ridge regression moving support-vector machines svm bayesian networks rule learners aprioiri cart random forest ending neural networks nn procedures suitable extracting available specifically designed solve regression questions example estimating future child based parents whereas others svm cart nn classification-type question e. g. pregnant/not pregnant dog/cat whether represent suitable means answering question depends many factors including volume type.besides methods induction statistics offers broad set tools measuring quality results estimations obtained measurements estimate potential errors monitor actual errors practice thus estimate child ’ future stated 175 cm deviation range +/– 4 cm pregnancy test yields positive result result might deemed 93 accurate pregnancy test good example need monitor two different parameters number false positives e. g. women pregnant pregnancy test positive number false negatives e. g. woman pregnant pregnancy test negative ideal statistical procedure would never result errors practice necessary weigh severity two errors decide false rate minimised worse woman find later date fact pregnant told woman told pregnant true two error types minimised time since generally case lower frequency one higher frequency balance must struck look different depending context part c echnical foundations 56 quality characteristics methods basis assessing quality results even possible guarantee quality results obtained using certain methods example estimation procedures uniformly minimum-variance unbiased estimator umvue ensure best possible results obtained using available regression using umvue-based parameters supplies result stating expected child 175 cm +/− 4 cm estimator would achieved smaller error similarly support-vector machine model determined basis relevant provided model found guaranteed best possible model method question certain cases well-founded procedures assessing quality either model estimates generated using model yet developed – applies particular method class neural networks quality indications also provided neural networks however measurements well model functions using previously unknown particularly important model taught using one set training assessed quality using different set test approach identify models reflect general rule learned training thoroughly cases kind referred overfitting overfitted model achieve significantly better quality values training test many statistical procedures solved analytically means question formulated mathematical equation system equations solved transformations even though often requires great deal skill however direct analytical solution impossible many methods example additional conditions regularisation term applied see cases made optimisation procedures approximate solution many small steps optimisation procedures necessarily optimal example calculated result may local optimum global optimum one different classes problems analytical procedures optimisation procedures direct analytical solution possible tasks “ find value equation y=4 · x+3 x=3 ” solution kind possible task “ solve linear equation · x1+ b · x2+⋯+ h · x8= many parameters possible b … g h equal 0 ” .an additional regularisation term applied purpose min · x1+ b · x2+⋯+ h · x8– sum parameter ≠0 optimisation procedures find solutions c 2. system elements 57 2.2.3 machine boundary traditional statistics machine term first defined mitchell,2 difficult delineate scales tip towards machine latest optimisation procedures → see section 2.2.2 details solve inductive inference problems different approaches estimation “ ” strategies fall heading machine differentiated basis formulation optimisation problem solved distinction made number different procedures ●supervised supervised procedures require knowledge correct output “ ” ipo model piece input “ ” classic example inferring child output parents input necessary know child advance also necessary know correct result pregnancy test actual weather follows weather forecast properties soil predicted soil analysis etc practice real challenge often lies obtaining correct output assessing quality output frequently referred label majority machine algorithms currently trained using supervised procedures 2 tom mitchell machine mcgraw-hill ●the decisive questions regard procedures formulate actual optimisation problem regularisation terms define loss function i.e errors treated different weightings levels severity e. g. comparing false negatives patients cancer incorrectly diagnosed healthy false positives healthy patients incorrectly diagnosed cancer quality labels labels also contain errors several levels complexity defined labelling 1. labels whose accuracy verified collected example one correct relevant value exists physical properties speed object temperature room individual ’ date birth principle therefore values ascertained labels algorithm 2. labels whose accuracy verified collected may certain cases verifiable later date 3. labels construed non-verifiable relationship real world example concepts social milieus character types developed view achieving better understanding analytical grasp humans behaviour concepts abstractions necessarily accurate representation “ truth ” far exists part c echnical foundations 58 ●reinforcement involves assessing agent ’ actions imposing punishment reward agent selects pool different actions performs whichever action selected action changes state system functions optimisation input addition state change state system brought agent ’ actions must also clearly defined reward function case supervised correct optimal solution available every input necessarily true case reinforcement instead optimisation goal pursued finding action strategies lead best end state reference optimisation problem actions deliver short-term improvements may need rejected achieve goal alongside optimisation problem relevant loss factor reward function plays particularly important role strategy ●unsupervised involves searching structures particular quantity input need correct structures known reward function exist precise definition structure searched required however example search carried clusters i.e groups imposing requirement difference points cluster minimised difference clusters maximised optimisation problem unsupervised identified basis unsupervised also referred mining decisive factors include procedures also availability sufficient volumes adequately high quality broad scope since close approximation optimisation goal otherwise achieved many cases volume quality scope lacking way meaning avenues must pursued ensure good outcomes nevertheless obtained using machine techniques identifying optimisation goal transport company planning alter bus routes reflect recent changes city operates many residents moved peripheral areas large inner-city brownfield sites developed gentrification brought huge changes composition population various districts project manager collected form passenger usage figures attempting optimise routes served city ’ needs met effectively possible without needing extra buses aware range different goals constraints could imposed optimisation using fewer buses using fewer drivers avoiding creation routes example depending optimisation problem formulated might possible achieve solution whereby densely populated neighbourhoods served bus lines compared districts anyone living suburb forced put longer travel times lower frequency buses since project manager lives affluent commuter belt personal preference optimisation strategy minimises longest travel time strategy kind would result faster connections areas city including outlying districts line manager unimpressed models believes goal transport many passengers possible puts short-distance routes plenty passengers advantage bad news longer routes four stops readily apparent decisions optimisation function social impacts many questions raised including following decide goal optimisation else say decision matter debated general necessary meaningful certain groups/neighbourhoods access legal remedies feel placed unfair disadvantage compared others c 2. system elements 59 example synthetic i.e generated artificially rather collected directly real world boast several advantages real-world data.3 produced quantity particularly important dealing simulations real-world yet generated created steps taken ensure entire range possible values included synthetic e. g. order test technical system would behave confronted unusual combinations quality measured necessary guaranteed individual cases properties set real-world reference retained alternatively distortions occurring sets real-world pinpointed removed order avoid discrimination set synthetic contains references persons anonymous fall within scope gdpr synthetic also train algorithms test however risk algorithm influenced properties artificially generated counterpart reality separate functional testing must therefore carried algorithm practical applications middle course frequently adopted form augmentation involves creating real-world greater range situations covered training stage pool enlarged relationship real-world preserved term “ augmentation ” describes process generating deviate slightly original example characteristic feature augmented images shifted rotated distorted way 3 jörg drechsler/nicola jentzsch synthetische daten innovationspotenzial und gesellschaftliche herausforderungen synthetic potential innovation societal challenges stiftung neue verantwortung may available /www.stiftung-nv.de/sites/default/files/synthetische_ daten.pdf 4 john mccarthy/marvin minsky/nathaniel rochester/claude shannon proposal dartmouth summer project 1955.2.2.4 current parlance field machine – specifically neural networks – referred term often gives rise confusion machine one specific procedure falls heading “ weak ” solve well-specified tasks way contrast “ strong ” methods expected tackle single task handle broad spectrum tasks potentially without intervention despite hopes raised term “ ” machine methods capable feats historically speaking concept first appeared dartmouth proposal published back 1956 usa,4 refer broad area within field computer science decades since first emerged field marked repeated cycles unrealistic expectations followed disillusionment ivory towers made inroads economy everyday life workplaces homes latest 1970s 1980s form “ expert ” efforts germany stepped gear 1980s achievements chalked include machine techniques also large number vitally important methods procedures pattern recognition knowledge representation inferences action planning user modelling applications procedures include speech image dialogue comprehension robotics multi-agent part c echnical foundations revision f_overhauledvalidation f x =y ystorage x production f x =y search f f x =ydevelopment production monitoring quality assurancerecalibration f_new60 figure 2 process model algorithm based machine ongoing monitoring assessment process starts algorithm f developed using training algorithm identified meets desired quality standards put production ensure monitoring quality control capabilities production process must make possible record input x enters algorithm output leaves algorithm relevant correct value basis monitoring algorithm production environment comparison carried determine extent output algorithm reflects expected value algorithm continue operated without changes event non-critical deviations values significant deviations detected may necessary re-evaluate i.e recalibrate parameters algorithm critical deviations detected algorithmic redesign recommended problem understanding comprehending humans often find difficult impossible understand methods intuitively described mathematical technical terms even goes experts field modelling even case relatively simple classification methods well understood mathematically logistic regression almost one intuit result return given set input values.neural networks image recognition good example phenomenon generally look photograph understand immediately looking looking structures input neural network intended classify photograph likely understand almost nothing means even familiar input values comprehends steps neural network necessarily understand recognition process error occurs example may able determine recognition failed problem humans machines recognise objects patterns according different sets rules always easy two c 2. system elements 61 problem understanding comprehending humans often find difficult impossible understand methods intuitively described mathematical technical terms even goes experts field modelling even case relatively simple classification methods well understood mathematically logistic regression almost one intuit result return given set input values.neural networks image recognition good example phenomenon generally look photograph understand immediately looking looking structures input neural network intended classify photograph likely understand almost nothing means even familiar input values comprehends steps neural network necessarily understand recognition process error occurs example may able determine recognition failed problem humans machines recognise objects patterns according different sets rules always easy two part c echnical foundations 62 2.2.5 algorithmic algorithmic system generally incorporates multiple algorithms work together rather single algorithm term “ component ” describe executable part system different components algorithm might based different technical implementations architectural style known microservices good example important remember individual components system kind might subject different regulatory requirements protection objectives addition different stakeholders might responsible different components algorithmic system example suppliers operators manufacturers borne mind different requirements different sets rules might apply individual components e. g. respect quality non-discrimination freedom contract.2.3 software algorithm formulated programming language formal language rather natural language executable automated form computer program software functioning software depends processes also context executed cf concepts “ stack ” contains hardware software components execution parameterisation parameters “ outside-in ” method configuring software make possible pass software ranging simple options path names complex models extensive parameterisation options generally go hand hand flexible software complex process making parameters important example software parameterised adapted different contexts relatively small amount effort without modifying source text i.e actual implementation special variants adaptive time automatically adapt context – individual using environment order guarantee improve efficiency high-quality software processes spite increasingly complex framework conditions order reduce communication problems processes model-driven approaches pursued successfully many years generic software component parameterised basis complex model using language specific application context mathematical statistical models represent special case differ domain-specific languages model explicitly specified programmed instead mathematical statistical model implicitly taught trained using → see section 2.2.3 machine c 2. system elements 63 2.4 hardware software executed hardware particular processors recent years processors seen steady gains performance devices seen continual reductions meaning array potential applications become ever wider moore ’ law according performance increase hundredfold every 10 years subject physical constraints however chip components become small barely bigger individual atoms fulfilling moore ’ predictions using silicon transistor material becomes increasingly costly technically challenging task researchers therefore currently investigating alternative materials graphene conjunction computing concepts photonic quantum computing question whether suitable everyday remains open however solutions focusing parallel computing established include multi-core many-core processors graphics processing units gpus order accelerate machine using bulk application-specific chips tensor processing units tpus optimised handle highly parallel addition multiplication matrices neural networks developed increasingly parallel nature computing without problems however humans find difficult identify related processor errors calculations performed hardware level almost impossible reproduce comprehend.2.5 system architecture applications today rarely run single computer instead many different software components run different computers interact perform task term “ distributed system ” refer method distributing work across different hardware nodes distributed system made different software hardware components interact within network network nodes communicate wired wireless links wide range protocols standards exist network communication basis processing network nodes forwarding network i.e transporting nodes specifications outlining requests submitted server published application programming interface api example general rule steps must taken prevent interfaces incorrectly accessed attackers infrastructures reached via internet referred cloud cloud applications accessed billions users groups related cloud applications often referred platforms many – “ big four ” “ gafa ” google apple facebook amazon “ gafam ” microsoft also included – high level name recognition part c technical foundations hub 1 e. g. smart lighting hub 2 e. g. heating control router hub 3 … hub 4 … components controller smart home gateway internet network communication wireless wired64 early days internet things sent directly cloud processed large platforms way contrast increasing number solutions currently developed involve processing least pre-processing immediately close possible place collected words “ edge ” internet practice processing near collected referred edge computing distinguish situations processed cloud cloud computing pre-processing particularly important since allows minimisation communication effort also creation privacy-friendly since references individuals required removed point close collected complex system landscape emerged recent years incorporating internet edge computing iot entails high level interconnection making hard distinguish individual one another.the way architecture distributed designed also significant impact business processes supported system since acts factor decisions network nodes software runs interfaces protocols communications parties involved communications example manufacturers want hardware collected devices purpose long-term efforts improve devices choice setting communication infrastructure making user ’ infrastructure available asking user make available via interface way kind handled cooperative processes transparent agreed contractually necessary technical parameters may place constraints contractual provisions governing exchange figure 3 example system architecture smart home c 2. system elements 65 way architecture distributed designed also significant impact business processes supported system since acts factor decisions network nodes software runs interfaces protocols communications parties involved communications example manufacturers want hardware collected devices purpose long-term efforts improve devices choice setting communication infrastructure making user ’ infrastructure available asking user make available via interface way kind handled cooperative processes transparent agreed contractually necessary technical parameters may place constraints contractual provisions governing exchange figure 3 example system architecture smart homehub 1 e. g. smart lighting hub 2 e. g. heating control router hub 3 … hub 4 … components controller smart home gateway internet network communication wireless wired blockchain distributed ledger technologies significant improvements field distributed made possible distributed ledger technologies dlt technologies involve management multiple identical copies ledger different partners instead centralised management single ledger ledger entries added copies current accuracy database confirmed consensus underlying architecture kind varies linear approaches wide range graphbased solutions depending intended purpose structure transactions consensus also achieved using different methods methods outlined consensus protocols.one famous examples dlt architecture blockchain concept implementations include bitcoin ethereum blockchains store list records “ blocks ” blocks linked using cryptography meaning transaction stored implicitly confirms accuracy previous transactions i.e entire chain making extremely difficult fraudsters manipulate modifying deleting entries decentralised consensus protocol eliminates need additional instance confirms integrity transactions part multi-level governance complex ecosystems 68 part ulti -level governance comple x ecos ystems high level complexity dynamism ecosystems means challenges must overcome terms regulating controlling designing ethical legal framework upon ethics commission based work implemented practice require cooperation different stakeholders interaction different governance instruments many different regulatory levels multi-level governance part examines relevant governance instruments stakeholders details provided following two parts algorithmic particular regarding interplay different instruments stakeholders 69 1. general role state 1. general role state entitled exercise ethically justified obliged comply corresponding obligations – citizens companies government agencies – must actually able practice presents state wide range tasks first foremost state responsible establishing legal framework within society geared towards interest develop speed algorithmic developing infiltrating ever areas life poses major challenges legislature courts hand rulings clarifying legislative provisions state must ensure regulations adopted environment kind sufficiently hard-hitting steer developments time flexible enough continue fulfilling purpose even technological parameters change statutory provisions must therefore formulated technology-neutral manner innovative regulatory models must developed addition appropriate infrastructural technological prerequisites must place – enabling technologies institutions intermediaries complemented involvement broad gamut civil society actors ethics commission believes state must play key role guaranteeing safeguarding services general interest opportunities opened society also impose far-reaching educational remit state necessary identify skills required take creative yet reflective approach technologies determine framework conditions must put place appropriate training offered diverse range target groups state ’ educational remit understood broad sense incorporate outreach work aim raising awareness area.furthermore state also generally responsible encouraging r particularly important support r regard ethically sound technologies e.g uphold principles accountability transparency antidiscrimination extensive programmes needed ensure ethical legal principles taken account funding must channelled towards programmes funding needs provided state institutions closely aligned state state must put place framework legal otherwise society individuals businesses alike operate self-determined fashion basis ethical values principles individuals businesses provided adequate protection potential algorithmic harnessed shape worthwhile future germany ’ efforts direction ethically sound multi-level governance also include contributions debates international level global dimension technological developments means action single nation state regulations adopted national level alone inadequate ethics commission therefore welcomes international initiatives already launched commission oecd example view ensuring future shaped basis ethical principles safeguarding sovereignty germany europe international context vitally important task regard → see part g details 70 part ulti -level governance comple x ecos ystems 2. corporate self-regulation corporate responsibility responsibility mitigating risks digitalisation leveraging significant potential placed solely feet state legislators responsibility also shared parties develop disseminate technologies even absence legal obligation although state must shoulder responsibility least obliged protect citizens guaranteeing confidentiality integrity safeguarding fundamental self-regulation tools also vitally important particularly context transformation process term “ corporate responsibility ” cdr theoretical practical level refer idea companies manufacturers operators technologies assume responsibility consequences digitalisation like corporate social responsibility csr cdr falls broader umbrella corporate responsibility case voluntary corporate activities sphere go beyond currently prescribed law actively shape world benefit society general customers employees particular aim october federal ministry justice consumer protection launched initiative clarify principles concepts corporate responsibility www.bmjv.de/cdr according initiative cdr encompass many topics,1 including protection personal inclusion sphere transparency e.g relation algorithms protection innovations help achieve sustainability objectives algorithmic geared interest open security 1 corporate responsibility initiative shaping digitalization process responsibly joint platform available /www.bmjv.de/ shareddocs/downloads/de/news/artikel/100818_cdr-initiative_en.pdf __blob=publicationfile v=3 .the responsible products services must central priority corporate decisions taken levels company ethical questions must matter legal departments compliance officers alone instead must viewed cross-cutting task integrated processes parties involved must aware responsibility consider ethical values participation fairness equal treatment selfdetermination transparency negative social societal impacts digitalisation business models employees suppliers clients society whole wider environment thus minimised opportunities digitalisation offers achievement macrosocial goals leveraged applied correctly concept cdr lead improvements terms consumer protection participation sustainable economy cdr fundamentally similar corporate social responsibility csr requires companies take self-regulatory action voluntary basis internal strategies in-house industry-specific codes values therefore particularly effective way implementing cdr respect ethics commission welcomes proliferation professional ethical standards codes conduct published associations companies data-processing industry proviso standards codes must help clarify exactly needs done cdr must reduced metaphorical fig leaf allows companies pretend upholding principles ethics truth different 71 2. corporate self -regulation corporate responsibilit ethics commission ’ view protection impact assessment must relevant circumstances carried pursuant gdpr product still stage accompanied comprehensive general societal impact assessment focused assumption foresighted responsibility including impact employees customers company particularly affected transformation process also takes account long-term social effects data-driven business models might good idea companies commanding large market share set advisory panel along lines consumer customer advisory panels could consulted drawing impact assessments kind panel made representatives groups people affected relevant business model 72 part ulti -level governance comple x ecos ystems 3. education boosting skills critical reflection self-determination presupposes skills ethics commission therefore unreservedly welcomes efforts undertaken federal government consumer protection associations legal professional groups bodies raise awareness importance selfdetermined technologies smartphone settings inheritance planning provide straightforward easy-tounderstand available options well practical guidance also welcomes steps taken raise awareness among consumers potential inherent provide muchneeded real opportunities risks involved economic exploitation ethics commission recommends efforts continued stepped school pupils also made aware issues connected digitalisation early possible skills integrated curriculum teachers must provided comprehensive training subject regular intervals way ensure generations grow become competent “ natives ” able assess opportunities risks applications take informed decisions assert effectively addition lifelong education technologies must provided age groups social groups must borne mind skills require basic knowledge underlying turn requires ongoing education technical mathematical subjects also adequate familiarity economic legal ethical social sciences broad spectrum knowledge necessary comprehend discuss assess various opportunities risks complexity education training computer science science software particular relevance respect well basic instruction ethical legal issues in-depth teaching statistics methodology scientific theory needed particularly important ensure questions relating ethics ethics embedded discipline-specific methodological training must major push area ensure ethical legal considerations incorporated earlystage discussions parties develop products services involved decisions essential first step towards achievement goals cooperation many different entities possible including government agencies bodies closely aligned state private actors federal state bundesland municipal levels challenges involved providing general skills maintaining skills long term adapting individual ’ lived experience great could never tackled successfully single centralised said key role must played supervisory authorities protection authorities and/or relevant specialist supervisory authorities foundation protection consumer protection associations training providers media institutions involved media regulation also large part play connection must provide society technologies cast critical eye technical progress also establish forums debate although government agencies must remain chiefly responsible imparting skills general task realised full unless necessary civil society structures put place volunteering tech accountability journalism consumer-focused market observation ethics commission therefore recommends federal government provide long-term support establishment structures kind 73 3. education boosting skills critical reflection companies also responsibility provide training staff example company attain high ethical standards employees particularly management product adequate awareness potential ethical legal issues far education training concerned questions relating ethics law also included broad spectrum academic professional training routes workplace training particular attention given technical business professions view ensuring ethical legal considerations incorporated earlystage discussions parties develop products services involved decisions 74 part ulti -level governance comple x ecos ystems 4. technological developments ethical design efforts impart advanced skills general population must end shifting weight responsibility away manufacturers service providers towards users least users limited opportunities grasp comprehend steps involved processing underlying business models responsibility laid first foremost feet able exert influence products services concept embodied principle ethics design ethics design appears gdpr reference protection intrusions private sphere heading protection design default aligning technologies products including services applications ethical values principles outlined also good way increasing confidence products acceptance products time however design every product must tailored target user groups involving user groups needs early stage product participatory product may helpful respect particularly important products targeted vulnerable and/or less digitally literate user groups inclusive design including privacy-friendly default settings view protecting self-determination user groups inclusive design allows manufacturers operators meet constitutional requirement informational self-determination enshrined article 1 paragraph 1 german basic law dignity according protection must contingent upon individual capabilities personal circumstances.the popular methods platforms develop technologies commonly libraries code components rarely supported requirements ethics design date components “ better ” design perspective ethics protection law best niche interest need change area compliance ethical principles general protection principles particular becomes rule rather continuing exception ethics design requires gap different communities bridged certain implications professions affected goals approach could furthered methods catalogues also best-practice concepts supporting tools frameworks open-source code components platforms repositories components usable pools cases necessary prerequisite checks would make possible highlight specific properties required supply documentation needed provide opportunities exchanging know-how experience although ethics design crucial governance instrument allows process designing products processes services aligned individual interests outset provides guarantee resulting products services ethical ethical principles positive influence technological developments ethics task delegated furthermore decisions ethical principles implemented implemented example whether fairness metrics applied algorithmic metrics developers alone instead decisions negotiated context-specific basis necessary involvement parties affected 75 5. 5. although data-processing ethical design frequently developed showcased researchers gulf world real world one reasons may fact technical solutions example based cryptographic mechanisms counterintuitive nature difficult many people understand conventional methods prime example identification document changes appearance every time shown making impossible “ join dots ” holder ’ observed behaviours many people attempt understand innovative technologies drawing conceptual models surrounding analogue world latter provide insufficient basis comprehending appraising added value despite advantages offered technologies terms ethics protection law unlikely become widespread gains better understanding confident many cases cross-cutting therefore interdisciplinary cooperation essential starting point understanding implications developments designing ethical cooperation kind adequately rewarded discipline-bound metrics good science many areas interdisciplinary given due recognition shift mindset occurs applies universities peer reviews expert opinions example funding funnelled towards interdisciplinary cooperation delivers results would impossible achieve within silos individual disciplines allow necessary institutional frameworks long-term career paths established.in many cases high-quality promising technical solutions already emerged sector demand solutions currently still lacking also need methodologies technologies signpost route current implementation status improved state funding channelled innovation improved solutions move drawing board reality instead providing support outstanding success stories need broad-based progress field ethical design must acknowledged 76 part ulti -level governance comple x ecos ystems 6. standardisation latest lawrence lessig coined aphorism “ code law ” ,2 thereby emphasising relevance technical reality obvious technical standardisation essential factor implementation legal ethical requirements bodies responsible technical standardisation communications networks established international level iso/iec ieee ietf itu etsi w3c level cen national level din prime example germany alongside specific standards bodies technical standard legal force anyone uses technical system must also comply applicable legislation even provisions legislation run counter requirements imposed global technical standard nevertheless standardisation hugely influential terms available market wherever possible therefore steps taken avoid adopting standards infringe current legislation standardisation process often criticised lack democratic legitimacy true groups within society stand affected often deprived opportunity representative participation example non-governmental organisations civil society representatives seldom involved standardisation process generally speaking even protection authorities rarely involved standardisation technical worst-case scenario may mean operation technical system complies standards violates legislation another point criticism number international standards manufacturers operators supposed comply available free charge domain must instead purchased 2 lawrence lessig code laws cyberspace 1999.past standardisation efforts field security served major contributing factor addition extra security features gradual improvements level security example online banking yet snowden revelations made number services government agencies deliberately attempting weaken standards including security loopholes backdoors way safeguarding access future role technical standardisation expected gain importance coming years example result gdpr-imposed requirement take due regard state-of-the-art consequence german security act it-sicherheitsgesetz political influence exerted number different countries europe also expected increase impact assessment standards currently existence still debated must go beyond purely technical economic considerations expanded include ethical societal factors state ensure civil society actors protection authorities consumer protection experts spokespersons organisations representing parties affected play role standardisation process alongside stakeholders dominated date obligationsstandards algorithmic 77 7. two governance perspectives perspective algorithms perspective 7.two governance perspectives perspective algorithms perspective following two parts arguments set applied data-based algorithmic basis two different complementary approaches general ethical principles precepts basis ethics commission see part b important two respects firstly must guide governance measures particular view ensuring procedures collecting accessing using ethically sound secondly must guide design algorithm-based process including oft-cited “ ” perspective focuses primarily “ perspective ” perspective concentrates mainly algorithmic “ algorithms perspective ” regarded competing views two sides coin instead represent two different ethical discourses complement contingent upon different ethical discourses typically also reflected different governance instruments including different acts legislation perspective focuses train algorithmic basis algorithmically shaped decisions plethora purposes specifically associated context meaning semantics part c section 2.1 particular requires thinking origin potential impact processing may individuals involved context semantic ethical legal perspective important identify standards governance typically however individuals assert others play even significant role central distinction context personal non-personal since determines whether granted subjects protection law apply current debates pertinent connection include “ ownership ” open example.figure 4 perspective algorithms perspective way contrast algorithms perspective focuses architecture data-driven algorithmic dynamics ’ impacts individuals society ethical legal discourse area typically centres around relationship humans machines particular automation outsourcing increasingly complex operational decision-making processes autonomous enabled algorithms perspective differs perspective subjects affected system may necessarily anything original training processing even attention objective requirements apply observance may enforced failure comply may lead liability sanctions current debate “ algorithmic oversight ” relevant important respect part e 80 part e ata provide access lead knowledge knowledge bestows influence power light capabilities automated processing exponential increase memory computing capacity access mean enormous increase power opportunities controlling important resources inherently associated certain level responsibility thus like resources may lawful ethically acceptable purposes like resources impact individuals general whole must always assessed yet also exhibit certain characteristics differentiate resources.in following sections ethics commission therefore take specific characteristics starting point develop basis principles outlined part b without claiming exhaustive general standards governance → section 1 well corresponding obligations → section 2 set specific recommendations action relation standards personal → section 3 improvements controlled access personal → section 4 general access particular non-personal → section 5 81 e 1. general standards governance 1. general standards governance attempt identify specific principles governance must start differences traditional resources oil goods unique characteristics include particular following ●data created processed distributed dynamic process interaction number different players acting different roles e. g. subject operator data- generating system developer process principle never fully complete ●data non-rivalrous resource i. e. duplicated often necessary parallel multiple different players multiple different purposes ●data multifunctional across different sectors potential risks inherent depend exceptionally large extent controller ’ specific goals opportunities particular given importance effects ability combine data.1.1 foresighted responsibility special characteristics unusually dynamic nature unusually high context dependence opportunities risks associated mean particular need foresighted responsibility making decisions collecting using forwarding assessing potential impacts including risk infringing third parties particular consideration given following points ●the volume emerging collections particular cumulative effects network effects effects ●the technological means processing particular technological options available large corporations government bodies especially relation recombination decryption ●the purposes processing particular potential changes context players involved e. g. result access government agencies following corporate takeover case personal principle foresighted responsibility found standardised expression maxims minimisation storage limitation enshrined gdpr range duties gdpr need carry protection impact assessment mandatory requirements controller-to-processor contracts likewise follow principle 82 part e ata 1.2 respect parties involved must always underpinned respect others acts omissions ethically unacceptable unlawful general terms violate others become acceptable lawful simply committed way using e. g. fraud criminal offence regardless whether committed otherwise generated distributed processes interaction many different players parties way involved process generation example subject owner data-generating device may – ethical possibly also legal perspective – entitled genuinely data-specific relation → details see section 2 must respected whenever respect others implies much simply avoiding intrusion legally protected spheres another party ’ copyright needed instead ethical perspective in-depth consideration data-related legitimate interests parties specifically linked may therefore certain co-determination participation concerning in-depth consideration may also imply duties take action example granting another party access certain ways case personal principle respect third-party expressed particularly clearly principles lawfulness fairness purpose limitation enshrined gdpr gdpr sets number vested subject e. g. informed rectification restriction processing erasure portability.1.3 sharing good resources could key legally protected interests individuals e. g. health promote good particularly pursuit un ’ 17 sustainable goals relating economic social ecological aspects neglected basic principle ethical imperative resources cases would increase overall prosperity overriding conflicting interests parties particularly one special features make unique non-rivalrous resource “ wear ” even parallel many different players many different purposes duplicated almost infinite number times sharing mean player first shares least worse everyone else involved however loosely better would shared ethically responsible approach governance must take fact account sharing also enormously important terms safeguarding fair efficient competition time however conflicts sometimes arise principle furthering good sharing one hand principles foresighted responsibility respect parties ’ including considerations appropriate investment protection creation incentives voluntary sharing therefore always prioritised legislative requirements share exception 83 e 1. general standards governance 1.4 fit-for-purpose quality together context semantics stored regularly purports accurate possible representation reality currently stands accurate possible prediction future reality situations involve automated processing algorithmic immediately obvious everyone incorrect worthless also potentially harmful soon automation comes play however common people fall prey false objectivity show foolhardy willingness rely results calculations carried using incorrect incomplete therefore also likely share characteristics “ garbage garbage ” interests everyone therefore responsible governance society must also include efforts achieve standard quality appropriate intended purpose → part c section 2.1.1 meaning “ appropriate ” must always determined context- specific basis relation quality however example important remember may reflect societal preconceptions stereotypes discrimination turn influence functioning algorithmic system trained using → details see part f section 2.6 accurately reflect existing deficit may therefore unsuitable basis purposes even high statistical quality another important factor connection across different sectors different purposes fair principle findable accessible interoperable reusable may relevant context example regards storage encoding methods according principle must prepared stored way findable accessible must coded interoperable format way makes reusable different contexts many different players possible case personal desire achieve high level quality manifested principle accuracy enshrined gdpr 1.5 risk-adequate level security freely duplicated almost impossible recover gone astray wide range possibilities external attack many invisible outside mean also vulnerable malicious attempts falsify destroy high level security commensurate relevant risk potential therefore technical perspective directly related principles foresighted responsibility respect parties involved appropriate security encompassing broad spectrum measures different levels vital prerequisite mutual trust part involved society case personal concept security manifested principle integrity confidentiality enshrined gdpr 1.6 interest-oriented transparency since party uses effectively controls may gain influence power result party must principle able willing account actions one reasons protection parties whose might affected even violated interest-oriented level transparency required parties entities enforcing law benefit others determine whether extent fact affected violated lodge claims case personal transparency – i. e. ensuring processing operations easy subjects understand – basic principle gdpr also true principle accountability many provisions gdpr example relating documentation request access designed improve transparency foresighted responsibility respect parties involved fit-for-purpose quality riskadequate level securityinterest-oriented transparencydata sharing good84 part e figure 5 standards governance 85 e 2. corresponding obligations 2. corresponding obligations according ethical principle selfdetermination individuals merely perceived passive need protection facing actual potential threats rather self-determined actors society self-determined navigation society individuals requires individuals certain asserted others first foremost among relate individual ’ personal derive informational self-determination enshrined fundamental freedom guaranteed protection law currently force self-determination also encompasses self-determined economic exploitation one ’ self-determined handling non-personal example generated operation one ’ devices ethics commission takes view principle self-determination also applies companies legal entities – least extent – groups persons collectives context ethics commission believes possible identify general principles underpinning obligations go beyond protection alone.1 2.1 general principles obligations complex generation processes understood broader sense i. e. including various phases creation enhancement refinement often involve interactions different parties may pursuing different goals playing different roles contribute respective roles generation process contribution party i. e. natural legal person generation may relevant following true stored relates terms meaning party object associated party e. g. belonging 1 model obligations based preliminary drafts 2 february 3 october “ principles economy ” law institute eli american law institute ali made available ethics commission preliminary drafts yet adopted either ali eli yet represent official either organisations.b generated activity party operation object e. g. sensor belongs party c generated software another component e. g. sensors created invested party situation referred i. e. situation party subject stored relates natural persons particular significance since situation gives rise informational self-determination protection enshrined constitutional law given specific characteristics inextricable link personal personality ethics commission believes contribution generation give rise exclusive ownership said beyond existing intellectual property → see sections 3.3.2 5.2.4 instead contribution generation entitle party specific form co-determination participation turn impose obligations actors ethical perspective result dynamic special relationship party involved generation party controlling duration relationship may vary may intensity far personal concerned relationship largely determined applicable protection law ethical perspective recognition design corresponding obligations dynamic environments depend following general factors normally also factors underlying relevant legal provisions obligations already substantiated law scope nature contribution generation party asserting balance power partiesweight interest granted rightcontribution generation weight conflicting interests part others interests general publicdata obligations86 part e b weight party ’ legitimate interest granted said particular require desistance access rectification economic share c weight possibly conflicting interests part party third parties taking account potential compensation arrangements e. g. protective measures remuneration interests general e balance power party asserting party.these factors interact one another described flexible system interest access particularly high example may compensate relatively insignificant contribution generation consideration must always given general principles outlined part b order avoid situations crucially important individual interests undermined purported actual interest factors also determine certain details e. g. formats deadlines protective measures financial compensation fleshed put practice includes question whether action taken upon request party asserting e. g. access claim also proactively e. g. obligation publish figure 6 general factors shaping corresponding obligations 87 e 2. corresponding obligations granted subjects gdpr particularly important manifestation principles aimed specifically protecting natural persons pertains also extent standardised manifestation given hinge qualification personal principles formulated also applied non-personal however relate individuals also legal entities collectives 2.2 clarification general principles reference typical scenarios may number different goals include obliging another party desist using requiring erasure gaining access e. g. disclosure transfer full portability arranging rectified claiming economic share profits derived help 2 article 6 1 article 9 1 gdpr.2.2.1 scenarios involving desistance situations often occur party requests another party desist using certain way gdpr even works basic assumption personal unless legal basis number requirements met.2 general sense beyond scope gdpr party significant legitimate interest controller desisting outcome ethical perspective may require said desistance potentially even including erasure processing operation might cause harm party third party b inconsistent circumstances party contributed generation particular contribution made another purpose party could reasonably expected contribute generation foreseen present processing operation ii consent party would invalid overriding reasons require desistance affirmed however party ’ legitimate interest granted must weighed factors referred → section 2.1 example affirmed cases processing way exception justified compelling interests e. g. prosecution criminal offences 88 part e ata regard non-personal requests desist may become relevant example context value creation chains customer relationships non-personal often enormous economic significance party involved may significant legitimate interest assert → section 5.3 example 1 non-personal collected sensors modern agricultural machinery relating soil quality weather etc manufacturers basis many services provide precision farming predictive maintenance etc. manufacturers forward potential investors lessors land however latter would given might prove harmful agricultural holding negotiations land take place future assumed agricultural holding would helped generate voluntarily known would purpose assessing require desistance ethical perspective consideration must given balance power parties case hand also fact agricultural holding made extremely significant contribution generation third-party deemed worthy protection would include manufacturer ’ interest maximising profit general interest part investors lessors etc obtaining accurate information.from ethical perspective waiver require desistance possible limited circumstances waiver automatically ruled cases consent would invalid overriding reasons within meaning requirement b ii example illegal inconsistent policy legal system fundamental values underpinning exists thing liberty kind harm oneself others cases waiver may possible provided stringent requirements met e. g. separate agreement linked services involve party placed pressure ensure voluntary nature waiver meaning requirement b would longer apply example 1 agricultural holding could consent forwarded third parties e. g. basis individual agreement appropriate remuneration tractor dependent forwarded 89 e 2. corresponding obligations personal obligations desist normally follow already provisions protec tion law criteria outlined determine whether substantive limits consent exceeded → section 3.2.1 guide balancing different legitimate interests example example 2 relating activities social network user extensive personality profiling profile contains attributes “ mentally unstable ” “ esoteric tendencies ” result user shown advertisements companies offer personal horoscopes energy healing services significant cost almost daily basis often immediately posted signals stress anxiety often makes purchases result set user account clicked checkbox next following statement “ happy evaluated personal preferences attributes identified accurately services offered including third-party providers personalised needs profiling ” “ consent ” kind make subsequent processing operations lawful number different arguments reaching conclusion one processing purpose may cause significant harm user would inconsistent circumstances generated could reasonably expected known would purpose law allow abuse mental states kind cf section 138 german civil code bürgerliches gesetzbuch bgb .there many circumstances obligation desist mitigated consent balancing conflicting interests cases reference often made “ red lines ” “ limits ” requirement limits data-specific example reasonable prohibit election manipulation practices incompatible principle democracy regardless whether said practices involve view ethics commission example data-specific limits total surveillance individuals example 3 entering employment contract employee signs agreement stating location tracking functions smartwatch mobile telephone well number apps collect e. g. tracking sleeping behaviours emotions kept switched times even work hand devices employer requested order relevant accessed readily apparent arrangements taken together equivalent total almost total surveillance incompatible dignity self-determination privacy true even employee gave consent measures even decided accord enter contract employer even offers employment available 90 part e ata conversely criteria apply scenarios involving desistance may also bear indirect relevance situations ethical even legal obligation obligation may arise party general obligation protect certain legally protected interests time access could secure improve protection interests kind situation obligation arises corollary obligation protect certain legally protected interests unless third party conflicting require desistance example 4 hospital experiencing outbreak multi- resistant pathogen wants analyse health patients recently become infected order gain better idea certain individuals likely fall prey pathogen basis pinpointing inpatients might benefit move another hospital circumstances hospital general obligation provide patients best possible protection infection taking available reasonable precautions end includes health belonging patients already infected pathogen provided said might protect patients obligation emanating former group patients desist 3 way examples commission building economy 9 final 10 january pp 11 et seqq available /ec.europa.eu/transparency/regdoc/rep/1/2017/en/com-2017-9-f1-en-main-part-1.pdf commission towards common space 232 final 25 april pp 8 et seqq available /ec.europa.eu/transparency/regdoc/rep/1/2018/en/com2018-232-f1-en-main-part-1.pdf 2.2.2 scenarios involving access comes scenarios involving request access many situations party seeking access party effectively controls able reach agreement action taken voluntary arrangements kind welcomed provided conflicting overriding third-party interests particular provided parties require desistance based criteria given enormous potential value creation inherent however in-depth discussions also held circumstances conditions access even must granted ethical viewpoint.3 may apply situations access required perhaps even mandated law order enable party comply special obligation task e. g. prosecution criminal offence health concern access must consistent rules apply obligation task particular attention paid principle proportionality potential third-party require desistance → see section 2.2.1 must considered may also independent requests access example within existing value creation typically involve many different parties contribute generation different roles e. g. suppliers manufacturers retailers end users principle familiar agreed roles roles players involved → see section 5.3 details legitimate interests asserted party basis access request may particular include cases required following purposes 91 e 2. corresponding obligations asset line intended purpose within value creation system e. g. repair connected device end user b monitoring improving quality service provided within framework value creation system e. g. supplier c ascertain truth provide evidence e. g. legal dispute third parties avoid anti-competitive effects e. g. lock-in effects e create value using e. g. developing smart service example 5 supplier provides engines agricultural machinery referred example 1. would extremely useful supplier access certain tractor verify constantly improve quality engines stored manufacturer ’ cloud however latter unwilling allow supplier access situations kind important remember supplier made significant contribution generation engine urgently needed improve quality service provided within framework value creation system manufacturer also involved consideration must given balance power specific case hand also fact parties involved – including general – interest high-quality engines may however also relevant economic interests manufacturer ’ side particular relating confidentiality.access also discussed situations party seeking access party effectively controls yet part value creation system value creation system could originate involved outcome assessment based general criteria normally different situations kind party seeking access typically contributed generation justifications cited granting access rather interest considerations specific considerations safeguarding competition → see section 5.5 details example 6 example 1 manufacturer holds dominant tractor market collecting soil weather decades start-up recognises potential database investors using requests access case consideration must given fact start-up made contribution generation existence interest access significance interest depends whether manufacturer abusing market power much economy would benefit breaking small group market- dominant companies presuming start-up based europe case potential harmful effects disclosure trade secrets legitimate third-party interests interests manufacturer agricultural holdings example 1 must taken account 92 part e ata generally recognised principles open government ogd embody idea government made available private sector include “ open default ” re-use “ anyone purpose ” .4 calls many quarters expand open concepts include created effectively controlled private entities move towards open however also gives rise complex ethical questions example extent generalised assessment longer looks individual case acceptable example 7 municipality implements large-scale project collect mobility using smartphone signals view facilitating traffic management adjusting timing transport services example theoretically speaking “ anonymised ” sets combined sets additional knowledge however owner identified confidence level 95 number different parties interested gaining access include researcher wants basis identifying optimal design urban recreational areas start-up wants establish online detective agency via users pay access mobility profile spouse competitor etc institute tasked foreign government investigating political activities citizens case-by-case assessments three access requests would deliver different outcomes therefore difficult question whether municipality may even must make view many possible uses would promote good 4 see recital 16 directive 2019/1024 open re-use sector psi directive principles 1 3 g8 open charter signed g8 summit 18 june principle 1 international open charter signed september open government partnership summit.the ethics commission wishes emphasise context importance potential individual parties contributed generation particular subjects require desistance follows possible reasonable protective measures including anonymisation techniques improved ongoing basis taken weighing potential harm expected benefit good also – depending potential harm – granting blanket access may question → see section 5.4 details 2.2.3 scenarios involving rectification high quality problems particularly likely arise include unsuitable context inaccurate encoding incomplete sense deductions obtained using also incorrect circumstances kind party involved generation may ethically justified quire rectification underlying deductions obtained using threshold kind granted relatively low since principle neither protected individual interest interest processing inaccurate incomplete general rule following requirements must met processing inaccurate incomplete must potentially harmful party particular party relates b rectification must disproportionate taking account severity likelihood harm one hand effort involved rectifying 93 e 2. corresponding obligations example 8 high error rate detected engine stored manufacturer example 5. problematic company supplies engines deprives company possibility fulfil quality assurance remit also engine-related pooled engine-related engine suppliers basis evaluations poor performance metrics engines relevant supplier might reduce latter ’ chances securing orders manufacturers case processing inaccurate causes harm supplier indications effort involved rectification would disproportionate amount effort involved rectifying excessive potential harm significant require desistance frequently arise → see section 2.2.1 2.2.4 scenarios involving economic share cases party uses create value parties contributed generation said everyday occurrence good thing principle provided one entitled require desistance → see section 2.2.1 must normally tolerated parties contributed generation given strong affinity obligations set section considerations good potent arguments recognising general remuneration parties contributed generation instead parties must existing mechanisms collective economic participation particular taxation value creation.in cases valid contract back claim remuneration financial compensation considered mitigating measure example exercising without compensation appears disproportionate specific case hand → see section 2.1 factor c ethical perspective view ethics commission party contributed generation entitled independent remuneration others exceptional cases cases kind might arise party ’ contribution generation required unusual amount effort particularly unique would hardly possible economic viewpoint replace contributions players b exceptionally large amount value created using c circumstances contribution generation made mean would impossible unreasonable party engage negotiations remuneration amount remuneration paid exceptional cases must adequate particular basic incentives using create value must removed must also remembered party creating value typically incurred financial risks 2.3 collective aspects obligations answer must found issue whether extent arguments concerning require desistance access rectification economic share profits derived help also applied collectives sense defined groups persons e. g. indigenous peoples regard genetic i. e. whether collectives may entitled certain connection “ ” example 94 part e ata thought must given question whether – ethically speaking – population nation state generated economic share profits form taxes transfer payments ethics commission believes question principle answered affirmative example 9 internet giant earns billions generated individuals around world services yet even though megalith company generates 10-digit sums year year using eu-based individuals pays virtually taxes question arises whether company obliged ethical grounds allow general share taxation value creates issue raises fundamental questions distributive participatory justice economic system looks like however aspects market power unique nature contributions e. g. audio certain language develop voice-controlled services may also taken account relational nature many types makes particularly important include groups collectives debate relational nature apparent way many services require users disclose contacts “ friends ” example far corresponding obligations concerned “ friends ” may require desistance gain access etc time potential interests must always taken account weighing whether granted → see section 2.1 however also cases party contributes generation indirectly provide parties – even latter played role even broadest sense word generation particularly relevant sphere genetic also applies types still another closely related group cases individualised even aggregated form may implications potentially negative third-party effects extend beyond individual supplied example 10 health insurance company offers reduced premiums incentive sign health tracking schemes agree disclose benefit lower premiums refuse may end paying issues relating representativeness train algorithmic also interpreted problems relationality lack relationship parties supply training parties trained applied may result systematic bias potential discrimination → see part f section 2.6 details overcome hurdle individualistic approaches ethics law design must expanded include relational concepts cf also debate group privacy certain circumstances may therefore possible – least viewed lens ethics – one group member ’ contribution generation attributed group members well potentially entitling latter spite fact made individual contribution certain request desistance gain access example 95 e 3. standards personal 3. standards personal 3.1 personal relating legal entities relating identified identifiable natural person regarded personal identifiable natural person one identified directly indirectly particular reference identifier name identification number location online identifier one factors specific physical physiological genetic mental economic cultural social identify natural person article 2 1 gdpr even though remainder section focuses personal legal sense term ethics commission wishes stress protection companies legal entities valid concern relegated completely sidelines potential hazards confronting legal entities exacerbated yet networking machines exchange factory components storage production generated industry 4.0 plants “ twins ” individual sets generated operation devices example pooled together result may almost seamless overview company ’ internal operating procedures may – absence appropriate protective mechanisms – easily fall hands wrong parties outside company competitors negotiating partners authorities prospective buyers etc. ethics commission believes risk posed self-determination companies legal entities also sovereignty germany europe since flows predominantly involve third countries concerning ethical viewpoint steps must taken mitigate it.a key legislative starting point protecting enterprise protection trade secrets particular german act protection trade secrets gesetz zum schutz von geschäftsgeheimnissen geschgehg interpreting applying act efforts must made guarantee comprehensive protection sensitive business given central importance latter building fair competitive economic system basis economic social well-being many respects however directive 2016/943 provisions transposed act protection trade secrets adequately tailored reality iot industry 4.0. ethics commission therefore calls federal government step data-related protection german companies recommendations action relating personal put forward ethics commission remainder section example relation riskadequate interpretation applicable legal framework → section 3.2.2 privacy-friendly design products services → section 3.6 also apply protection relating companies legal entities modified attenuated form appropriate 3.2 self-determination challenge tackled legal system whole 3.2.1 cooperative relationship applicable legal regimes economy society heavily reliant personal huge variety different contexts yet always degree tension personal fundamental individuals constitutional informational self-determination part general personality essentially part protection dignity protection law particular gdpr clarifies benchmarks binding force private bodies 96 part e ata gdpr one great achievements legislator currently functions source inspiration countries important temper expectations piece legislation however gdpr focused protection rather comprehensive promotion individual welfare good economy taken isolation suitable tool averting harm individual may suffer result personal processed therefore regarded protecting integrity respects different mechanisms provided legal system whole must safeguard legally protected interests particularly specifically addressed provisions protection law e. g. economic interests life health physical integrity reputation applies even situations personal play concept consent enshrined protection law vitally important mechanism safeguarding informational self-determination analogue spheres yet concept self-determination subject substantive limitations includes freedom inflict kind harm oneself third parties would alien element legal system ethically indefensible law limit even prohibit individual ’ free informed consent – expression general freedom action protected fundamental – narrowly defined exceptional circumstances however consent protection law subject substantive limitations way analogy limitations freedom contract consent comes intrusions bodily integrity view ethics commission become average individual systematically overwhelmed number complexity decisions required take connection consent protection law 5 cf also recital 42 gdpr 6 relates particular fairness test applied general terms conditions business sections 307 et seqq german civil code bürgerliches gesetzbuch bgb principles morals section 138 civil code wilful immoral damage section 826 civil code contractual quasi-contractual protection fiduciary duties section 241 paragraph 2 civil code .difficulty involved estimating potential impacts processing ethics commission believes inadequate consent providers services one several reasons general loss trust society things stand individuals often longer rely fact state legal system put place framework conditions necessary navigate world safety relatively speaking free care without needing worry possibility suffering serious harm parties business-to-consumer transactions contract law specifically unfair contract terms control provided basis ‘ rational indifference ’ part consumers far-reaching protection even low-value cases result achieved way applying fairness test declarations consent .5 applying fairness test general values principles underlying legal system whole must taken account 3.2.2 risk-adequate interpretation applicable legal framework ethics commission wishes stress existing legal framework must interpreted applied way mitigate maximum hazards facing connection widespread collection analysis personal notwithstanding need comply requirements protection law processing operations also subject number limits wherever possible uses go beyond limits prevented interpreting applying law force6 manner consistent fundamental view ethics commission relevant example 97 e 3. standards personal ●incursions personal privacy integrity incompatible fundamental result profiling and/or scoring e. g. certain methods determining personality traits emotions expected behaviours ●total surveillance incompatible dignity inter alia “ comprehensive surveillance footprint ” “ super scoring ” ●immoral exploitation situations urgent need medical conditions ●election manipulation practices run counter principle democracy legislation currently force already categorises ethically reprehensible attempts mislead manipulate consumers commercial context – include business practices aimed persuading party disclose personal – misleading aggressive commercial practices german unfair competition act gesetz gegen den unlauteren wettbewerb uwg regardless whether provisions protection law infringed attempts therefore trigger appropriate legal consequences e. g. rescission grounds fraud threat injunctive relief compensation ethics commission wishes cite following potential examples practices ●addictive designs i. e. technologies exert undue influence user particular means mechanisms promote addictive behaviour therefore liable substantially adverse impact freedom decide whether stop using ●dark patterns i. e. technologies mainly user interfaces designed way deceive user certain facts and/or manipulate taking certain decision may financial implications 7 cf instruments referred footnote 6.absolute limits must also imposed processing order protect individuals placed undue financial disadvantage existing legislation contains various provisions enforce protection.7 view ethics commission examples unfair contract terms violations contractual pre-contractual duties fiduciary nature include following ●preventing access generated device required normal said device including performance repairs independent workshop making unreasonably difficult access e. g. access granted accordance article 12 gdpr i. e. within one month even three months ●preventing access needed operate pre-owned networked device making unreasonably difficult access e. g. individual bought house equipped smart home ●making harder individuals switch provider means lock-in i. e. refusing hand analyses user already paid economic perspective protected trade secrets ●processing user generated manufacturer another member supply chain purpose runs completely counter user ’ economic interests e. g. price differentiation aim extracting maximum individual willing pay 98 part e ata 3.2.3. need clarify tighten applicable legal framework things currently stand level protection legally protected interests line constitutional requirements achieved many questions arising society case-by-case interpretation general legal concepts blanket clauses supervisory authorities courts ethics commission believes situation untenable general legal concepts blanket clauses offer advantage flexible keeping future options open yet authorities courts often take years even decades develop established case law phenomena phenomena particular meantime results structural enforcement gap regard law force lack legal social media monitoring social media monitoring systematic oversight social media particular topic evolved utilisation tool takes advantage fact social networks expand users ’ communication options also allow behaviour constantly monitored companies frequently deploy generated social network users e. g. purpose market marketing although public-sector bodies far slower make opportunities afforded social media monitoring means unheard-of practice example tax authorities web crawlers trawl publicly available internet way pinpointing business sellers paying vat algorithmic make collated social media monitoring usable exploitable far-reaching intrusive purposes particular creation personal profiles commercial purposes provided weighing interests pursuant article 6 1 f gdpr supports exploitation another legal basis processing may entirely consistent law pursuant recital 51 gdpr fact subject disclosed exploitation ethics commission takes view monitoring activities rate deemed crossed boundary lawful unlawful publicly available monitored scope monitoring could gauged subject disclosed example – generally speaking – statements made minors without due consideration alternatively highly sensitive example suicidal ideation statements even applicants job willingly made recruitment process represent great intrusion personal integrity clearly related applicant ’ job history e. g. statements sexual orientation applies systematic evaluation originating individual ’ private life e. g. tracking particularly modes exploitation far-reaching intrusive weighing interests may result limits placed admissibility e. g. businesses target advertisements basis sexual orientation exploit individuals known emotionally vulnerable state certain providers particular providers social networking sites technically capable carrying in-depth evaluations communications exchanged via central platforms operate even general access prevented using end-to-end encryption metadata provide means obtain highly instructive analytical findings legislative ban evaluation communications individuals within closed groups also imposed private providers keeping principle telecommunications secrecy ethics commission therefore recommends federal government delay efforts secure introduction ban forthcoming negotiations adoption eprivacy regulation 99 e 3. standards personal 3.2.3. need clarify tighten applicable legal framework things currently stand level protection legally protected interests line constitutional requirements achieved many questions arising society case-by-case interpretation general legal concepts blanket clauses supervisory authorities courts ethics commission believes situation untenable general legal concepts blanket clauses offer advantage flexible keeping future options open yet authorities courts often take years even decades develop established case law phenomena phenomena particular meantime results structural enforcement gap regard law force lack legal certainty given extent issue affects fundamental uncertainty whether solutions emerge meet constitutional requirements ethics commission believes prompt action establish binding regulatory framework falls squarely within remit democratically legitimised legislator view hazards posed individuals personality-sensitive profiling sometimes resulting scoring ethics commission believes urgent need take effective action tighten current legal framework particularly critical area order effectively counter risks individuals manipulated suffering discrimination social media monitoring social media monitoring systematic oversight social media particular topic evolved utilisation tool takes advantage fact social networks expand users ’ communication options also allow behaviour constantly monitored companies frequently deploy generated social network users e. g. purpose market marketing although public-sector bodies far slower make opportunities afforded social media monitoring means unheard-of practice example tax authorities web crawlers trawl publicly available internet way pinpointing business sellers paying vat algorithmic make collated social media monitoring usable exploitable far-reaching intrusive purposes particular creation personal profiles commercial purposes provided weighing interests pursuant article 6 1 f gdpr supports exploitation another legal basis processing may entirely consistent law pursuant recital 51 gdpr fact subject disclosed exploitation ethics commission takes view monitoring activities rate deemed crossed boundary lawful unlawful publicly available monitored scope monitoring could gauged subject disclosed example – generally speaking – statements made minors without due consideration alternatively highly sensitive example suicidal ideation statements even applicants job willingly made recruitment process represent great intrusion personal integrity clearly related applicant ’ job history e. g. statements sexual orientation applies systematic evaluation originating individual ’ private life e. g. tracking particularly modes exploitation far-reaching intrusive weighing interests may result limits placed admissibility e. g. businesses target advertisements basis sexual orientation exploit individuals known emotionally vulnerable state certain providers particular providers social networking sites technically capable carrying in-depth evaluations communications exchanged via central platforms operate even general access prevented using end-to-end encryption metadata provide means obtain highly instructive analytical findings legislative ban evaluation communications individuals within closed groups also imposed private providers keeping principle telecommunications secrecy ethics commission therefore recommends federal government delay efforts secure introduction ban forthcoming negotiations adoption eprivacy regulation profiling “ profiling ” defined article 4 4 gdpr form automated processing personal consisting personal evaluate certain personal aspects relating natural person particular analyse predict aspects concerning natural person ’ performance work economic situation health personal preferences interests reliability behaviour location movements profiling ultimately involves making deductions drawing conclusions basis input particular using certain statistical inference methods → part c section 2.2.2 deductions may relate actual purported “ properties ” individual e. g. “ mental stability ” “ reliability ” “ social acceptability ” and/or take form predictions relate individual ’ future behaviour e. g. particular consumption pattern .in addition profiling attempts frequently made assign users predefined stereotype category basis observed behaviour interacting using “ matching algorithms ” example someone books holiday might classified sports fan culture enthusiast family man woman keen hiker sales representative gourmet stereotype instantiated individual user store typical preferences goals personality traits subsequent algorithmic processing operations sometimes profiles stored instead ad-hoc deductions particular behavioural predictions generated dynamically real time using raw e. g. “ ready purchase shoes ” 100 part e ata given profiling makes possible personalise wide range products services degree many users perceive convenient helpful categorical ban would overshoot mark however ethics commission recommends federal government speak – forthcoming evaluation gdpr example – favour expanding gdpr include specific rules profiling go beyond existing provisions article 22 gdpr permissibility automated decision-making alternatively federal government could lobby separate legislative act would effectively counter risks profiling poses fundamental individuals adequately hard-hitting solution proves unworkable foreseeable future legislative rules put place national level within scope permitted law regulate profiling procedures pose potential risk fundamental ethics commission believes particularly urgent need provisions horizontal and/or sectoral profiling concerning following matters far solutions already follow correct interpretation gdpr imposition limits i. e. prohibiting law certain critical applications e. g. selecting pool job applicants profiles generated basis originating private lives profiling procedures involve highly sensitive personal example connection emotion detection software biometric processing operations entail unacceptable potential harm subjects society 8 high-level expert group policy investment recommendations trustworthy 26 june pp 14 40 available /ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence .b imposition admissibility requirements critical profiling procedures including quality requirements relation meaningfulness accuracy profiles generated → see part f section 4.2.1 details risk-adequate system opt-ins opt-outs latter appropriate level risk low c clarification principle proportionality inter alia regards requirements apply nature scope profiling permitted level detail conclusions drawn profiling purposes particular purposes profiling may admissibly imposition specific labelling disclosure obligations inter alia regards existence purpose algorithmic may carry ad-hoc deductions critical deductions already carried instead providing automated decisions taken later stage process e provision feasible options subjects exert influence profiles created including option erase/rectify/ verify also includes “ start ” involving erasure existing profiles e. g. upon reaching age majority recently suggested high-level expert group.8 voice assistants voice assistants promise great deal terms convenience easier access technologies particularly people disabilities yet also harbour risks far self-determination subjects concerned voice assistants record ambient noise often without user activated related function recordings include speech user third parties regarded biometric purpose gdpr speech recordings analysed real time response given spoken commands automated processes often log certain types log file unique timbre individual ’ voice speech patterns analysed basis uniquely identifying individual deciphering speech emotions profiling kind represents particularly deep invasive intrusion core area personality entails risk exacerbating structural imbalances demand supply side market enormous potential misuse also present given possibility recombining digitally reconstructing spoken word deep fakes reality individual users often vague idea processing carried indeed whether carried particularly user relatively inexperienced technical matters may easily persuaded disclose additional sensitive personal upon hearing authentically human-sounding voice many cases voice assistants limited simply recording going immediate vicinity instead – networked virtual assistants smart home products – act control centre “ technological heart ” modern homes.the ethics commission believes creation comprehensive profiles based voice assistants integration wide range software hardware components poses critical risk ease convenience apparent benefits connecting voice assistants devices may ultimately lead users “ plug-and-play trap ” view ethics commission range measures taken mitigate risks associated voice assistants include bans particularly critical profiling procedures applications also following binding technical requirements implement principles protection design default → see also section 3.6 especially processing speech files exclusively local basis well option erase files locally restrictions stating may forwarded operators third parties form commands already translated machine language e. g. order placed b binding technical requirements include option switch microphone internet connection way telling i. e. visual indication whether microphone → see also section 3.6 c transparency obligations designed manner appropriate medium → see part f section 4.1 i. e. ensure important also provided acoustically either pertinent situation arises regular intervals 101 e 3. standards personal b imposition admissibility requirements critical profiling procedures including quality requirements relation meaningfulness accuracy profiles generated → see part f section 4.2.1 details risk-adequate system opt-ins opt-outs latter appropriate level risk low c clarification principle proportionality inter alia regards requirements apply nature scope profiling permitted level detail conclusions drawn profiling purposes particular purposes profiling may admissibly imposition specific labelling disclosure obligations inter alia regards existence purpose algorithmic may carry ad-hoc deductions critical deductions already carried instead providing automated decisions taken later stage process e provision feasible options subjects exert influence profiles created including option erase/rectify/ verify also includes “ start ” involving erasure existing profiles e. g. upon reaching age majority recently suggested high-level expert group.8 voice assistants voice assistants promise great deal terms convenience easier access technologies particularly people disabilities yet also harbour risks far self-determination subjects concerned voice assistants record ambient noise often without user activated related function recordings include speech user third parties regarded biometric purpose gdpr speech recordings analysed real time response given spoken commands automated processes often log certain types log file unique timbre individual ’ voice speech patterns analysed basis uniquely identifying individual deciphering speech emotions profiling kind represents particularly deep invasive intrusion core area personality entails risk exacerbating structural imbalances demand supply side market enormous potential misuse also present given possibility recombining digitally reconstructing spoken word deep fakes reality individual users often vague idea processing carried indeed whether carried particularly user relatively inexperienced technical matters may easily persuaded disclose additional sensitive personal upon hearing authentically human-sounding voice many cases voice assistants limited simply recording going immediate vicinity instead – networked virtual assistants smart home products – act control centre “ technological heart ” modern homes.the ethics commission believes creation comprehensive profiles based voice assistants integration wide range software hardware components poses critical risk ease convenience apparent benefits connecting voice assistants devices may ultimately lead users “ plug-and-play trap ” view ethics commission range measures taken mitigate risks associated voice assistants include bans particularly critical profiling procedures applications also following binding technical requirements implement principles protection design default → see also section 3.6 especially processing speech files exclusively local basis well option erase files locally restrictions stating may forwarded operators third parties form commands already translated machine language e. g. order placed b binding technical requirements include option switch microphone internet connection way telling i. e. visual indication whether microphone → see also section 3.6 c transparency obligations designed manner appropriate medium → see part f section 4.1 i. e. ensure important also provided acoustically either pertinent situation arises regular intervals 102 part e ata addition special legislative measures kind aimed protecting users federal government examine extent would possible lobby expanded legislative framework ensure appropriate governance preferably level otherwise national level framework entirely separate goals protection law i. e. outside scope gdpr ethics commission wishes issue following special recommendations connection → see section 3.2.2 examples case blacklisting data-specific unfair contract terms sections 308 309 german civil code bürgerliches gesetzbuch bgb data-specific contractual pre-contractual duties fiduciary nature section 241 paragraph 2 civil code b specification data-specific torts umbrella existing tort intentional infliction harm contrary policy e. g. section 826a civil code c blacklisting data-specific misleading aggressive commercial practices addictive designs dark patterns expanding blacklist already exists german unfair competition act gesetz gegen den unlauteren wettbewerb uwg full harmonisation approach ’ unfair commercial practices directive means change would need initiated level however.when profiling carried government agencies potential cumulative infringements fundamental aggregated surveillance must taken account must potential side effects “ collateral damage ” ethics commission believes particular potential abuse individual subsystems connected resulting pooling analytical findings different areas sectors significantly steps intensity surveillance intelligent pattern recognition techniques particular facial recognition make easier link personal across variety surveillance merge profiles view fact ethics commission recommends firstly pattern recognition techniques kind come play absolutely vital prerequisite fulfilment state obligations secondly legal limits – beyond separation rule concerning activities – must imposed exchange patterns authorities may also encompass legal provisions banning particular types exploitation particularly regards sharing government agencies engaged preventive repressive measures 103 e 3. standards personal 3.2.4 uniform market-related supervisory activities task supervising compliance protection law players german economy shared federal land authorities discrepancies observed terms interpretation protection law approach enforcement raises certain challenges parties affected although protection board edpb introduced member states aim ensuring uniform application gdpr institution also power adopt binding decisions individual cases coexistence different protection authorities various german länder within framework federal system date prevented emergence binding uniform approach national level event proves impossible strengthen formalise cooperation german protection authorities thereby safeguarding uniform consistent application protection law consideration given establishment protection authority federal level market-related activities concentrating supervisory powers within single would make possible build specialist expertise required enforce protection law environment characterised highly dynamic technological developments single authority – either acting alone close cooperation authorities – would also need able safeguard enforcement data-related areas law close functional ties protection legislation e. g. general private law unfair commercial practices law establishment single able wield market supervisory powers field protection might also make germany ’ voice louder within protection board since member states already represented edpb protection authority national jurisdiction finally centralisation official competencies go hand hand designation single court responsible judicial control market-related supervisory authorities field protection court also build relevant expertise set forth consistent case law.various models conceivable perspective organisational law based powers regulate economic law federal government could transfer supervisory competences protection economy i. e. private sector federal commissioner protection freedom provide latter relevant resources setting number different satellite offices commissioner could ensure nation-wide presence protection bodies similar federal office migration refugees bundesbank alternatively länder could establish joint facility basis interstate treaty way analogy similar projects broadcasting sector example central offices länder safety engineering health protection joint facility responsible supervisory activities field protection would need independent principle enshrined interstate treaty irrespective decisions taken connection authorities provided better material resources allow “ punch weight ” reasons constitutional law protection authorities land level retain jurisdiction sector 104 part e ata 3.3 personal asset 3.3.1 commercialisation personal economic significance personal hard overestimate generally acknowledged protection personality fundamental also encompasses individual ’ decide whether certain aspects personality made available fee e. g. one ’ image words whether exploited economic purposes.9 way complete ban exploitation individuals however rules categorically stating personal may exploited economic purposes initiative third parties people compare situation trade organs comparison flawed several respects unlike organs non-rivalrous resource mere fact personal processed someone else necessarily cause harm subject – harm caused processing specific contexts specific purposes interpreting informational self-determination natural corollary dignity makes limits imposed economic exploitation personal generally coincide general limits placed processing personal → see sections 3.2.1 3.2.2 including substantive limitations consent backdrop economic exploitation personal neither subject stringent rules general privileged way economic aspects frequently come play general protection rules applied however example consent may longer freely given subject exposed economic pressure 9 see e. g. section 22 german act protection copyright works art photographs gesetz betreffend das urheberrecht werken der bildenden künste und der photographie kunsturhg 10 way examples commission building economy 10 january 9 final available /ec.europa eu/transparency/regdoc/rep/1/2017/en/com-2017-9-f1-en-main-part-1.pdf arbeitsgruppe “ digitaler neustart ” der konferenz der justizministerinnen und justizminister der länder working group “ start ” conference ministers justice länder report 15 may pp 29 et seqq available /www.justiz.nrw.de/jm/schwerpunkte/digitaler_neustart/zt_bericht_arbeitsgruppe/bericht_ag_dig_ neustart.pdf .3.3.2. ownership issue financial compensation things stand ethics commission believe adequate grounds introducing additional ownership-like exploitation would allow subjects request economic share profits derived help often referred concepts “ ownership ” “ producer ” .10 protection law general private law already provide individual range legal effective vis-à-vis third parties basis individuals could theoretically make toleration activities dependent payment appropriate fee individual fails negotiate fee kind attributed circumstances e. g. lack negotiating power and/or poorly functioning competition nothing absence additional ownership-like exploitation theory imbalance negotiating power could counter-balanced introduction collective societies collectively exercise ownership-like exploit extending concept personal include ownership-like economic component would however potentially odds protection particular regards voluntary nature consent ability withdraw consent time request erasure would also create questionable financial incentives encouraging generation maximum personal would put pressure individuals particular vulnerable groups minors low earners disclose much possible industry passes costs remuneration customers privacyconscious individuals might also forced shoulder comparatively greater burden financial terms 105 e 3. standards personal arguments hold water extent comes anonymised however given huge number individuals contribute generation processing level complexity fair remuneration system 24/7 monitoring would required measure flows would proportion potential gains terms justice quality might also negatively affected since incentives would created generate “ artificially ” e. g. creation fake profiles ultimately producing distorted picture reality ethics commission therefore counsels introducing exploitation designed exclusive either anonymised types 3.3.3. counter-performance large number service types e. g. search engines social networks messenger services online games offered end users monetary consideration financed ways particular payments received third parties exchange personalised advertising personalised services targeted users user profiles user scores personal therefore often referred shorthand terms “ counter-performance ” services example original draft article 3 1 directive although term removed later point legislative procedure .11 extent economic model described fact compatible prohibition article 7 4 gdpr “ tying ” “ bundling ” consent provision service12 must ultimately clarified court justice 11 commission proposal directive parliament council certain aspects concerning contracts supply 9 december 634 final available /ec.europa.eu/transparency/regdoc/rep/1/2015/en/1-2015-634-en-f1-1.pdf 12 protection supervisor opinion 4/2017 proposal directive certain aspects concerning contracts supply 14 march p. 15 available /edps.europa.eu/sites/edp/files/publication/17-03-14_opinion_digital_content_en.pdf .the ethics commission argues referred “ counter-performance ” provided exchange service even though term sums issue nutshell helped raise awareness among general firstly personal form integral part individual ’ personality protected constitutional law secondly classification counter-performance might unintended consequences example might abused argument favour largely excluding data-related standard contract terms unfairness control justification triggering contractual sanctions consumers withdraw consent exercise erasure etc connection german legislator – implementing directive 2019/770 certain aspects concerning contracts supply services – leeway available member states way might prevent individual seeking legal remedies protection law particular individual withdraws consent processing provider may terminate provision service immediate effect however possible provider request payment services already provided retrospective automatic reversion pay option pay options increasingly discussed way avoiding “ tying ” “ bundling ” consent provision service yet even smallest financial burdens represents disadvantage particular vulnerable population groups may dissuade subjects encourage disclose excessive amounts personal also feared financial burden privacy-conscious individuals would disproportionate commercial users previously able certain services free e. g. company ’ page social networking site therefore preferred source funding 106 part e ata pay options may however increase consumer awareness financial value also create transparency reasons ethics commission believes offering pay options alternative may ethically acceptable way ensure consent given users genuinely free time however price must abusive exceed market prices consumer ’ perspective must represent realistic alternative disclosure personal ethical viewpoint safeguards must put place protect privacy-conscious users “ cross-subsidise ” users equally needs socially vulnerable groups must taken consideration example government transfers 3.3.4 basis personalised risk assessments price-related predictions obtained using algorithmic purpose personalised risk assessment e. g. one-off basis approving loan ongoing basis case black schemes operated insurance companies characterised higher level granularity ultimately sector-specific case certain profiling technique associated scoring procedures → see section 3.2.3 part f section 4.2.2 details profiling general processing additional personal purpose personalised risk assessments regularly requires consent subjects individuals hope gain economic advantages result particularly likely grant consent yet granting consent one individual may significant impacts others give rise chain reactions problematic ethical viewpoint unravelling effects may put subjects disproportionate pressure jeopardise voluntary nature consent.example 11 insured parties healthy particularly likely consent processing health insurance company result others come pressure also grant consent order avoid arousing suspicions regarding state health cases individual behaviour influence parameters models kind also significant influence people lead lives another ethical consideration particularly relevant insurance sector goal increasingly granular risk assessments runs counter basic principle collective risk sharing community insured persons taken extreme i. e. insurer access “ comprehensive ” adjusts price individual risk whole concept insurance would reduced absurdity ethics commission therefore believes personalised risk assessments must comply following ethical requirements particular processing must intrude core individual ’ private life must restricted areas individual already contact exterior world must therefore expect conclusions drawn basis behaviour principle dictates would ethically acceptable car insurance company example record miles driven traffic offences committed driver purely private behaviour inside vehicle even behaviour might relevant risk perspective e. g. often yawns whether chats passengers even driver ’ state health e. g. heart problems lifestyle factors e. g. purchasing behaviour relation coffee alcohol b causal relationship must exist processed risk determined linking must avoid discriminatory repercussions → see part f section 2.6 details 107 e 3. standards personal c must allow conclusions drawn directly implications relatives third parties full transparency required regards specific parameters weighting impacts pricing conditions individual must also provided comprehensible explanations improve conditions → see part f section 2.7 e order keep unwanted chain reactions check difference “ optimal ” conditions conditions apply consent refused must exceed certain ceiling e. g. maximum price difference 3.3.5 reputational capital coupled personalised economic conditions personalised prices personalised ranking personalised products services personal profiles scores serve reputational capital personalised behavioural rewards aimed increasing customer loyalty e. g. granting discounts depending quantity purchased previous month incentivise consumers consent processing personal may apt influence way lead lives evidence ethical limits outlined → section 3.3.4 currently disregarded german economy connection customer loyalty programmes come attention ethics commission developments continue monitored 13 cf article 9 regulation access many general provisions e. g. general terms conditions business ranking.in view ethics commission problems arising connection price differentiation narrow sense measures similar ilk relate regulation algorithmic → see part f details time however price differentiation morphs problem soon consumers led believe access prices lower overall disclosing much personal possible exhibiting certain behaviours tailored relevant criteria e. g. making online purchases using computer manufactured certain company conversely suggested consumers refuse consent processing purpose personalised pricing always pay higher prices average ethics commission believes latter would also pose ethically questionable risk voluntary nature consent true reputational also external third parties e. g. “ stars ” indicating someone profile online platform good person business gaining ever economic nonmaterial significance certain extent reputational kind covered regulation 2019/1150 promoting fairness transparency business users online intermediation services.13 regulatory approach chosen lawmakers drafted regulation – based part transparency requirements self-regulation – cautious ethics commission welcomes approach principle however worth noting certain sectors heavily dependent true reputational factor particular might lead significant lock-in effects may jeopardise competition cause problems individuals unable take switching different online intermediary platform 108 part e ata example 12 micro entrepreneur offers taxi services via online platform ranked highly many former passengers wishes switch platform take rankings ethics commission aware problems would arise general obligation recognise ranking profiles built different platform enshrined law however recommends federal government examine conditions commercial users profiles kind might nevertheless granted portability view lobbying broader regulation level.14 way contrast rise significance social reputation number “ likes ” “ followers ” “ friends ” part wider trend society – limited exception “ influencers ” – longer viewed predominantly lens personal economic asset must instead discussed relation systemic societal implications 3.3.6 tradeable items significant number companies already deriving financial gain cases earning great deal money compiling personal profiles scores personalised statistical evaluations carried using aggregated raw reselling third parties enriching existing profiles estimated placing market following section business models kind referred “ trading ” 14 cf example articles 6 7 draft “ model rules online intermediary platforms ” law institute made available ethics commission.the gdpr currently contain provisions relating specifically trading instead business models kind categorised merely normal processing operations subject general provisions gdpr many cases closer examination applicable provisions leads inescapable conclusion certain types trading infringe provisions gdpr therefore contrary law generally speaking however field trading characterised significant enforcement gap ethics commission therefore believes urgent action taken protection authorities relation sector protection board edpb alternatively conference independent protection authorities federal government länder konferenz der unabhängigen datenschutzaufsichtsbehörden des bundes und der länder develop – keeping gdpr ’ risk-based approach – clearly delimitable categories different types lawful trading greater clarity needed regarding instances trading subject must grant consent forwarding instances subject object processing instances compelling reasons rule even object regard general principles governing processing article 5 gdpr forwarding third parties permitted within closely prescribed limits situations covered existing provisions protection law ethics commission therefore recommends federal government speak level – connection forthcoming evaluation gdpr example – favour expanding scope gdpr include specific provisions trading following ethical considerations already enshrined gdpr taken account drafting future legal provisions kind 109 e 3. standards personal individual ’ informational selfdetermination starting point balancing exercise meaning trading principle requires prior consent subject due regard substantive limitations consent → sections 3.2.1 3.2.2 b processed legal basis consent likely occur isolated cases individual must straightforward opportunity exercise object advance e. g. unchecking checkbox immediately collected must forced communicate objection via separate communication channels c trading models deprive subjects choices whatsoever rarely considered extent need forwarded order interests manifestly outweigh countervailing interests comprehensive legislative clarification category required gdpr contains detailed provisions transfer processors forwarding third countries given rationale gdpr would illogical assume requirements apply transfers third parties within less stringent apply transfers outside certain points also inferred general provisions e. g. requirements regarded “ appropriate safeguards ” nevertheless ethics commission recommends urgent action taken clarify explicitly law obligations apply transferring third parties e. g. control obligations well circumstances parties may held liable.e controllers obliged document disclose specific source collected generated algorithmic well identity individual recipients must provided standardised machine-readable format allows e. g. automated management using privacy management tool/ personal management system → see section 4.3 details would take due account fact subjects largely dark regards existence traders means simple list different categories sources recipients would little f given large number traders market subjects able exercise effectively central mechanisms established facilitate process assume responsibility e. g. protection authorities → see section 3.2.4 privacy management tools/personal management see section 4.3 details g given dispersion effects give rise higher risks potential loss control traders subject certification obligation protection law includes regular audits certification bodies ethics commission recommends specific certification criteria adopted appropriate independent protection authorities federal government länder criteria take due account risks recommendations outlined 110 part e ata 3.4 inheritance modern communication technologies processing capacities make possible record every last detail individual ’ private activities decades end evaluate recordings using automated handing collected deceased individual heirs another third party adds whole dimension privacy risk deceased person particular individuals communicated lifetime often compared diaries personal correspondence comparison flawed many channels communication messenger services chats e-mails etc serve functional replacement ephemeral spoken word rather letters 3.4.1 precedence living wills ethics commission believes bestcase scenario subject make intentional informed dispositions lifetime many cases however people neglect make dispositions sole reason unaware legal practical options put level uncertainty backdrop ethics commission believes justified grounds obliging service providers alert users option making dispositions provide ongoing incapacity provide consent e. g. due dementia death provide technical means making said dispositions minimum barriers i. e. fewest possible changes medium corresponding provisions could added german telemedia act telemediengesetz tmg .15 15 previous discussion topic see mario martini juristenzeitung jz p. 1154.in view ethics commission situation following subject ’ death merely extreme example serve prompt reflection general design modes communication ethics commission therefore recommends federal government examine possibility making obligatory messenger services offer default option erasing messages certain period time user chose option message would automatically erased expiry relevant period unless manually archived recipient sender 3.4.2 role intermediaries growing awareness topic inheritance allowed business models flourish large number companies offering services field ranging central storage account passwords comprehensive inheritance management services may provide useful guidance also associated certain hazards including inadequate provision cases company goes bankrupt otherwise liquidated shortcomings security including genuine fraud ethics commission believes quality assurance regulations characterised cautious approach awareness-raising potential advantages risks required order protect citizens 111 e 3. standards personal addition recommends federal government part remit provide services general interest set least subject state supervision provides affordable basic inheritance protection planning services citizens services must reflect latest developments field security german citizen writes choose store privately notary district court similar options private privatesector solutions government-run service also available individual ’ inheritance 3.4.3 post-mortem protection ethics commission recommend wholesale rejection principles set forth german federal court justice16 regarding transfer estates heirs since potential advantages would far outweighed effects either undesirable and/ excessive different default solution e. g. trust model imposed law distinction user account regarded asset user account regarded highly personal conversely inheritance law apply nature user account e. g. online account alcoholics anonymous group renders within financially worthless highly sensitive cases principle telecommunications confidentiality applies inter alia protect deceased ’ communication partners legislator case still reconcile inheritance enshrined fundamental example corresponding reference part civil code devoted inheritance law 16 judgment german federal court justice 12 july ref iii zr 183/17.the principle set forth federal court justice – estate transferred deceased ’ heirs – linked existence contractual relationship contractual relationship transfer heirs take place owing highly sensitive nature heirs legal recourse since post-mortem protection provided gdpr also means legal recourse relatives current state protection law ethical concerns raised fact controllers almost unlimited power dispose deceased ’ personal result ethics commission therefore recommends federal government follow footsteps several member states make option provided recital 27 gdpr enacting provisions post-mortem protection even death subject latter ’ relatives able exercise fundamental erasure rectification incorrect time suitable measures taken ensure compliance dispositions made deceased lifetime even dispositions implied e. g. deliberate choice publish “ life story ” 112 part e ata 3.5 special groups subjects 3.5.1 employees fact employers collect employees ’ location performance widespread phenomenon certain modern workplaces poses significant risk employees ’ informational self-determination general personality true creation biometric profiles necessary precursor certain forms collaboration questions considered include legal basis processing granting co-determination employee representation bodies also obligations provide employees e. g. hazards posed multi-sensor fusion depending context opportunities object issues regarding retention procedures terms retention extent employees ’ may disclosed third parties rectification incorrect obsolete personal profiles example appropriate erasure procedures points consideration include framework conditions limited control surveillance employees restrictions tracking employees ’ locations ban comprehensive location profiles restrictions obligation share social media accounts allow employer access context “ bring device ” models framework conditions biometric restrictions psychological investigation methods.the ethics commission recommends federal government invite social partners work towards common legislative provisions adopted view stepping protection employee based examples best practices existing collective agreements concerns individuals non-standard forms employment also taken account process collective agreements works council agreements continue play significant part employee protection yet foundational principles employee protection regulated solely collective agreements works council agreements firstly employees covered latter secondly importance principles fundamental perspective also worth noting legal uncertainty currently reigning scope gdpr provisions negative impact investment security reference wider field legal bases processing employee ethics commission believes traditional construct consent protection law suitable contexts since difficult put place framework conditions necessary consent given voluntarily employment situations impossible find appropriate balance cases employer ’ needs option employees revoke consent request erasure time employee protection measures therefore legal grounds justification specifically tailored employment context guarantee high level protection appropriate weighing interests fundamental outcomes may look similar consent certain respects taking account power structures typically exist employment context 113 e 3. standards personal deciding whether interest groups granted co-determination rights17 relation processing within companies due regard must given asymmetry knowledge exists employers employees regards operating principles details processing operations need models go existing mechanisms allowing interest groups access external expertise time ensuring appropriate involvement company protection officer also protection trade secrets given constant advancement data-processing within companies software updates self-learning elements etc shift away consent single one-off event towards ongoing oversight processes interest groups progress field employee protection neglect stages applying job entering employment relationship example care must taken ensure provisions applicable law prohibit employers asking certain questions application procedure recruiting individual e. g. asking whether woman pregnant circumvented “ resources ” algorithms request grant employer access social media accounts 17 examples current legislative provisions see e. g. section 87 1 6 german works constitution act betriebsverfassungsgesetz betrvg relation works councils section 75 3 17 german federal staff representation act bundespersonalvertretungsgesetz bpersvg relation staff councils 18 german ethics council deutscher ethikrat big health opinion 30 november available /www.ethikrat.org/fileadmin/ publikationen/stellungnahmen/englisch/opinion-big-data-and-health-summary.pdf .steps must also taken ensure persons nonstandard forms employment excluded progress field employee protection upsurge forms employment platform economy means many people longer access traditional employee co-determination imbalance power arises client platform operator one hand contractors platform workers often significant may implications terms protection informational selfdetermination appropriate legislative provisions adopted ideally level institutional framework developed e. g. interest group mitigate risk 3.5.2 patients view benefits could gained digitalising healthcare basic principle ethics commission recommends swift expansion infrastructures sector introduction procedures reviewing assessing healthcare services range quality digitalised healthcare services improved allow patients exercise informational self-determination become health literate.18 even things stand today provision healthcare services involves processing huge volumes personal involved typically health genetic words special categories personal within meaning article 9 gdpr designing future health landscape primarily nature comprehensive account must taken need provide special protection time boosting self-determination patients health insurance policies inter alia field → see section 4.1 114 part e ata connection ethics commission emphasises urgent need introduce roll electronic health record view improving quality transparency cost-effectiveness medical care.19 given vital role electronic health record would play digitalising healthcare sector ethics commission wishes make greater attention paid security patient autonomy implementing system existing cryptosecurity concept based decentralised management keys pins insured parties continue apply example also possible electronic health record even patient incapable granting consent based provisions concerning legal representation otherwise apply regardless type health insurance policy held patient health services products collectively financed consumer-funded health market becoming ever important least healthcare services offered statutory health insurance funds far date important underestimate relevance services – include fitness health wellness apps particular self-monitoring apps associated wearables – context digitalised healthcare sector yet apps often questionable poorly verified quality meaning collect limited usefulness carries risk health affected patients users cases significant furthermore assumed patients able assess quality products services independently particular compliance principles protection security equally access healthcare services dependent individual financial wherewithal mind ethics commission welcomes plans federal institute drugs medical devices bundesinstitut für arzneimittel und medizinprodukte introduce procedure examining assessing apps kind 19 see respect ethics commission ’ previous recommendation participatory electronic health record dated 28 november available www.datenethikkommission.de .3.5.3 minors ethics commission welcomes efforts undertaken – include adoption legislation voluntary self-regulation – develop special protective mechanisms allowing minors exercise self-determination primary goal mechanisms step level protection degree protection profiling manipulation dark patterns addictive designs etc secondary goal provide greater protection age-appropriate glorifies violence example time however ethics commission wishes make protective mechanisms prove futile unless reliable identity management system place ensuring age minors detected treated appropriately relying users honest age without question wrong approach viewed lens ethics however would also problematic ask providers ascertain user ’ age collecting personal may highly sensitive e. g. facial recognition transferred provider ’ cloud time placing entire burden whoever holds parental authority may easily result situation latter feels much asked ethics commission therefore recommends federal government promote emergence family-friendly technologies allow minors exercise self-determined time reliably guaranteeing protection 115 e 3. standards personal ethics commission recommends federal government lobby level measures enforce compliance principles protection design default enshrined gdpr particularly case mobile end devices order protect informational self-determination minors protect privacy german protection authorities competition authorities media regulators technical regulatory authorities take action within relevant remits spheres responsibility force manufacturers operating mobile end devices providers services adhere legislative requirements apply age groups question services age-appropriate parties responsible procuring relevant view schools kindergartens also incorporate requirements tendering procedures detailed discussion need force manufacturers comply principle protection design default found → section 3.6.1 far action area concerned consideration also given introduction eu-wide obligation forces manufacturers child-friendly mobile end devices program outset devices specifically intended children ensure “ jail breaking ” “ rooting ” impossible possible key devices programmed way enforce compliance legislative provisions aimed protecting children services age-appropriate relevant settings enabled device/ operating system upon activation minors able change settings without parents ’ consent solution kind would also offer advantages parental control apps firstly apps often pose protection security problems secondly raise ethical questions terms opportunities afford total surveillance private life.3.5.4 vulnerable care-dependent persons many cases belonging vulnerable individuals processed benefit individuals e. g. care sector technologies make much safer older people remain environment accustomed example may also help alleviate negative impacts skills shortage care sector ensure better healthcare provision particular assistance – correctly – serve bridging adjust adaptively varying needs different people life bodily integrity also informational self-determination fundamental must reconciled accordance principle practical concordance particular consideration must given two questions particular whether risks posed life health extent informational self-determination encroached upon 116 part e ata ethics commission believes standards guidelines surveillance professionals care sector developed conference independent protection authorities federal government länder particular standards guidelines specify legal provisions upon professionals base action particular situations cases especially consent granted subject caregiver surveillance either prohibited possible basis article 6 1 f gdpr also arrangements provision whereby ethics commission takes differentiated surveillance options provided prior institutional setting e. g. care home kindergarten school consent must also obtained differentiated basis cases legal basis processing standards guidelines kind would also appropriate way provide legal certainty care home operators care staff reduce liability risks section 1901a civil code amended accordingly clarify fact living wills also include dispositions relevant subject grants prior consent processing basic principle particularly high level protection also accorded people homes since likely regard space within four walls safe privacy technologies opened expanding options surveillance private individuals private individuals e. g. surveillance romantic partners children persons disabilities range way ethically alarming prospect total private surveillance given awareness topic lacking many quarters ethics commission recommends awarenessraising campaigns area initiated federal government governments länder since latter often hold jurisdiction field although recommends federal government continue monitoring developments believe legislative measures e. g. criminal offences required present.3.6 protection technical design citizens companies government agencies parties entitled assert ethically justified obliged comply corresponding obligations must first place necessary technical framework must put place enabling technologies must play prominent role respect yet enabling technologies kind must lead situation responsibility protection fundamental freedoms offloaded onto individual users instead state must matter principle adopt regulations required provide reliable protection fundamental freedoms without need action part individuals 3.6.1 privacy-friendly design products services heading suggests article 25 gdpr makes mandatory controllers comply principles “ protection design default ” designers technologies must therefore take due account concerns relating protection based interpretation term applied article 5 gdpr following risk-adequate approach technical organisational measures must implemented end may required prior processing i. e. controller determines means processed well processing operation 117 e 3. standards personal design specifications protection law high level practical relevance relation end devices end devices designed worn wearables e. g. smartwatch smart textiles least carried close e. g. smartphone others designed mobile means e. g. networked car immobile e. g. smart home facilities designing software end devices kind amount time spent reflecting ethical questions raise depends likelihood close proximity private intimate spheres e. g. bathrooms bedrooms probability affect particularly vulnerable persons e. g. protection design default 1 working group conference independent protection authorities federal government länder das standard-datenschutzmodell – eine methode zur datenschutzberatung und -prüfung auf der basis einheitlicher gewährleistungsziele v.1.1 – erprobungsfassung standard protection model – method protection consulting assessment basis uniform warranty objectives v.1.1 – test version available /www.datenschutzzentrum.de/sdm/ .data protection design imposes conditions selection technical organisational measures relating state art implementation costs processing risk posed freedoms natural persons example protection default imposes conditions since principle must adhered without exceptions practice however often case excessive amounts personal e. g. identifiers processed inadequate restrictions placed processing retention periods long inappropriately high number people able access field “ privacy engineering ” therefore emerged banner additional protectionrelated goals non-linkability transparency intervenability standard protection model sdm german protection authorities incorporates goals “ warranty objectives ” .1 like baseline protection catalogues published federal office security bundesamt für sicherheit der informationstechnik bsi sdm defines modules controllers designers technologies basis choosing technical organisational measures appropriate protection needs although first modules currently available others planned fact many developers baseline protection catalogues iso 2700x series standards reference works means developers familiar fundamental concept able take better account legal requirements designing implementing technical choice centralisation decentralisation another question must clarified caseby-case basis designing technical general rule centralised allow operators exercise higher level control influence might good thing example underlying aim incorporate features contribute protection security yet also bad thing since potential misuse – either malicious third parties wanting steal sabotage processing operators exploiting large volumes amassed purposes notified subjects – greater stored centrally processing also controlled centrally designed appropriately however decentralised help decrease prevent linkability reduce disruptions overall system availability 118 part e ata children young people care-dependent persons persons disabilities extent encroach upon individual ’ personality high level responsibility autonomy granted demanded users assemble configure operate devices represents particular challenge attempting design technologies foster selfdetermination ethics commission recommends federal government step support r efforts technical standards end devices also urges federal government lobby level introduction technical requirements aimed safeguarding self-determination product safety private sphere particular reference end devices consumers ethics commission takes view following principles minimum enshrined end device requirements adopted ●products must protected cyber attacks improper measures taken must commensurate need protection comply state art suitable guarantees must provided particular sensitive e. g. health high level cyber resilience must achieved joint task incumbent upon state industry individual ●users must able times identify functions currently enabled particular must able see whether camera microphone gps sensors switched whether device connected internet whether transferred outside closed local area ●it must easy turn transfers including transfers outside local area stored locally function switched must transferred without user ’ consent next switched must also true individual applications e. g. smartphones smart tvs ●if basic device functions technically possible without transfers kind functions must remain available transfers turned e. g. smart fridge must continue keep contents cool ●devices supplied “ user onboarding ” software onboarding take place automatically devices first put operation possible repeat onboarding process often necessary even second users provided users cover mode operation also collection processing user ●if end devices direct connection internet e. g. routers secured using password possible put operation without changing factory password beforehand system side passwords allowed comply state art 119 e 3. standards personal way products services applications designed huge influence extent controllers processors able comply protection obligations incumbent upon yet manufacturers directly responsible processing personal fall outside scope gdpr controllers want solutions developed must therefore insist “ bakedin ” protection.20 mind ethics commission recommends federal government either take steps support action parties aim forcing manufacturers shoulder greater share responsibility suitable measures might include following 20 cf recital 78 gdpr 21 christiane wendehorst verbraucherrelevante problemstellungen zu besitz- und eigentumsverhältnissen beim internet der dinge teil 2 wissenschaftliches rechtsgutachten consumer-oriented problems relating possession ownership structures internet things part 2 scientific legal opinion studien und gutachten im auftrag des sachverständigenrats für verbraucherfragen studies opinions behalf advisory council consumer affairs december p. 120 available /www.svr-verbraucherfragen.de/wp-content/uploads/wendehorst-gutachten.pdf ●direct imposition legislator product design product safety requirements ●new effective legal remedies along distribution chain shift burden responsibility inadequate protection design default onto manufacturers21 whereby certain amount progress made directive 2019/771 certain aspects concerning contracts sale goods terms shifting burden responsibility consumers onto retailers along distribution chain comprehensibility transparency protection design also encompasses comprehensibility transparency including applications scripts sources elements point time procedure process ethics commission welcomes ongoing efforts develop best-practice models good terms conditions business “ onepagers ” consumers part multi-level approach consumers initially provided simple “ boiled-down ” important processing operations necessary informed detail general terms conditions business protection measures however approach solve underlying problem provided often fails job either inadequate and/or exceeds consumer ’ capabilities consumers make informed purchase decisions standardised machine-readable readily understandable graphical symbols icons introduced level following broad consultations industry civil society icons convey key characteristics products including products apps services “ basic functions available internet connection ” “ internet connection required enhanced functions ” “ user transfers ” “ user tracking ” examples possible characteristics icons could also colour coded would particularly useful case product characteristics apply greater lesser degree ethics commission recommends federal government lobby commission develop standardised icons kind keeping article 12 8 gdpr increased transparency consumers could also achieved supporting certified electronic shopping assistants would identify product brick-and-mortar online shop serve product consumer format likely understand 120 part e ata ●calls tenders guidelines procurement measures designed way require evidence all-round compliance gdpr including principles protection design default ●incentives encourage compliance particularly high standards protection design default example requirements effect government funding programmes 3.6.2 privacy-friendly product importance protection technical design must also taken account product enhancement stages applies particular algorithmic since latter typically require bulk example training → see part c section 2.2 details privacy-friendly training algorithmic 1 datatilsynet privacy report january pp 27 et seq available /www.datatilsynet.no/globalassets/global/english/ai-and-privacy.pdf .various options available complying principles protection enshrined article 5 gdpr training algorithmic january example datatilsynet norwegian protection authority proposed privacy-friendly means methods training algorithmic systems:1 8. minimisation procedures relation training e. g. synthetic using generative adversarial networks example federated data-minimising variants proposed neural networks 9. encryption procedures differential privacy homomorphic encryption procedures allow retrieval without granting full access database 10. procedures promote transparency achieve higher level comprehensibility traceability ethics commission believes still needed areas also applies options privacy-friendly testing algorithmic 121 1 ethics commission recommends measures taken ethically indefensible uses examples uses include total surveillance profiling poses threat personal integrity targeted exploitation vulnerabilities addictive designs dark patterns methods influencing political elections incompatible principle democracy vendor lock-in systematic consumer detriment many practices involve trading personal 2 protection law well branches legal system including general private law unfair commercial practices law already provide range instruments prevent ethically indefensible uses however spite widespread impact enormous potential harm little done date terms harnessing power instruments particularly market giants various factors contributing enforcement gap must tackled systematically 3 well steps make front-line players e. g. super visory authorities aware existing options urgent need legislative framework force fleshed clearly strengthened certain areas examples recommended measures include blacklisting data-specific unfair contract terms fleshing data-specific contractual duties fiduciary nature data-specific torts blacklisting certain data-specific unfair commercial practices introduction much detailed legislative framework profiling scoring trading 4 order allow supervisory authorities take action effectively authorities need significantly better material resources attempts made strengthen formalise cooperation different protection authorities germany thereby ensuring uniform coherent application protection law attempts fail consideration given centralisation market-related supervisory activities within federal-level authority granted broad mandate cooperates closely specialist supervisory authorities authorities land level remain responsible supervisory activities relating sector however.summary important recommendations action standards personal 122 part e ata 5 ethics commission believes “ ownership ” i. e. exclusive modelled ownership tangible assets intellectual property would solve problems currently facing would create problems instead recommends refraining recognition also advises granting subjects copyrightlike economic exploitation respect personal might managed collective societies 6 ethics commission also argues referred “ counter-performance ” provided exchange service even though term sums issue nutshell helped raise awareness among general regardless protection authorities court justice ultimately take regard prohibition gdpr “ tying ” “ bundling ” consent provision service ethics commission believes consumers must offered reasonable alternatives releasing commercial e. g. appropriately designed pay options 7 stringent requirements limitations imposed personalised risk assessment e. g. “ black ” premiums certain insurance schemes particular processing may intrude intimate areas private life must causal relationship risk difference individual prices charged basis personalised nonpersonalised risk assessments exceed certain percentages determined also stringent requirements respect transparency nondiscrimination protection third parties 8 ethics commission advises federal government consider issues falling heading “ inheritance ” settled federal court justice ’ ruling ephemeral spoken word replaced many situations communications recorded less entirety possibility records handed deceased ’ heirs adds whole dimension privacy risk range mitigating measures taken including imposition obligations service providers quality assurance standards estate planning services national regulations post-mortem protection 9 ethics commission recommends federal government invite social partners work towards common legislative provisions adopted view stepping protection employee based examples best practices existing collective agreements concerns individuals non-standard forms employment also taken account process 10 view benefits could gained digitalising healthcare ethics commission recommends swift expansion infrastructures sector expansion range quality digitalised healthcare services include measures better allow patients exercise informational self-determination measures could taken respect include introduction roll-out electronic health record building participatory process involves relevant stakeholders procedures reviewing assessing medical apps insurer-funded consumer-funded health markets 123 e 3. standards personal 11 ethics commission calls action significant enforcement gap exists regard statutory protection children young people sphere particular attention paid mandatory provision technologies including effective identity management default settings guarantee reliable protection children young people also familyfriendly i. e. neither demand much parents guardians allow even encourage excessive surveillance home environment 12 standards guidelines handling personal vulnerable care-dependent persons introduced provide greater legal certainty professionals care sector time consideration given clarifying relevant legal provisions living wills may also include dispositions regard future processing personal far processing require care-dependent person ’ consent e. g. dementia patients provide legally valid consent 13 ethics commission believes number binding requirements introduced ensure privacy-friendly design products services principles privacy design privacy default gdpr imposes controllers already put practice upstream manufacturers service providers requirements would particularly important regard consumer equipment context standardised icons also introduced consumers able take informed purchase decisions 14 action must also taken number different levels provide manufacturers adequate incentives implement features privacy-friendly design includes effective legal remedies pursued parties along entire distribution chain ensure also manufacturers held accountable inadequate application principles privacy design privacy default consideration also given particular requirements built tender specifications procurement guidelines bodies conditions funding programmes applies privacy-friendly product including training algorithmic 15 debates protection tend quite rightly centre around natural persons important ignore fact companies legal persons must also granted protection almost limitless ability pool together individual pieces means obtaining comprehensive picture company ’ internal operating procedures passed competitors negotiating partners parties interested takeover bid poses variety threats – inter alia sovereignty germany europe – view significant volumes flow third countries many ethics commission ’ recommendations action therefore also apply mutatis mutandis basis legal persons ethics commission believes action must taken federal government step level data-related protection afforded companies 124 part e ata 4. improving controlled access personal types personal non-personal represent key resource within economy serve vital ingredient many applications foster good breakneck speed technologies – benefit every one us enormously – attributed part ability evaluate generated billions users although protection must always remain central priority applications involving personal people asking whether general improvements area controlled access personal might ethically tenable even desirable keeping principle sharing good → section 1.3 within framework prescribed protection law 4.1 enabling uses personal 4.1.1 preliminary considerations serves basis almost technical achievements current onslaught digitalisation means data-based becoming increasingly important significance already recognised gdpr backed certain cases national law i. e. german federal protection act bundesdatenschutzgesetz bdsg protection acts länder ethics commission wishes emphasise fact processing operations involving genetic biometric health enormous value terms furthering goals promoting preventive methods developing diagnostic therapeutic approaches holds promise significant progress certain areas – depending problem tackled – may rely large pools issue releasing health purposes referred “ donation ” recurrent topic debate term “ donation ” misleading however donated – unlike organs money – reused often necessary parallel even donor 22 cf conference independent protection authorities federal government länder orientierungshilfe der aufsichtsbehörden für anbieter von telemedien guidance supervisory authorities telemedia providers march p. 14 available /www.datenschutzkonferenz-online.de/media/oh/20190405_oh_tmg.pdf .provided part described public-good activity terms way uses e. g. providing healthcare services developing sustainable mobility concepts improving living conditions broader sense ethics commission recommends full made existing privileges protection law viewed particularly valuable good weighing competing interests.22 additionally recommends länder exercise regulatory powers already hold example area higher education law within framework protection law way foster innovation keeping aforementioned notion special privileges broad interpretation placed term “ scientific ” context inter alia reference consistent past decisions federal constitutional court irrelevant whether question carried government-funded private institutions ethics commission wishes point – challenging though task may – appropriate balance must sought researchers ’ fundamental subjects ’ informational self-determination carrying weighing interests required law special priority accorded protection sensitive associated subjects patients insured parties example duty confidentiality imposed certain individuals doctors subject code professional secrecy cf section 203 german criminal code strafgesetzbuch stgb may also apply work institutions latter collected stored individuals question procedural precautions imposed law view protecting informational self-determination would need observed 125 e 4. improving controlled access personal 4.1.2 legal clarity certainty although law currently stands permits promotes data-based questions interpretation arise relation certain details questions require clarification supervisory authorities courts example yet definitively clarified whether processing already lawfully collected one purpose e. g. healthcare provision – basis article 5 1 b gdpr light recital 50 “ appropriate safeguards ” within meaning article 89 gdpr – automatically deemed lawful processed purposes whether requirement separate legal basis pursuant article 6 1 – 3 article 9 gdpr applies first collected example section 27 federal protection act states healthrelated processed express consent provided interests “ substantially outweigh ” subject ’ interests also suggested certain quarters process invoked party collected first place similar uncertainty reigns scope term “ ” regards product enhancement even though legal framework exists data-based germany inter alia relation healthrelated special categories finer details regulatory framework lack uniformity country ’ federal structure means federal government länder hold constitutionally enshrined legislative powers perspective resulting legal uncertainty exacerbated yet ongoing lack reliable guidance particular regards criteria must met order consent deemed valid order subject ’ interests “ substantially outweighed ” interests within meaning section 27 federal protection act legal uncertainty could prove stumbling data-based germany ethics commission believes recommendations action interpretative criteria therefore developed – perhaps conference independent protection authorities federal government länder involvement relevant stakeholders politics healthcare industry civil society – relevant rules applied feasible legally compliant way pseudonymisation anonymisation standards → see section 4.2 view harmonisation aimed overcoming regulatory discrepancies field different regulatory approaches member states division regulatory scope federal protection act protection acts länder special regulations specific subjects ethics commission recommends federal government push synchronisation research-specific legal bases federal protection act protection acts länder subjectspecific acts b drive forward projects level aimed greater harmonisation regulatory frameworks put place member states respect protection c lobby duty notification incumbent upon member states adopting national laws area establishment clearing house cross-border projects 126 part e ata 4.1.3 consent processes sensitive voluntary informed explicit consent subject critically important means protecting individuals test subjects participating projects particularly case clinical involving health particularly sensitive categories provides test subject opportunity exercise informational self-determination since necessitates provision easy-to-understand project also ensures test subject discover later date values preferences prevent participating study protective instrument enshrined law improves transparency therefore increases people ’ level confidence least among benefits fact also promotes integrity researchers yet researchers act controllers face considerable challenges comes obtaining informed consent particularly project involves sensitive example researchers want embark project using health already available database subjects must contacted consent obtained unless subjects originally consented reuse future provided – term preferred within ethics discourse – broad consent researchers wishing health collected course routine medical care purposes must first contact patients ask grant informed consent task fraught huge practical obstacles mind ethics commission recommends appropriate model procedures obtaining consent designed developed view making easier process purposes.with explicit reference link exists consent subject ’ fundamental ethics commission also calls innovative consent models sector dynamic consent models involve tailoring declarations consent individual context already trialled example connection must ensured consenting party remains able control even granting consent order ensure case ethics commission recommends emphasis placed design privacy management tools pmt personal management pims → see section 4.3 sector consent assistants agents consent assistants kind may make significantly easier subjects keep track processing operations granted consent even operations commenced equally may make possible go back ask subjects consent circumstances change provide subjects straightforward way revoking consent calls heard increasingly often – particularly connection using health – blanket consent models involve subject granting consent wide range uses field without reference specific course treatment event although sector advance compelling reasons models kind number concerns obstacles must overcome adopted particular need consent informed linked specific purpose would make impossible take consenting party ’ preferences values account differentiated basis even far-reaching legal safeguards provided misuse encroachments upon privacy 127 e 4. improving controlled access personal backdrop ethics commission recommends discussion innovative model known “ meta consent ” .23 appropriately informed – without situation consent specifically required – subject decides type projects contexts wishes grant consent type consent involved specific broad consent may limited basis considerations following ●research context e. g. private commercial non-commercial national international ●data sources e. g. electronic health record tissue health lifestyle wearables ●type e. g. preventive cancers neurodegenerative disorders kind health researchers later wish specific project subject informed advance given opportunity object 23 thomas ploug søren holm bioethics 30:9 pp 721 et seqq.each real-life implementation model oversight trust scheme ethics commission another responsible tasked ensuring consenting party ’ preferences fact taken account also possible subject amend terms meta consent time technical regulatory framework required must place example 13 example 13 subject specifies electronic health record may commercial also specifies blood tissue samples may commercial degenerative diseases consents processing electronic health record provided transferred europe company spain would like electronic health record well tissue samples dementia subject informed intention told four weeks object way 128 part e ata deliberating designing model kind care must taken ensure constraints placed freedom privilege secondary equivalent scope restrictions imposed current legal system preference given meta consent models emphasise ability subjects express values preferences regarding health purposes would also increase confidence health governance another ethical question must considered accountability – relation also relation non-use since may potential progress vital areas result discrimination certain groups result exclusion progress example methodological reasons mean clinical studies involving older people suffering several different chronic diseases taking several different kinds medication time must necessarily limited scope highquality procedures evaluate health however key findings might obtained interactions different medications actions everyday conditions findings could productive basis extensive treatment patients going forwards mind given significance healthcare sector medical economic perspective ethics commission recommends proactive support “ healthcare system ” healthcare provision continuously improved making systematic qualityoriented health generated day-today basis keeping principles evidencebased medicine healthcare system imposes high requirements terms multi-level governance requires cross-disciplinary approach healthcare provision puts insured party patient front centre.4.1.4 legal protection discrimination time however ethics commission wishes emphasise parties involved developing designing health-related projects must take due account significant potential discrimination opened availability sensitive e. g. subject looks job takes insurance policy technical progress made possible sequence decode genome scientists able analyse biometric behavioural collected course daily life means also possible profile individual ’ risk falling ill future typically based likelihood suffer disease – genetic come play relatives may also affected mind federal government examine possibility including grounds action german general act equal treatment allgemeine gleichbehandlungsgesetz agg well specific bans using person ’ health way analogy corresponding provisions genetic german genetic diagnostics act gendiagnostikgesetz gendg 129 e 4. improving controlled access personal 4.2 anonymisation pseudonymisation synthetic operations involve accessing personal must always comply applicable provisions protection law abide rules processing laid provisions – purpose limitation principle appropriate protective measures certain circumstances therefore may vitally important businesses users know certain operations either fall outside scope protection law compliant protection law ethics commission believes lack legal certainty number different areas example concerning anonymisation pseudonymisation identification consideration link individuals allegedly anonymised sets synthetic anonymised pseudonymised anonymisation involves processing set personal way link subject broken irrevocably distinction made randomisation generalisation different ways approaching task anonymisation individually combination randomisation involves modifying way anonymised longer matched subject achieved falsifying individual sets example appropriately designed randomisation methods ensure statistical properties original set retained example swapping values rather changing generalisation involves aggregating pieces less detailed age categories instead dates birth names regions instead postcodes periods time instead time stamps accurate nearest second three main strategies identify natural persons set singling method pinpointing sets relating specific individuals larger pool example using unique characteristics make possible identify individuals b linkability method involves linking least two sets relate individual group individuals basis matching values appear sets identifiers spatial coordinates times even small amount available individual augmented using linking strategy allowing identified c inference method involves deriving highly probable value characteristic values number characteristics allowing relating individual augmented increasing likelihood identified 130 part e ata anonymised sets make impossible recreate links existed individuals relate create links first time given technological means reasonably likely available developed time processing cf recital 26 gdpr attacker wishing identify one subjects deanonymisation would find task impossible modifications set – particular addition fuzziness also referred noise blurring depending context – ensure impossible pull belong specific individual linkable inferences drawn modifications typically also place constraints utility user aware evaluations later carried using set anonymisation procedures optimised mind example retaining necessary level detail relevant characteristics wherever possible applies comparisons different sets interoperability user knows comparisons carried appropriate anonymisation methods designed categorising identical groups required taking account increase risk may occur result incorporating sets pseudonymisation involves processing way longer assigned specific subject without additional may take form mapping tables cryptographic hash methods example pseudonymisation differs anonymisation reference person legal sense term retained controller must prevent unauthorised access additional whenever pseudonymised processed future since otherwise would possible map subjects gdpr refers pseudonymisation several times technical organisational measure reducing risk freedoms natural persons.both anonymisation pseudonymisation involve processing set already available must distinguished pseudonyms deployed user side users may choose pseudonymised identifiers e. g. user names online services e-mail addresses identifiers provided automatically technological system example online id function electronic id card attribute-based authorisation certificates designed protection concerns mind vast majority cases pseudonyms provides little way protection identification subject particularly across contexts communication partners allows user-specific profile linked augmented conversely constantly changing “ transaction pseudonyms ” restricted specific context making much harder identify individual question internet-based procedures aimed concealing link subject relating subject generally regarded anonymisation strict sense term may nevertheless provide level protection identification observation simple web proxies make possible surf internet using identifier i. e. ip address intermediary server multiple users whose identifiers known proxy server may therefore identifier far destination web servers concerned provided avoid identifying cookies etc steps prevent identification taken arranging multiple intermediary servers one behind another example mix networks tor mix cascades jondo noise added sending artificially created “ dummy traffic ” additional obstacle path anyone attempting observe users 131 e 4. improving controlled access personal 4.2.1 procedures standards presumption rules often possible anonymise – i. e. completely break link subject belong way recreated – without losing ’ utility time however perfect anonymisation often required firstly many goals upon closer examination achieved using somewhat lower level utility secondly gdpr already contains exemptions processing operations serve good e. g. sector meaning even personal processed without obtaining consent subjects nevertheless efforts aimed developing effective anonymisation technologies procedures stepped view allowing processed wholly outside scope gdpr ultimately legal certainty achieved developing standardised technologies procedures must always take due account whirlwind pace technological ethics commission therefore recommends federal government lobby – particular level – easy-to-use anonymisation standards would benefit subjects users pseudonymisation measures commensurate level risk faced subjects private lives featured agenda federal government ’ summit 24 federal office security technical guidelines bsi tr-02102 cryptographic mechanisms recommendations key lengths last updated february available /www.bsi.bund.de/shareddocs/downloads/en/bsi/publications/techguidelines/tg02102/bsitr-02102-1.pdf __blob=publicationfile v=9 .in particular anonymisation standards combined rules imposing rebuttable legal presumption would provide legal certainty users could rely processing operations falling outside scope gdpr standard met context important remember restrictions may need imposed presumption rules example period validity way analogy cryptographic procedures ,24 authorised methods processing example stating may published made accessible unspecified number people long legal basis rebuttable presumption rules federal government support technical best practices industry-specific codes conduct view building experience fields certain fields standardisation anonymisation pseudonymisation procedures may also impose rules way link subject relating broken making possible compare different sets improving interoperability least areas improved interoperability sought-after outcome ethics commission recommends context-specific rules developed preferred groupings e. g. value ranges age categories postcodes ip addresses similar approach already followed germany ’ statistical offices handling example 132 part e ata anonymisation pseudonymisation procedures carried repositories known least suspected contain personal differ repositories thought contain personal could means least starting point either combination creating link purportedly anonymous subject belong ethics commission recommends binding implementation standardised methods checking whether subjects identified set methods must allow user conclude reasonable degree certitude either personal non-personal 4.2.2 ban de-anonymisation presumption rules also accompanied appropriate bans de-anonymisation infringement bans i. e. cases proves possible identify subject using formerly anonymous example result technological developments subject penalty bans would need designed way avoid placing roadblocks way detection removal links subjects repositories since options de-anonymisation available must investigated view developing appropriate anonymisation standards verifying effectiveness addition introduction bans de-anonymisation penalties infringement must misused pretext downgrading standards apply anonymisation diluting meaning term “ personal ” gdpr since companies involved vitally important efforts drive forward anonymisation using technical means would otherwise placed competitive disadvantage applies reversal pseudonymisation absence justified reasons list drawn 25 jörg drechsler nicola jentzsch synthetische daten innovationspotenzial und gesellschaftliche herausforderungen synthetic potential innovation societal challenges stiftung neue verantwortung may available /www.stiftung-nv.de/sites/default/files/synthetische_ daten.pdf .4.2.3 synthetic distinction made genuine synthetic i. e. generated artificially rather collected directly real world synthetic boast several advantages real-world 25 firstly produced quantity particularly important dealing simulations real-world generated secondly steps taken synthetic created ensure entire range values mapped comprehensively possible e. g. order test technical system would behave confronted unusual combinations thirdly quality synthetic measured necessary guaranteed individual cases properties set real-world reference retained alternatively distortions occurring sets real-world pinpointed removed order avoid discrimination set synthetic contains references persons anonymous fall within scope gdpr ethics commission recommends federal government support field synthetic number different issues including question whether extent contexts synthetic might replace realworld processing operations closely synthetic resemble real-world terms properties ethics commission recommends investigations creation synthetic particular emphasis topics including quality avoidance bias discrimination 133 e 4. improving controlled access personal 4.3 controlled access management trust schemes 4.3.1 privacy management tools pmt personal management pims ever complex environment one major challenges faced individuals exercising lack oversight personal – subjects typically records documenting times granted consent example sharing original controller also result “ scattering ” significant decrease transparency corresponding increase protection risks subjects → see section 3.3.6 regarding problem trading currently enough standards software tools subjects track control ongoing basis granted access transferred would necessary exercise effectively increasing number technical institutional measures proposed response problem privacy management tools pmt range applications make consent management easier users dashboards etc tools automatically implement individual user preferences “ agents ” much provision technical applications rather service end common term personal management pims services range single sign-on services local safes online storage offers comprehensive less third-party management user trust models designed trust models pims may support selfdetermination shouldering responsibility exercising subject ’ protection law granting withdrawing consent exercising rectify erase portability object ethics commission recommends federal government promote innovation standardisation relation software tools services kind 4.3.2 need regulation pmt/pims notwithstanding privacy management tools/ personal management may pose risks fail comply certain requirements go beyond scope gdpr tools fail properly designed example risk subjects empowered exercise true self-determination instead unwittingly find path external determination particular privacy management tools/ personal management designed way subjects “ write blank cheque ” handing majority decisions operators tools/systems result subjects taking decisions contrary interests influence tools/systems would ultimately inconsistent ethical value self-determination privacy management tools/personal management must available aids subjects must usurp power latter take self-determined decisions must certainly manipulate using dark patterns et al → see section 3.2.2. 134 part e ata given significant risks tools may pose fundamental lack options subjects carry quality assurance measures ethics commission recommends federal government develop quality standards privacy management tools/personal management introduce certification monitoring system latter apply particular act behalf place subject – result technical design – play major role steering channelling subject ’ decisions cases stored directly operators tools/systems i. e. stored decentralised basis simply managed also possible provision must also made company ’ insolvency liquidation privacy management tools/personal management operate reliably cooperation part relevant controllers guaranteed possibility achieve wideranging coverage required imposing legal obligation applies appropriate conditions controllers within meaning gdpr view ensuring access personal monitored tool/system relevant terms protection reaches tool/system tool/system effectively protect subject ’ interests relation personal sector-specific approach – social networks example – might realistic option start with.in view ethics commission kind could either operated non-profit basis without involvement commercially motivated actors – charitable foundations similar independent bodies – organised private-sector enterprises provided operator derives profits managing rather using either case fiduciary duties owed subject must precisely defined legislation involvement parties conflicting interests must ruled appropriate opportunities oversight must built system whole minimise bias discrimination privatesector option chosen also necessary ensure operator ’ commercial motivations undermine role plays custodian subject ’ interests operators access personal based union ethics commission recommends federal government lobby appropriate amendments gdpr form clearer legally secure framework privacy management tools/personal management steps also taken addition action legal matters relating mandates etc prevent excessive centralised storage personal since arrangements kind increase level risk subjects event cyber attacks similar incidents machineinterpretable formats communication protocols must standardised automated execution services 135 e 4. improving controlled access personal 4.3.3 pmt/pims potential interface economy provided appropriate regulations adopted privacy management tools/personal management could also serve dual function one hand tools/systems might help individuals exercise informational self-determination effectively verify compliance limitations imposed hand however could also release confines “ silos ” allow within economy particular exercising portability granted article 20 gdpr main idea underlying privacy management tools/ personal management improve individual ’ control personal promote third-party access indirect access function might however compatible principle underpinning trust schemes third parties allowed access pursue certain purposes approved subject → connection example see section 4.1.3 economic exploitation served subject ’ interests took place express consent → see section 3.3 discussion problems raised treating personal economic asset ethics commission believes – decided privacy management tools/personal management play dual role also serve platform legally secure access companies – must ensured qualified dual-function tools/systems ultimately subvert goal protecting subjects ’ strict compliance principles privacy ethics design must enforced particular objective pursued must broadest possible exploitation “ scattering ” ethics commission wishes emphasise fact privacy management tools/personal management must continue serve dedicated custodians subjects ’ interests conflicts interest must ruled 136 part e ata 4.4 access portability 4.4.1 promotion portability portability granted article 20 gdpr tool subject determine whether companies gain access personal another company already collected companies includes receive provided “ structured commonly machinereadable format ” transmitted directly another controller portability two main implications prevents unwanted lock-in effects subjects switch providers thereby protecting individual subjects ’ economic selfdetermination free competition b even subjects switch providers allows ask controller make available either companies provides companies option gaining access might otherwise available bearing mind need separate legal basis processing protection law e. g. consent contract .26 26 example debates requirement separate legal basis kind protection law see article 29 protection working party guidelines portability wp 242 rev 01 last revised adopted 5 april p. 7 available /ec.europa eu/newsroom/document.cfm doc_id=44099 .despite fact providing “ structured commonly machine-readable format ” basic prerequisite must met order subjects exercise portability effectively date requirement subject enormous range varying interpretations practice ethics commission therefore recommends federal government protection authorities – implementation recital 68 gdpr – support industryspecific codes conduct standards level portability realised uniformly effectively practice benefit parties involved absence intermediaries → see section 4.3 stimulus exercise portability often stems company gained customer companies offer convenient automated process subjects exercise portability likely particularly successful e. g. provider map service allows ported mobility service provider click button also grounds assuming – view potential network effects effects – companies likely benefit portability least medium term already hold dominant market accumulated huge amounts ethics commission therefore recommends federal government observe developments closely far judges necessary lobby level measures specifically encourage facilitate porting market-dominant data-rich companies market participants including start-ups 137 e 4. improving controlled access personal 4.4.2 scope portability extended debates ongoing whether scope portability extended various ways particular expanding cover raw provided controller e. g. certain forms processed derived widening include dynamic real-time portability e. g. real-time streaming flows things currently stand recommendation ethics commission proposes federal government lobby amendments gdpr aimed extending scope current portability given gdpr force short period time “ wait see ” approach instead adopted clarity gained practical application supervisory practice protection authorities interpretation courts 4.4.3 portability interoperability interconnectivity network effects e. g. case messenger services mean portability alone sufficient mitigate risks posed existing future service oligopolies lower barriers market entry competitors extent represent serious challenge market-dominant providers ethics commission therefore recommends federal government push introduction sector-specific interoperability obligations sort previously imposed postal services mobile telephony example time measures must taken ensure interoperability features comply protection principles privacy-friendly default settings examples include option different changing identifiers instead single universal identifier reduction central components collect large volumes suitable examples interoperable technical interaction different levels.asymmetric interoperability obligations could imposed powerful companies market entrants respectively example market-dominant provider messenger services might obliged allow customers smaller providers send messages directly customers allow customers send messages directly customers smaller providers time however must ensured interoperability requirements abused purpose increasing yet flow personal towards data-rich powerful companies risk reliably averted would useful impose certain interconnectivity obligations e. g. short messaging services social networks view counteracting concentration effects networks promoting aims portability effectively i. e. healthier competition easier access market entrants data-intensive economy model kind also prerequisite building strengthening certain basic services society europe thereby promoting sovereignty germany europe 138 part e ata 4.5 crowdsensing good crowdsensing also hailed way opening resources society economy order deploys users ’ technical devices form “ sensors ” collect certain locality example forward higher-level instance analyses collected ethics commission acknowledges potential inherent especially put good example crowdsensing smart city real-time analysis traffic conditions state repair infrastructure air quality time however ethics commission believes achieving ethically appropriate design significant challenge analysis carried using crowdsensing techniques typically extremely high level granularity meaning involved may fall category “ sensitive ” perspective individuals generated certain circumstances also perspective anyone vicinity efforts must therefore stepped introduce standards anonymisation pseudonymisation → see section 4.2 view preventing situations traced back non-consenting users potentially persons affected also forms misuse crowdsensing-related transfers may also overstrain resources users ’ devices raise security issues.consideration must given points even users participate voluntarily intentionally crowdsensing programmes “ participatory sensing ” thought must therefore given substantive limitations consent exist connection → see section 3.2 even purposes serve good must always ensured requirements outlined legislation – particular protection law consumer protection law – complied full case also remembered decisions measures taken government agencies must based solely customarily collected using participatory sensing techniques since necessarily incomplete owing voluntary nature participation likely also exhibit bias ethics commission believes discussion whether crowdsensed personal collected forwarded compiled without user ’ knowledge “ opportunistic sensing ” ignores potential measures violate fundamental principles protection believes decisions must taken case-by-case basis whether legal obligation justifiably imposed force subjects make available technical devices devices collected forwarded automatically extent analysis promotes vital interests 139 summary important recommendations action improving controlled access personal 16 ethics commission identifies enormous potential purposes serve interest e. g. improve healthcare provision protection law currently stands acknowledges potential principle granting far-reaching privileges processing personal purposes uncertainty persists however particular regards scope so-called privilege secondary scope counts “ ” context product ethics commission believes appropriate clarifications law necessary rectify situation 17 fragmentation research-specific protection law within germany among member states represents potential obstacle datadriven ethics commission therefore recommends research-specific regulations harmonised federal land level different legal within introducing notification requirement research- specific national law could also bring improvement could establishment clearing house cross-border projects 18 case involving particularly sensitive categories personal e. g. health guidelines produced researchers obtain consent legally compliant manner innovative consent models promoted explicitly recognised law potential options include roll-out consent assistants recognition so-called meta consent alongside endeavours clarify scope privilege secondary 19 ethics commission supports principle move towards “ healthcare system ” healthcare provision continuously improved making systematic quality-oriented health generated day-to-day basis keeping principles evidence-based medicine progress made direction however greater efforts must made time protect subjects significant potential discrimination exists sensitive categories might involve prohibiting exploitation beyond defined range purposes 140 part e ata 20 procedures standards anonymisation pseudonymisation central efforts improve controlled access formerly personal legal presumption compliance standard achieved longer qualify personal “ appropriate safeguards ” provided respect subject ’ would improve legal certainty long way measures accompanied rules – pain criminal penalty – prohibit de-anonymisation anonymised e. g. becomes available would allow re-identification subjects reversal pseudonymisation absence narrowly defined grounds also field synthetic shows enormous promise funding funnelled area 21 fundamentally speaking ethics commission believes innovative management trust schemes hold great potential provided designed robust suited real-life applications compliant protection law broad spectrum models falls heading ranging dashboards perform purely technical function privacy management tools pmt comprehensive consent management services personal management services pims underlying aim empower individuals take control personal overburdening decisions beyond capabilities ethics commission recommends field management trust schemes identified funding priority also wishes make adequate protection legitimate interests parties involved require additional regulatory measures level regulatory measures would need secure central functions without operators since scope action would otherwise limited hand also necessary protect individuals parties assume acting interests reality prioritising financial aims interests others event feasible method protection found trust schemes could serve vitally important mediators protection interests economy interests 22 far portability enshrined article 20 gdpr concerned ethics commission recommends industry-specific codes conduct standards formats adopted given underlying purpose article 20 gdpr make straightforward change provider also allow providers access easily important evaluate carefully market impact existing portability analyse potential mechanisms prevented small number providers increase yet market power findings evaluation available expansion scope example cover provided subject real-time porting would seem premature advisable 23 certain sectors example messenger services social networks interoperability interconnectivity obligations might help reduce market entry barriers providers obligations designed asymmetric basis i. e. stringency regulation increase step company ’ market share interoperability interconnectivity obligations would also prerequisite building strengthening within europe certain basic services society 141 e 5. debates around access non -personal 5. debates around access non-personal economy play key role future competitiveness german companies growing penetration internet things iot internet services ios means collected automatically sensors potentially serve basis developing business models innovations acquiring evergreater industrial significance germany cutting edge developments far many iot/ios-related technologies concerned e. g. sensor mechanical engineering embedded also plays leading role broader field industrial production services cater sector given increasingly cut-throat nature international competition must build head start order safeguard country ’ future prosperity differentiated robust landscape diversified economic structure reputation global leader key technological segments industry 4.0 put germany perfect leverage potential associated economy basis creating future value 5.1 appropriate access macroeconomic asset ethics commission believes providing appropriate access german companies decreasing current level dependency small number oligarchs would go long way towards building market-oriented economy serves good towards strengthening sovereignty germany europe connection access narrower sense firstly relates extent required particular business model project de jure de facto basis order benefit access narrower sense however stakeholders must sufficient degree dataawareness skills necessary make also access proves disproportionately advantageous stakeholders already built largest reserves best infrastructures hand ethics commission therefore wishes stress factors referred always receive due attention discussing whether improve access keeping asisa principle awareness – skills – infrastructures – stocks – access discussions section non-personal genuinely non-personal hold enormous potential science economy society yet potential often underestimated scientific categorised non-personal include originating technical sciences e. g. engineering materials science fields physics e. g. particle accelerators biology e. g. plant animal kingdoms geology chemistry environmental weather ocean economic e. g. financial markets analysed using big methods example develop applications example non-personal hold enormous value science economy society focused support must therefore provided researchers using systematic efforts must undertaken make access easier task broad nature gdpr ’ definition “ personal ” means safely assumed substantial proportion repositories mixed nature i. e. contain non-personal could become personal time processing personal vital prerequisite certain activities fall heading economy provide benefits individuals general discussion access concentrates solely non-personal would therefore appear counter-productive appropriate approach would work towards general access arrangements superseded protection law cases personal processed meaning activities falling heading economy would need comply provisions gdpr equally forgotten gdpr already allows economic exploitation personal many circumstances addition consent example article 6 1 gdpr five additional justifying grounds article 6 1 b – f explicitly tailored economic interests needs 142 part e ata 5.2 creation necessary framework conditions 5.2.1 awareness raising skills create value presupposes operators whether belong private sector serve interest adequately well-informed relevant options risks also skills required may involve drawing technical economic ethical legal knowledge → see part section 3. certain areas german economy companies still tapped potential exists make productive flows repositories cases benefit ethics commission welcomes steps taken raise awareness build skills various stakeholders e. g. chambers industry commerce associations vocational institutions value-based approach improving skills across board however required example form initial continuing training courses objective courses must always raise awareness risks posed individuals society viewpoint protection law ethics government bodies slow recognise import implications huge volumes already generated statistical purposes example advantages risks entailed models share businesses government-to-business g2b sharing businesses share operating business-to-government b2g sharing current reticence part authorities utilise opportunities means large-scale shift mindset required modelled forerunners field e-governance scandinavian countries estonia ethics commission also recommends federal government support work area relevant institutions.5.2.2 building infrastructures needed databased economy although germany continues occupy leading field science tech companies providing vital analysis infrastructures economy primarily hail usa increasingly china means great deal – consumer enterprise – stored outside europe analysed third countries using software belonging non-european companies makes crucially important germany develop data-based economy using home-grown infrastructures ethics commission recommends federal government support following measures level initiated commission establishment expansion support centre sharing b model contracts economy c support forums consortiums tasked developing open standards legally compliant exchanges particular formats programming interfaces apis tailored exchanges increase traceability flows promotion platforms legally compliant exchanges e establishment open science cloud eosc key precursors achievement sovereignty germany include access control sensitive option carry appropriate audits critical analysis software would require manufacturers disclose source code design criteria example given ethically problematic nature analyses wherever possible carried within geographical purview german legal system 143 e 5. debates around access non -personal ethics commission expressly welcomes number initiatives federal government stakeholders aimed creating secure international spaces spearheaded germany different application domains allowing companies organisations sizes sectors industry retain sovereignty exchange securely other.the ethics commission also recommends setting ombudsman ’ office federal level provide assistance support relation negotiation problematic access agreements dispute settlement competent protection authorities consulted cases involving personal decision-making power must ultimately rest aforementioned authorities order avoid conflicting decisions establishment infrastructures federal government ’ initiatives aimed establishing infrastructures include following f efforts german foundation deutsche forschungsgemeinschaft establish national infrastructure aim implement science-driven process systematically opens repositories provides long-term storage backup accessibility across boundaries different disciplines länder g open international spaces consortium ids formerly industrial space promoted federal ministry education aim provide companies organisations taking part standardised interface platform exchanging based federal architecture concept h initiative develop comprehensive network big centres nodes distributed throughout germany part national generally accessible ecosystem aim network provide access large amount variety 24/7 basis time offer easy-to-use tools along entire value creation chain preparation analysis visualisation exploitation develop basis user feedback.in addition technical platforms interesting developments include platforms developed federal government collaboration associations view promoting coordinated standardisation practical implementation data-hungry applications form socially economically innovative future projects industry 4.0 smart service world level commission implementing similar projects e. g. futureoriented fiware project currently developing freely available toolbox open-source software components configure innovative internet services short space time big value public-private partnership organised commission big value association bdva developed interoperable data-driven ecosystem level launchpad business models using big already delivered impressive number flagship projects lastly institute innovation eit fostered emergence europe-wide technical economic ecosystem involving 180 companies institutions 144 part e ata 5.2.3 sustainable strategic economic policy far economy concerned biggest challenges facing europe include lack sustainable funding often problem projects paucity venture capital latter required make ideas already developed marketready inject capital appropriate points start-ups reach competitive one reasons usa successful field products services country ’ many “ angels ” willing invest billions high-risk projects many cases forfeit investments another trend worth noting innovative companies often bought foreign companies forced international investors move headquarters countries outside europe thinking outside “ path ” explicitly endorsed ethics commission → see part g german start-ups must given access larger pool funding better tax incentives germany continue attract brightest best remain cutting edge sectors education administration medicine characterised high level interest existence mandatory values expressed legal system professional ethics time enormous potential achieve efficiency gains digitalisation sectors global platforms yet gained stranglehold market extent areas ethics commission therefore recommends funding channelled three areas particular incentivise platforms germany reflect values also internationally scalable 27 solution endorsed preliminary drafts 2 february 3 october ali-eli principles economy see footnote 1 example.5.2.4 improved industrial property protection also perspective economy ethics commission see benefit introducing exclusive often discussed using terms “ ownership ” “ producer ” → see section 3.3.2 kind would need incorporated aligned existing provisions protection law intellectual property law rules personality trade secrets ownership storage media etc. would nothing increase already significant level complexity legal uncertainty without indication kind would necessary even particularly helpful making marketable ethics commission nevertheless believe calls made industry government bodies afford limited third-party effects contractual agreements e. g. restrictions utilisation onward transfer recipient justified legal situation currently stands thirdparty effects kind afforded extreme situations unless protection intellectual property law applies including “ sui generis ” protection databases consideration given extending scope recognition third-party effects along lines model provided article 4 4 trade secrets directive directive 2016/943 27 according approach acquisition disclosure would also considered unlawful whenever person time acquisition disclosure knew ought circumstances known obtained directly indirectly another person using disclosing unlawfully approach would interests economy also fit seamlessly existing primarily contract-focused model 145 e 5. debates around access non -personal 5.2.5 partnerships ethics commission believes cautious current legal framework would also appropriate field anti-trust law breakneck pace developments economy poses fresh challenges field law return provisions anti-trust law pose fresh challenges companies ethics commission recommends federal government pay particularly close attention opportunities risks entailed partnerships consideration given introduction mandatory confidential procedure notifying partnerships anti-trust authorities supervisory authorities protection law case personal mention also made proposals presented commission experts competition law 4.0 headings “ exchange ” “ pooling ” .5.3 access existing value creation 5.3.1 context fair efficient access plays significant role modern value creation area law suitable regulating fairly efficiently ability various stakeholders access commercial context contract law since branch legal system autonomy private entities “ private autonomy ” expressed explicitly time general presumption freely negotiated agreements – except cases market failure – achieve efficient allocation resources thus increase general level prosperity unfair inefficient contractual arrangements may arise however result imbalances power asymmetry example certain issues relating access typically underestimated negotiation process result skimmed omitted entirely given dynamic nature data-specific interests correspondingly dynamic assessment obligations → see section 2.1 often difficult parties determine exactly access regime look like order remain fair efficient entire term contract insignificant number cases accordingly found later date – contract put test real world – balance interests shifted unpredictable ways benefit one party major impact equilibrium obligations originally agreed since one parties typically stands benefit state affairs contracts often renegotiated opportunity regulate access properly efficiently 146 part e ata particularly complex value creation frequently direct contractual relationship party requesting access party effectively controls interposing link distribution chain example interests fairness efficiency however access arrangements would desirable b2b sector incursions freedom contract form obligation contract currently result almost exclusively provisions anti-trust law well small number general provisions law case essential commodities monopoly positions generally speaking however restricted small number extreme situations 5.3.2 presence contractual relationship estimation ethics commission steps initially taken view ensuring fair efficient access arrangements include raising awareness promoting skills → see section 5.2.1 practical support form model contracts provide fair distribution access infrastructures intermediaries facilitate shared ensuring protection trade secrets example → see section 5.2.2 28 commission towards common space 232 final 25 april p. 10 available /ec.europa.eu/ transparency/regdoc/rep/1/2018/en/com-2018-232-f1-en-main-part-1.pdf .in cases contractual relationship already exists principles fair access enforced primarily contract interpretation including way gap-filling courts example assuming existence appropriate contractual ancillary obligations standard contract terms control pursuant section 307 civil code “ fairness test ” however one problems inherent substantive fairness tests virtual absence default provisions benchmark tests specific abusive contractual practices could therefore spelt explicitly blacklisted contract terms → see section 3.2.3 corresponding recommendations b2c contracts significant changes occur since conclusion contract may possible party invoke provisions frustration contract section 313 civil code connection ethics commission wishes reiterate general basic principles governing businessto-government b2g sharing formulated commission communication april entitled “ towards common space ” .28 basic principles provide following transparency regarding access purposes using b recognition several parties contributed shared value creation c respect ’ commercial interests undistorted competition e minimised lock-in particularly regard repositories potentially include personal well non-personal consideration could also given expanding principles include informational selfdetermination subjects principle “ harm ” 147 e 5. debates around access non -personal 5.3.3 absence contractual relationship contractual relationship participants value creation system despite support provided neither rules interpretation contracts substantive fairness tests standard contract terms apply also impossible rely frustration view ethics commission however simple fact party requesting access contributed generation means special legal relationship exists party party effectively controls → see section 2.1 true relationship exists within value creation system primarily shaped contracts special legal relationship may give rise certain duties fiduciary nature including duty enter negotiations fair efficient access arrangements future legal framework make explicit reference fact ethics commission therefore recommends amending section 311 civil code include subparagraph mentioning special relationship exists participants value creation system e. g. suppliers manufacturers brokers end users would entail certain relevant duties including regard enormous significance general legal economic relations means justified grounds inserting subparagraph law rather subsuming relations general heading “ similar business contacts ” would neither constitute separate legal basis processing personal would restrict protection law way 29 discussion personal see louisa specht datenrechte – eine rechts- und sozialwissenschaftliche analyse im vergleich deutschland – usa teil 1 rechtsvergleichende analyse des zivilrechtlichen umgangs mit daten den rechtsordnungen deutschlands und der usa abidagutachten analysis perspective legal social sciences based comparison germany usa part 1 comparative law analysis governance civil law within framework german us legal pp 89 et seqq available /www.abida.de/sites/default/files/abida_gutachten_datenrechte.pdf discussion non-personal see ali-eli principles economy footnote 1 .beyond consideration could given introducing data-specific rules law obligations based principles referred → section 2 aimed judicial “ gap-filling ” benchmark carrying substantive fairness tests standard contract terms.29 particular provisions contracts kind might define conditions parties entitled access and/or request desistance access and/or request rectification however ethics commission also concerned rules specifically spelt law albeit default rules might give rise additional disputes 5.3.4 sector-specific access need identified extensive access within existing value creation priority given sector-specific solutions ethics commission therefore recommends federal government pay greater attention access issues adopting and/or revising sector-specific regulations 148 part e ata 5.4 open sector 5.4.1 preliminary considerations recently revised directive 2019/1024 open re-use sector psi directive national level german reuse act informationsweiterverwendungsgesetz iwg german e-government act e-government-gesetz egovg additional special acts provide sound legislative basis disclosure public-sector basis ogd concepts premise underlying concept open government citizens companies already subsidised generation taxes pay therefore allowed share associated benefits rather incurring double financial burden making public-sector available reuse private sector also benefits economy since open government often hold enormous potential private-sector value creation companies develop innovative products services helping increase general level prosperity process looking beyond economy access government also vitally important democracy open debate involving society since increases administrative transparency facilitates participation allows oversight fact-based discussions open government also many different shapes forms social initiatives innovations social ecological purposes example basic principle therefore ethics commission supports open charter adopted g8 summit defines following central principles governance administrative open default expectation administrative made without compromising privacy b quality quantity high-quality timely fully described open c usable anyone much possible many open formats possible improved governance open transparency sharing expertise regarding collection standards publication procedures e innovation user consultations support future generations creative minds ethically speaking government decided provide commercial operators free-of-charge access instead selling profit otherwise exploiting economic purposes decision would need justified approximated basis corresponding increases prosperity macrosocial level ethics commission also wishes draw attention degree tension calls privacy default one hand open default – broader sense – debate protection debate open government personal made legally compliant manner basis open-data concepts guarantee security mechanisms put place protect informational self-determination form explicit implicit restrictions reuse form technical organisational protection measures continue applied general provisions protection law concerning reuse also issue furthermore since article 30 gdpr requires “ categories recipients ” documented government bodies almost never monitor compliance “ appropriate safeguards ” required pursuant article 89 gdpr disclosure might point become personal regarded potentially high-risk measure subjects 149 e 5. debates around access non -personal applying ogd concepts area informational self-determination protected fundamental must always weighed carefully public-good interests pursued ogd banner freedom also protected fundamental freedom ogd recipients exercise trade profession ethics commission submits cases doubt priority given state ’ duty protection compliance duty important individuals may able decide freely entrust government bodies may particularly apt believe government bodies refrain forwarding personal third parties 5.4.2 legal framework infrastructures ethics commission welcomes federal government ’ national action plan implement g8 open charter efforts part federal government governments länder include ogd concepts digitalising administrations recommends federal government take action ensure across-theboard implementation obligation publish structured unprocessed open default allow without limitations principle free charge already applies direct federal administration section 12a 1 e-government act given aforementioned tension open government protection obligations imposed section 12a e-government act apply relation certain types particular undergone effective anonymisation procedures .the ethics commission welcomes legislator ’ attempts change governance culture within administration acknowledges task made significantly challenging highly fragmented nature current legal situation often difficult – authorities potential ogd users – forge path tangled regulatory thicket made different legal regimes set general specialised rules access reuse e-governance federal government land level complicating factor interplay regulations protection law intellectual property particular copyright law often fiendishly complex practice connection ethics commission recommends merging synchronising various legal bases exist germany well clarifying demarcation lines various legal arrangements another obstacle stands way culture change needs take place fact currently impossible verify reliably whether authorities fact complying provision obligations already force example section 12a 1 e-government act imposes obligation direct federal administrative authorities provide access explicitly states parties requesting access enforceable made publicly available companies wish access therefore deprived effective avenues forcing authorities comply statutory obligation making open default view ethics commission introduction request publication might encourage proactive approach provision open part administrative authorities within limits placed obligation e-government act reuse act quality standards must achieved respect provided government bodies another question open current legal situation particular e-government act states obligation provide access met handing unprocessed reused easily manner complies ogd objectives high level quality guaranteed 150 part e ata aside legal framework establishment expansion infrastructure framework e. g. open government portals govdata also essential particularly local level e. g. form municipal platforms applies investments appropriate quality assurance tools 5.4.3 state ’ duty protection keeping mind state ’ duty protect entrusted appropriate precautions must taken ensure central interests private individuals e. g. relating personal operating trade secrets sensitive confidential relating procurement procedures given comprehensive level protection key interests e. g. security interests interests relating national sovereignty ethical premise underpinning ogd concept – citizens companies already paid tax contributions – places certain constraints reuse particular care must taken ensure private sector develop services products may ultimately restrict freedom citizens businesses and/or available unfair conditions.the ethics commission therefore recommends federal government make opportunity afforded article 8 recast psi directive developing model conditions standard licences including restricted-use agreements conditions transfer third parties alternatively lobby conditions introduced level may even advisable make model conditions mandatory least sector-specific basis based number key considerations including following pursuant article 8 1 psi directive conditions must objective proportionate nondiscriminatory justified grounds interest objective shall unnecessarily restrict possibilities re-use shall restrict competition b rules imposed companies contain clearly defined safeguards affected third-parties mechanisms allow compliance rules verified c intellectual property developed using must disallow activities carried government bodies fulfilment remit make activities subject payment licence fee product service developed using offered government bodies preferential conditions e companies large market share subject reciprocal obligation make generated operations available identical conditions f business activities take place minimum product service processes take place 151 e 5. debates around access non -personal basic principle compliance agreed safeguards restrictions longer reliably verified transferred copies sent recipient stored infrastructure controlled latter given duty incumbent upon government bodies protect may harm third parties even harm would possible de-anonymisation linking sets special consideration must given model government bodies allow supervised access supervised processing infrastructures control costs incurred connection passed companies seeking access 5.5 open private sector 5.5.1 platforms operating generated companies levels german economy course everyday business enormously valuable innovation particularly combined generated participants value creation chain german economy already established sector-specific platforms express purpose linking different types examples different platform models include 1 merger several different companies gmbh limited liability company 2 in-house operation single company involvement partners 3 proprietary platform operated service third parties various sectors economy increasingly coming around idea shared platforms also common regulatory approaches use.the ethics commission believes reasonable assume within value creation continue organised industrial players sector-specific basis market entrants start-ups continue find opportunities innovate within landscape since market participants stand benefit working together trailblazing start-ups develop disruptive innovations sharing end trend companies club together establish platforms modelled along various lines welcomed allows build industrial know-how already exists europe fosters higher-quality including higher standards protection security ethics commission proposes federal government lend support emergence increasing number private-sector platforms view achieving necessary market effects allowing german businesses harness shared strength compete international stage 5.5.2 additional incentives voluntary sharing already large number business models based private providers voluntarily allowing access example 14 example 14 one case point geoinformation industries take basic geodata cases official sources enrich allowing users access specialist geodata wide range purposes examples include map services openstreetmap google maps feature purely topographical administrative also wide range interesting details also tailored offerings weather forecasts traffic predictions 152 part e ata ethics commission recommends voluntary access arrangements kind supported addition practical support measures recommended → section 5.2 consideration therefore given additional incentives voluntary sharing example transfers releases open access strategies favourably viewed ●under tax legislation ●under procurement law ●when making grant awards either inside outside sector ●when carrying authorisation procedures voluntary sharing transfers releases open access strategies however envisaged fields referred risk infringing confidentiality requirements procurement law operating trade secrets provisions protection law result 5.5.3 statutory access way contrast debate voluntary sharing main idea underpinning discussion statutory access society “ get something back ” large repositories many members society helped build case social networks example viewed conjunction fundamental value social solidarity public-good interests may relevant specific cases concept could serve basis granting extensive respect access disclosure obligation part private individuals.30 30 details see viktor mayer-schönberger thomas ramge das english title reinventing capitalism age big pp 195 et seqq 31 frand fair reasonable non-discriminatory 32 “ three-factor test ” features several international agreements basis assessing whether exemption i. e. limitation copyright represents acceptable encroachment copyright holder ’ according test exemptions kind subject three conditions may apply certain special cases ii may conflict normal exploitation iii may unreasonably prejudice legitimate interests holder calls increasingly made test include iv mandatory consideration third-party interests general interests.one potential measure often discussed context improving general access privately held repositories introduction general portability non-personal modelled along lines article 20 gdpr would mean business supplied raw controller would request controller make available business commonly machine-readable format ask controller forward directly third party reasons essentially similar cited arguments extension scope article 20 gdpr → section 4.4.2 ethics commission recommends federal government initially adopt “ wait-and-see ” approach developments relating interpretation article 20 gdpr complexity issue exacerbated yet fact issue proper allocation portability i. e. equivalent “ subject ” regard nonpersonal would raise head range measures ultimately synonymous statutory access also discussed view improving general access privately held repositories potential options respect include statutory obligation publish reports containing internal analytics access private individuals e. g. mandatory licensing complies frand31 principles and/or incorporates three-factor four-factor test copyright law32 disclosure general open access based either general model market-share model ethics commission believes least following factors taken account initial examination options 153 e 5. debates around access non -personal need protect personal operating trade secrets access may given may disclosed b need ensure encroachment fundamental private entities affected access disclosure obligation proportionate relates particular freedom exercise trade profession c need avoid negative impacts competition resulting access disclosure example owing strategic competitors may obliged disclose return need ensure incentives still exist invest business models economy e need protect strategic interests german companies face global competition particular consideration must given whether companies would still able compete effectively international stage forced provide access repositories giants – already stand head shoulders companies terms proficiency infrastructures particular volumes hold – exploit open-door policy regard ethics commission recommends preference given sector-specific approach far spatial concerned inspire directive provisions transposing national law already set sectorspecific access rules rules apply government bodies however one first privateenterprise applications sector-specific access found payment services industry ethics commission proposes steps 33 jacques crémer yves-alexandre de montjoye heike schweitzer competition policy era special advisers ’ report commission pp 87 et seqq available /ec.europa.eu/competition/publications/reports/kd0419345enn.pdf 34 competition framework economy report commission ‘ competition law 4.0 ’ september available /www wettbewerbsrecht-40.de/kw40/redaktion/de/downloads/a-new-competition-framework-for-the-digital-economy_.pdf __blob=publicationfile v=3 .should taken identify level demand implementation options number selected industries example media mobility energy sectors 5.5.4 role competition law although framework competition law currently place contains almost provisions relating general thrust also applies economy example essential facilities doctrine efd slightly modified form necessary market-dominant company holds exclusive control resource e. g. network/infrastructure crucially important competition neighbouring market aftermarket doctrine relates cases lock-in effects mean consumers primary product unable exercise full freedom choose secondary market e. g. market repairs/spare parts third-party provider secondary market kind faces anti-competitive barriers.33 yet uncertain legal situation stringent requirements apply amount time money involved relevant procedures means supervisory efforts prevent abuse currently regarded fix-all solution access problems applicable provisions competition law either individually entirety could however act central building framework economic law one crucial components range solutions access problems findings commission experts competition law 4.0 taken account respect.34 154 part e ata 5.6 access public-sector b2g public-interest purposes thought given whether controllers subject obligation grant access specific subsets order allow either public-sector bodies certain publicgood purposes scope obligation access belonging private entities obligations disclose might particularly relevant sector easier access might lead general advances science provided access arrangements designed appropriately take due account subjects ’ corresponding access private-sector might also make easier ngos media similar institutions deliver social remit thereby helping protect democratic polity particularly priority must also given times averting risks e. g. issuing storm warnings view ethics commission preference given sector-specific approach tailors design access disclosure obligations specific requirements constitutional law come play one hand practical circumstances characterise relevant sphere activity health sector mobility sector energy sector regarded particular priorities action respect ethics commission also calls broad-based societywide debate precursor decisions general obligations provide access e. g. connection projects serve good ethics commission wishes reiterate basic principles governing business-to-government b2g sharing set commission communication 25 april entitled “ towards common space ” :35 35 commission towards common space 232 final 25 april pp 13 et seq available /ec.europa.eu/ transparency/regdoc/rep/1/2018/en/com-2018-232-f1-en-main-part-1.pdf .a proportionality i. e. justified demonstrable interest proportionate terms details relevance protection b purpose limitation i. e. clearly limited one several purposes assurances obtained unrelated administrative judicial procedures c “ harm ” i. e. respect legitimate interests subjects ’ informational selfdetermination trade secrets commercially sensitive exploitation interests acknowledgement interest goal agreeing conditions reuse preferential treatment government bodies non-discriminatory conditions government bodies reduction overall burden citizens companies e quality management obligation offer reasonable proportionate support help assess quality stated purposes general obligation improve quality f transparency societal participation respect parties agreement objectives insights best practices basic principles may serve good starting point drafting provisions freely negotiated contracts exchanges also designing extensive sector-specific statutory measures improve access 155 summary important recommendations action debates around access non-personal 24 access companies appropriate non- personal appropriate quality key factor growth economy order benefit enhanced access however stakeholders must sufficient degree data-awareness skills necessary make also access proves disproportionately advantageous stakeholders already built largest reserves best infrastructures hand ethics commission therefore wishes stress factors referred always receive due attention discussing whether improve access keeping asisa principle awareness – skills – infrastructures – stocks – access 25 ethics commission therefore supports efforts already initiated level promote improve infrastructures broadest sense term e. g. platforms standards application programming interfaces elements model contracts support centre recommends federal government efforts continue matched corresponding efforts national level would also advisable set ombudsman ’ office federal level provide assistance support relation negotiation access agreements dispute settlement 26 ethics commission ascribes enormous importance holistically conceived sustainable strategic economic policy outlines effective methods preventing exodus innovative companies acquisition thirdcountry companies also excessive dependence third-country infrastructures e. g. server capacities balance must struck context muchneeded international cooperation networking one hand resolute assumption responsibility sustainable security prosperity europe backdrop ever-evolving global power dynamic 27 also perspective boosting economy ethics commission see benefit introducing exclusive “ ownership ” “ producer ” instead recommends affording limited third-party effects contractual agreements e. g. restrictions utilisation onward transfer recipient third-party effects could modelled regime protection trade secrets ethics commission also recommends adoption legislative solutions enabling companies cooperate example using trust schemes without running afoul anti-trust law “ partnerships ” 156 part e ata 28 accumulated existing value creation e. g. production distribution chains often enormous commercial significance inside outside value creation system many cases however provisions access appear contractual agreements concluded within value creation system unfair and/or inefficient lacking entirely certain cases contractual agreement efforts must therefore made raise awareness among businesses sectors far outside commonly perceived “ economy ” provide practical guidance support e. g. model contracts 29 ethics commission furthermore recommends cautious adaptations current legislative framework first stage process make explicit reference section 311 german civil code bürgerliches gesetzbuch bgb special relationship exists party contributed generation value creation system controller clarifying parties may certain quasi-contractual duties fiduciary nature duties normally include duty enter negotiations fair efficient access arrangements consideration also given whether additional steps taken could range blacklisting particular contract terms also b2b transactions formulating default provisions contracts introducing sector- specific access 30 ethics commission believes open government ogd concepts hold enormous potential recommends concepts built promoted also recommends series measures promote shift mindset among authorities something yet fully taken place make easier practice share basis ogd concepts measures include establishment relevant infrastructures e. g. platforms also harmonisation improvement existing legal framework currently fragmented sometimes inconsistent 31 nevertheless ethics commission identifies degree tension efforts promote ogd relying principles “ open default ” “ open purposes ” efforts enhance protection protection trade secrets legally enshrined concepts “ privacy default ” ethics commission submits cases doubt priority given duty protecting individuals companies entrusted state often without given choice matter e. g. tax state must deliver duty implementing range different measures may include technical well legal safeguards misuse 157 e 5. debates around access non -personal 32 particular would beneficial develop standard licences model terms conditions public- sector sharing arrangements make mandatory least sector-specific basis standard licenses model terms conditions include clearly defined safeguards third parties affected access arrangement provision also made way ultimately harms interests also still greater accumulation market power part big players would likely undermine competition taxpayer pay twice 33 regards open-data concepts private sector priority given promoting supporting voluntary data-sharing arrangements consideration must given improvement infrastructures e. g. platforms also broad range potential incentives might include certain privileges context tax breaks procurement funding programmes licensing procedures statutory access corresponding obligations grant access considered fall-back options measures fail deliver desired outcomes 34 generally speaking ethics commission believes cautious approach taken introduction statutory access ideally developed sector-by-sector basis sectors level demand analysed include media mobility energy sectors case statutory access even disclosure obligation introduced full impact assessment needs carried examining weighing possible implications include implications protection protection trade secrets investment decisions distribution market power well strategic interests german companies compared companies third countries 35 ethics commission recommends considering enhanced obligations private enterprises grant access interest public-sector purposes business-to-government b2g cautious sector-specific approach however recommended respect well part f algorithmic 160 part f lgorithmic 1. characteristics algorithmic numerous products applications days voice assistants automated lending “ autonomous ” driverless cars based less “ smart ” algorithms due many different forms types technical take seemed advisable ethics commission base considerations general concept “ algorithmic ” → see part c section 2.2.5 key questions presented federal government regarding topics “ algorithmic prognosis decision-making processes ” well “ ” therefore discussed together questions concerning algorithmic however following distinctions particular must taken account part ethical legal assessment individual algorithmic ●from technical perspective different algorithmic different characteristics spectrum ranges operate completely deterministic basis machine develop action plans independently order achieve goal specified operator algorithmic system ●where algorithmic social informatics ethically legally relevant processes established different system levels i. e. level pool algorithm technical sense level individuals involved implementation assessment correction system ●the purpose consequences using algorithmic vary considerably algorithmic support replace decision-making prognoses often direct impact individuals ’ interests examples include automated lending automated administrative acts however algorithmic also link decision-making indirectly established case example various processes constitute “ autonomous ” driving predictive maintenance mechanical engineering ●algorithmic affect different ethical legal principles depending context externally discernible “ action ” “ autonomous ” cyber-physical example typically raises questions key aspect example debate surrounding robotics healthcare principles human-centred design essential assessment algorithmic “ physically embodied ” similar way conversely often system ’ externally invisible method making “ decision ” attention discussions may example centre system ’ transparency principle final decision made accordance article 22 gdpr example automated credit checks however distinction “ action ” -oriented “ decision ” -oriented perspectives becomes upon closer inspection every “ action ” system point preceded “ decision ” example construction system every “ decision ” impact another system component including base “ action ” 161 f 1. characteristics algorithmic ethics commission believes distinctions made particular algorithmic closely involved decision-making processes algorithm make decision ethically substantial sense since value-based preferences accord three different levels involvement algorithmic decision-making distinguished based specific distribution tasks humans machine ●algorithm- based decisions decisions based either whole part obtained using algorithmic calculations examples include clinical decision support provide doctor treatment recommendations using patient electronic medical records based assessment scientific literature taking recommendation consideration doctor makes decision together patient treatment option ultimately selected algorithm-based decisions nevertheless subtly yet significantly influence decisions example algorithmic system collates humans/objects/procedures contain value judgment user may necessarily aware ●algorithm- driven decisions decisions shaped outputs algorithmic way ’ decision-making abilities capacity self-determination effectively restricted particular decision made within algorithmically determined prescribed paths one example industry 4.0 application whereby part human-machine interaction robotic system provides involved production process limited room manoeuvre ●algorithm- determined hence fully automated decisions prima facie made independently fact outputs algorithmic system trigger consequences automatically provision made explicit decision examples applications range price differentiations e-commerce fully automated administrative acts known autonomous weapons decisions nevertheless involved sense must decided algorithmic system purpose way example 1 differences illustrated algorithmic system process selecting candidates job algorithmic system simply collates individual candidates employer question basis employer make decisions constitutes algorithm-based decision-making process system lead algorithm-driven decisions provided employer contains evaluation individual candidates example ranking could significantly influence likelihood individual candidates selected actual restriction employer ’ ability make decisions becomes even apparent system already screens candidates advance meaning employer longer even sees applications case algorithm-determined selection process notification regarding acceptance rejection application would automatically provided algorithmic system without ever checking selection 162 part f algorithmic classifying algorithmic system one three types often difficult hybrids possible within complex software architecture level determination humans point also different depending way system works example decision-making process algorithmic system filters individual candidates advance rejects algorithm-determined point view candidates filtered algorithm-driven remaining candidates.there overlaps practical operation account known automation bias default effects even case algorithm-based decisions humans full decision-making authority may tend simply go algorithmic system ’ recommendation without carrying sufficiently critical check otherwise would feel uncomfortable need decision would get impression risk blamed wrong decision would increase nevertheless fundamental distinction relevant assigning responsibility risk assessment therefore also regulation.figure 7 characteristics algorithmic design implementation adaptationalgorithm-determined algorithm-driven algorithm-based decision 163 f 2. general standards algorithmic 2. general standards algorithmic general ethical legal principles primarily dignity → see part b section 3 constitute benchmark design algorithmic terms principle prospective responsibility intentional unintentional effects users individuals affected algorithmic system must taken consideration part assessment specific algorithmic also necessary think plan social consequences depending intended purpose context especially regard network effects effects effects scope consequences range positive effects social innovations sometimes subtle negative effects example diversity culture social debate essential condition functioning democracy basis ethics commission believes following key requirements design algorithmic set terms governance perspective taken must met interplay especially developers companies users state bodies 2.1 human-centred design centre requirement strive algorithmic human-centred value-oriented design takes fundamental freedoms consideration ethics commission believes human-centred approach must permeate entire design process must ensured means wide range different measures may also particular involve nclusion participation algorithmic systems.human-centred design requires particular taking account changes self-perception self-design resulting individual ’ confrontation algorithmic gains losses expertise using effects people ’ lifestyles formation opinions well physical well-being must taken consideration early system stage attention also paid emotional state affected individuals may differ directions depending whether humans conventional algorithmic significant individual affected decision also user consideration given example fact direct interpersonal interaction fulfils variety functions go far beyond “ good decision-making ” example 2 medical diagnoses supported algorithmic accuracy diagnosis identified first foremost intended purpose however need care contact consultations concerning treatment corresponding significance success treatment strong must disregarded need doctors able contribute medical experience conversely certain situations example case embarrassing symptoms may find comfortable confide primarily another person 164 part f lgorithmic functions include example satisfaction basic need communication feeling principle able assess person ’ line thinking reactions understood person opportunity convince person one ’ point view well certain control effect arising fact directly confronted reaction individual affected decision example 3 emotional aspects also play major role algorithmic human-machine interaction example system intrinsically intended support employees may perceived employees invasive patronising since system analyses employees ’ behaviour takes certain tasks hands actually come enjoy makes think performance inferior “ robotic colleague ” well-being individuals affected including example robotics nursing central guiding value absolutely must taken consideration part ethical approach design important note well-being extremely subjective change depending context time therefore needs constantly reassessed 2.2 compatibility core societal values depending area application impacts algorithmic may relevant society whole example may affect democratic process citizen-centred state action competition future work also sovereignty germany europe.example 4 smart providers able build business model large amounts privileged starting since many applications algorithmic depend amounts analysed likely correlations findings generated taken together network effects effects effects scope typical platform markets market power companies begins strengthen monopolies formed certain threshold reached ultimately enables companies prevent players entering market interfere market-regulating forces competition depending area application companies control social opinionforming processes market behaviour order counteract create framework conditions fair competition competition law control mechanisms must readjusted necessary subsequently tightened ethics commission view supra- individual consequences often handled state bodies legislative measures alone instead need taken consideration phases design algorithmic extent developers companies users shared social responsibility particular corresponding consequences seem likely example case algorithmic affect communication people relevant democracy necessary already design process thoroughly assess purposes unintended indirect consequences system question examine extent system affect democracy fundamental secondary law basic principles rule law far possible culture “ incorporating ” basic principles democracy rule law fundamental system architecture established process designing 165 f 2. general standards algorithmic many aspects interplay society admittedly still unclear ethics commission believes therefore necessary shed light social impacts algorithmic develop corresponding strategies limit negative effects 2.3 sustainability design algorithmic assessment personal social effects algorithmic must also global nature limited regard time reason deciding design algorithmic sustainability skills retention particular must also taken consideration important remaining control functions e. g. “ human-in-the-loop ” principle failure algorithmic exceptional circumstances e. g. event disaster cyber attacks ensuring innovative prowess future generations e. g. technologies first foremost question basic advanced training well education sense lifelong ensuring future generations also necessary general skills limiting training user ’ perspective teaching developing skills also promotes social sustainability social framework conditions example institutions procedures must organised way ensure promotion participatory inclusive design algorithmic serve interest sustainable also includes ecological dimension irrespective positive contribution algorithmic make environmental protection key ethical requirement reducing need electricity certain resources “ rare earths ” using efficiently.economic sustainability requires perspective looks beyond exclusively short-term economic profits also takes long-term effects consideration short-term commercial success long-term disastrous consequences demonstrated global financial crisis several years ago limit freedom economic activity attention responsibility associated economic activity within context social market economy principle prospective responsibility well considerations fairness solidarity must regard sustainability specifically taken consideration design algorithmic case handling risk assessment crucial importance ecological economic social sustainability design algorithmic 2.4 high level quality performance algorithmic must work well reliably order achieve goals pursued help also promote ethical aims technical legal specifications designed improve develop safeguard state art take ethical quality support replace activities deemed irrespective intrinsic value activity implementing ethical principles better previously example 5 ethically sound algorithmic healthcare sector firstly requires necessary medical quality i. e. accuracy assessment findings accuracy diagnosis probability recommended treatment successful success rate medical intervention etc must system least good view sensitive usage context ideally better conventional humans 166 part f lgorithmic quality performance improved wide range different measures include example appropriate risk models inclusive participatory possible standards systemic management control approaches process design aimed continuous improvement entire system role humans part algorithmic system understood social-informatic ensemble → see section 1 must always taken consideration context number algorithmic still rely input critical experts perform optimally quality-oriented system design therefore also includes mechanisms help enhance capabilities prevent counteract reduction skills critical ability readiness reflect example connection automation bias examples productive interaction humans machines also designed ensure skill retention found algorithm-supported diagnostic imaging healthcare sector.2.5 guarantee robustness security algorithmic must robust secure otherwise legitimate goals pursue achieved achieved expense potential harm ethically legally protected interests ethical perspective said robust secure system design appropriate system usage therefore affect respective purposes system need protect system result robustness security requirements identical specific requirements differ based specific need protection usage context example 6 robust secure control pose immediate threat people environment example control emission pollutants industrial plants control robots steer autonomous driverless cars traffic failure could even cause harm important legally protected life limb order prevent processes put place define current state art legal rules regulations enacted make mandatory follow state art measures implemented guarantee effective enforcement standards 167 f 2. general standards algorithmic robust secure system design involves securing system external threats e. g. means encryption anonymisation etc also protecting humans environment negative influences system particular systematic risk management approach e. g. basis risk assessment must also incorporate phases processing technical organisational components risks arise technical design also result errors caused decisions taken using algorithmic algorithmic way incorporated organisation ’ management system also required checks ensures effectiveness measures view changing conditions example newly discovered risks 2.6 minimising bias discrimination prerequisite fair decisions key aim regulating algorithmic ensure decision-making patterns upon algorithmic based systematic distortions bias leading discriminatory unfair decisions first noted biased discriminatory unfair decisions also found conventional humans conversely prejudiced decisions individual humans algorithmic however bear danger using system large broad impact individual decision-makers could never cause mind discussion surrounding bias discrimination algorithmic view ethics commission also seen opportunity detect existing problems existing decision-making contexts general achieve better decision-making processes.example 7 algorithmic system detect skin cancer trained predominantly patients white skin probability correctly detecting skin cancer therefore significantly higher case patients white skin case patients different coloured skin medical device system would permitted patients white skin effect would admittedly also noted dermatologist training practised clinical professional exclusively specific cultural environment ultimately cases steps would need taken ensure patients irrespective skin colour receive proper medical care even cases direct intention discriminate developing algorithmic discriminatory decisions may still made i. e. decisions systematically put certain groups unfair disadvantage particular case machine problem rather learn models using available resulting predictions recommendations extrapolate past future whereby existing social injustices obscured incorporation seemingly neutral potentially amplified example 8 algorithmic system assess applications managerial trained managers proven relevant company past decades since predominantly male managers employed past decades system trained set consistently assesses male candidates better equally qualified female candidates 168 part f lgorithmic keyword bias covers range different types systematic distortions range different causes case decision-makers cognitive bias social preconceptions prejudices stereotypes negatively affect decision-making process case algorithmic bias refer technical reproduction social preconceptions prejudices stereotypes reproduction take place various points primarily within context machine often insufficient level representation low number cases social group training leads distortions whereby specific characteristics group sufficiently recognised process therefore taken account addition training technical methodological decisions e. g. regarding target variables labels also lead discriminatory models therefore unfair decisions lastly problems may arise actively practice example algorithmic changing social framework conditions unforeseen usage contexts algorithmic directly categories legally explicitly recognized highly sensitive gender origin particularly critical point view discrimination direct sensitive may depending area application important correct processing also often permissible within legal limits example 9 many diagnosing diseases know patient ’ gender age take account sensitive characteristics may also within context business decision implementing business strategies example business expanding specific age group occupational group region characteristics define customer segment example simplified acceptance criteria apply.the indirectly codes sensitive categories however also problematic example 10 household income creditworthiness assessments germany average income varies genders result algorithmic system uses household income may incorrectly assess creditworthiness men women involved terms distribution fully preventing discrimination even terms legally recognised categories gender origin difficult within context algorithmic furthermore algorithmic lead totally groups thrown together based coindicing characteristics excluded socially protected due certain classification system without cause confronted negative consequences light involved system must made aware complex conditional discriminatory effects prevent counteract far possible → see section 4.2.4 however technical measures designed minimise discrimination limitations even continuous improvement processes partly different technical fairness targets achieved simultaneously criteria nondiscrimination fairness appropriate context technical social political question accordingly decisions must entrusted developers alone instead part future regulation algorithmic included operational obligations controllers prerequisite criteria must decided specifically based context well democratically 169 f 2. general standards algorithmic algorithmic difficult analyse precisely order able detect prevent discrimination controllers oversight bodies must opportunity gain idea undesirable discrimination effects occur within algorithmic system within context productive deployment effects identified processes risk assessments output analyses tension specifications limit collection storage discriminatory characteristics concern retain possibility detect discriminatory effects able prove nondiscrimination different requirements must balanced case-by-case basis may influence tests different phases system lifecycle standard collation potentially discriminatory therefore sensitive sole purpose proving result discrimination taking place would justified greater efforts needed produce practical concordance anti-discrimination law protection law .2.7 transparent explainable comprehensible order able carry reliable ethical legal assessment algorithmic system essential enough available scope functionality pool analysis truly transparent system examined determine whether pursuing legitimate purpose transparency principle key functions depending type addressee possible transparency obligations regard sufficient transparency must created sufficient available socio-political discourse algorithmic supervisory authorities oversight bodies must able decide whether legal technical specifications met algorithmic individual citizens must able take informed confident decisions regarding algorithmic event negative effects freedoms able assess whether extent wish exercise consequence ethical principle self-determination view increasing complexity demand transparency practice confronted fact even experts hardly able go individual components system fully look interact comprehend everything within reasonable amount time particular case individual machine methods difficult today ’ state-of-the-art science state input led specific output system also fact even technically simple algorithmic often incorporated complex social informatics ecosystems i. e. worksharing processes numerous manufacturers operators involved 170 part f lgorithmic example 11 visual personalised online advert result complex processes advert delivered paid basis behaviour-based analysis segmentation particular analytics services deployed site owners across websites incorporating corresponding program code javascript code tracking components also change example manufacturers provide versions adaptive and/or selflearning legal aspects also limit certain forms disclosure via algorithmic source codes hardware designs often protected trade secrets operators also often legitimate interest preventing manipulated algorithmic process personal protection law also limit interest affected citizens however transparency requirement regarding system concerns disclosure source code contain personal protection law stand way disclosure however ever-present complexity refute goal designing algorithmic transparent lack transparency like aforementioned legal grounds aspects must nevertheless taken account drafting transparency obligations must based legally actually possible principle transparency also requires continuously developing make disclosure easier example opensource software open hardware developing approaches reduce complexity also required banner “ explainable ” researchers working increasing success producing meaningful findings internal processes algorithmic systems.the demand transparency must always take different levels expertise parties potentially interested transparency account example disclosure computer code supervisory authorities carrying necessary checks may make much easier understand system conversely laypersons often need clearly comprehensibly prepared system ’ basic characteristics enables carry risk assessment suitable everyday purposes time interest seldom limited system “ ” order prevent negative decisions future explanation rather also required decision specifically concerning came factors weighting specific drafting specifications transparency explainability based affected individuals ’ level understanding always comprehensible sense rules transparency explainability safeguard citizens ’ capacity act self-determination 171 f 2. general standards algorithmic 2.8 accountability structures control implies obligation accountable power opportunity control algorithmic must also accompanied willingness answer one ’ actions i. e. liable necessary complexity algorithmic practice make difficult assign responsibility hardware software manufacturers providers algorithm developers operators individual components clients users either organisation individual employees contribute system components often change without knowledge control user example result important updates required security purposes involved often also located different parts world efforts required levels order prevent diffusion responsibility establish accountability structures starting technical design legal specifications example form concept protection law “ joint control ” article 26 gdpr .2.9 result responsibility-guided consideration assessing ethical aspects algorithmic practice extremely complex due large number factors need taken account well fact specific area application different individuals may put “ better ” “ worse ” said social consequences sustainability aspects rarely unequivocally classified either “ positive ” “ negative ” however mean humans surrender judgment cases difficult weigh everything everyone required take particular care assessments decisions algorithmic applications may potentially develop phenomenally impressive performance scope questions raised concerning future mankind weighted assessments opportunities risks increasingly reach limits fundamental anthropological ethical discussions required precisely principle prospective responsibility fundamental importance regard democratic process provides ways means balancing conflicting convictions ideally supported special deliberative processes institutions society ensure inclusive participatory way possible challenges presented algorithmic addressed 172 part f lgorithmic rarely case activity algorithmic system need weighed latter ethically relevant respects achieves “ better ” result humans using conventional case however ethics commission believes algorithmic ethically commanded general ethical preference activity machines expense protection important legally protected justified view ethics commission however regard question whether machine activity preferable → see part b section 1 factors routinely need taken consideration emotional well-being people skills retention sustainable ultimately requires weighing options may go favour algorithmic system example 12 diagnostic algorithmic system specific clinical area leads 2 patients dying whereas 10 patients would die result misdiagnoses system would depending circumstances specific case ethically advisable even result minor tolerable reductions patients ’ emotional well-being occurred additional measures would taken ensure skills retention.however taking circumstances account algorithmic system expense important legally protected leads inferior result conventional humans example wrong decisions made increase efficiency convenience algorithmic must principle rejected ethical reasons however ethically defensible exceptions could made case based economic considerations would minimal impairment exceptionally high potential saving would benefit good 173 f 3. recommendation risk -adapted regulator approach 3. recommendation risk-adapted regulatory approach regulatory point view fact algorithmic need assessed differently ethical perspective depending intended purpose performance robustness security well terms impacts suggests risk-adapted regulatory approach1 required follows principle greater potential algorithmic cause harm stringent requirements far-reaching intervention means regulatory instruments risk spectrum algorithmic therefore ranges application involves low risk could lead irreversible harm individuals society causes risks example inadequate models unsuitable pool particular case selflearning inappropriate basic assumptions weighting → see sections 2.3 2.6 potential harm caused algorithmic vary nature include financial loss nonmaterial damage physical harm example individual applications cause potentially serious financial loss example lending insurance terms affect opportunities participation example discrimination hiring involve violations fundamental risks life health consumers example case robotic nurses mobility applications 1 compare particular tobias krafft katharina zweig transparenz und nachvollziehbarkeit algorithmenbasierter entscheidungsprozesse transparency traceability algorithm-based decision processes studie im auftrag des verbraucherzentrale bundesverband e. v. vzbv study commissioned federation german consumer organisations vzbv 22 january pp 18 et seqq available /www.vzbv.de/sites/default/files/downloads/2019/05/02/19-01-22_zweig_krafft_transparenz_adm-neu.pdf 2 sarah fischer thomas petersen deutschland über algorithmen weiß und denkt – ergebnisse einer repräsentativen bevölkerungsumfrage germany knows thinks algorithms – results representative population survey bertelsmann stiftung available /www.bertelsmann-stiftung.de/de/publikationen/publikation/did/was-deutschland-ueber-algorithmen-weiss-und-denkt/ .the overarching objective regulating algorithmic prevent detrimental effects individual supra-individual level particular algorithmic affect matters sensitive terms fundamental legal provisions concerning design also needed regulation strive intervene much necessary little possible order hamper innovation creativity time ensuring protection fundamental freedoms values efficient proper regulation help increase trust algorithmic perception self-learning particular controllable adds corresponding scepticism towards technology.2 ethics commission takes view primary addressees regulation manufacturers operators algorithmic due state ’ direct obligation uphold fundamental necessary differentiate however private state algorithmic → see section 7 particular regulation drawn detail given model role model character state action federal government advised exercise particular care using algorithmic state purposes 3.1 system criticality system requirements risk-adapted regulatory approach made concrete orienting towards criticality model algorithmic system system criticality based system ’ potential cause harm determined based likelihood harm occur severity harm 174 part f lgorithmic severity harm could potentially result example faulty decision depends among things significance legally protected interests affected particular example determine one ’ personal freedom expression fundamental life physical integrity well equal treatment extent potential harm resulting infringement furthermore assessment severity potential harm must take account specific sensitivity level potential harm individuals groups including non-material harm loss utility hard calculate monetary terms number individuals affected total figure potential damage harm society whole may go well beyond straightforward summation harm suffered individuals consequences using algorithmic system based area application considered terms ecological social psychological cultural economic legal dimensions general ethical values principles → see part b set standard regard assigned value likelihood harm occur also influenced following system properties factors ●the role algorithmic calculations decision- making process mere inspiration humans without claim accuracy algorithmdetermined decisions → see section 1 ●the complexity decision made simple deterministic depiction reality probabilistic appraisal reality multifactorial non-determinate prediction future reality ●the effects decision purely abstractly conceivable context action specific context action direct implementation ●the reversibility effects full reversibility irreversibility .the likelihood potential harm severity harm may also depend whether state private party taking action particularly economic contexts market power party using algorithmic system due fact state private nature action market power relevant terms obligation uphold fundamental potential harm society whole also determine possible alternative options affected affected persons depend algorithmic system example terms access markets goods services criticality increases limitation options due various different causes example network effects effects effects scope turn reflected market power lack equivalent alternatives greater system criticality stricter requirements imposed system regulatory perspective requirements formed particular corrective oversight mechanisms b specifications regarding transparency algorithmic explainability comprehensibility results c rules assignment responsibility liability within context algorithmic → see sections 4 5 8 175 f 3. recommendation risk -adapted regulator approach variety complexity dynamics algorithmic pose major challenges regulation based limited toolbox must depending system ’ criticality implement different corrective control instruments different regulatory levels order achieve objectives regulation ensure risks involved manageable spectrum possible instruments ranges forgoing special legal provisions “ soft ” incentives self-regulation giving authorities monitor requiring final decision taken banning certain intended purposes contexts using algorithmic provisions regarding transparency explainability comprehensibility results → see section 2.7 key components corrective control regime algorithmic also extent criticality system determines scope obligations provide requested comprehensibly communicated varies depending addressees system hence also intended purpose usage context ethical legal perspective crucial dealings algorithmic responsibility impacts clearly assigned decisionmakers times rules liability particular also key importance question proper organisation liability regime certain products and/or services must also addressed view criticality system → see section 8 .in terms governance perspective adopted ethics commission relevant stakeholders state companies developers must participate specifying drawing differentiated regulatory requirements ethics commission points even without special regulation algorithmic must measured general legal norms include particular civil liability law fundamentally states compensation mandatory event action infringes legally protected interests provisions existing regulation unfair competition also apply example event consumers misled well criminal law crimes committed help algorithmic examining conditions norms criticality resulting system requirements also legal significance accordance general standards algorithmic order fulfil specific functions order assess system criticality ethical assessment intended purpose therefore also crucial importance intended purpose ethically indefensible example infringes fundamental freedoms breaches free democratic basic order “ red lines ” “ limits ” – algorithmic humans example algorithmic system political manipulation fraud collusive price-fixing must seen per se ethically objectionable 176 part f lgorithmic intended purposes often multifaceted individual facets particular regarding secondary purposes may need assessed differently ethical perspective identifying intended purpose decisive assessment often sense requires difficult value judgments assessing intended purpose algorithmic complicated case products market launch phases increasingly overlap intended purpose product may also change launched market due updates deployment usage contexts complex intended purposes case media intermediaries number media intermediaries search engines essential internet age provide access online channel flood actually enable individuals internet first place extent purposes desirable unproblematic ethical terms however media intermediaries ethically problematic terms specific design provide users personalised selection leads selection displayed however since result overwhelming majority displayed displayed lower priority individual ’ spectrum perception narrowed intermediary decides programming user ’ head user sees far business models media intermediaries driven advertising case major social networks risk operators economic interest disseminating also ethically questionable even extremist promises keep users platform longer thus increasing advertising revenue due interplay sorting narrowing seen additional danger influencing user non-transparent third-party interests possibility influence non-transparently exerted example political decisionmaking process could even result political manipulation significant danger free formation opinions basic foundation democracy 177 f 3. recommendation risk-adapted regulatory approach 3.2 criticality pyramid ethics commission recommends consistently determining degree criticality algorithmic using overarching model degree criticality guide legislators society seeking suitable regulatory thresholds instruments also provide developers operators guidance assessing products finally also basic advanced training educate increase awareness amongst various stakeholders extent regard potential algorithmic cause harm ethics commission differentiates private state operators five levels criticality figure 8 criticality pyramid risk-adapted regulatory system algorithmic level 1applications zero negligible potential harmno special measuresbeginning specific regulationlevel 2applications potential harmmeasures formal substantive requirements e. g. transparency obligations publication risk assessment monitoring procedures e. g. disclosure obligations towards supervisory bodies ex-post controls audit procedures level 3applications regular significant potential harmadditional measures ex-ante approval procedureslevel 4applications serious potential harmadditional measures live interface “ always “ oversight supervisory institutionslevel 5applications untenable potential harmcomplete partial ban algorithmic system ban 178 part f lgorithmic unproblematic usage contexts normally necessary require developers clients operators go specific ethical legal oversight procedures many applications zero negligible potential harm i. e. lowest level level 1 criticality pyramid ethics commission sees need special oversight would go beyond general quality requirements apply even products without algorithmic elements example 13 algorithms drinks vending machine certain potential harm since user could example receive goods lose money however potential harm exceed threshold specific potential harm within algorithm context sufficient rely general mechanisms oblige contractual partners fulfil contractually undertaken performance obligations manufacturers produce devices function properly.in case applications potential harm i. e. level 2 criticality pyramid regulation implemented however scope necessary measures limited view low level criticality excessive burden manufacturers operators specifically avoided order excessively hinder technological social innovations market measures could offered level 2 include example ad-hoc ex-post controls example form input-output control reason suspect system malfunctioning furthermore obligation produce publish appropriate risk assessment → see section 4.1.3 addition sector-specific basis obligations disclose supervisory institutions including establishing interface supervisory institution carry input-output controls increased transparency obligations well access individuals affected → see section 4.1 details may useful codes conduct also considered would developed specifically industry approved competent supervisory authorities compliance would need tested supervisory authorities using spot checks well ad-hoc basis → see section 5.2 criticality case smart mobility applications provider smart mobility applications access pool generated using vehicle mobility exclusively predicting traffic jams level criticality classified negligible however flow traffic also controlled using smart mobility algorithms example identify route optimum route travelling b based overall usage mobility system consisting road rail water air transport determined real time using vehicle corresponding route suggested user based user ’ preference e. g. fastest/ environmentally friendly/cheapest etc route however also question whether state stipulate certain routes user consideration state-prescribed criteria view changed potential harm level criticality would higher would therefore require stricter regulation appropriate 179 f 3. recommendation risk -adapted regulator approach example 14 dynamic pricing example based criteria supply demand e-commerce however involve personalised pricing potential harm generally low still exceeding threshold relevance example concerning covert discrimination case applications regular tangible potential harm level 3 criticality pyramid specific cases addition mechanisms already required level 2 ex-ante control form licensing procedure may justified → see section 4.2.5 account fact many algorithmic highly dynamic regular review required event licence granted.example 15 price algorithms setting personalised prices i. e. setting price based criteria tailored individual customer usually estimate maximum personal willingness pay involve appreciable potential harm example concerning discrimination particularly vulnerable groups best possible undergone licensing procedure must apply applications significant potential harm level 4 applies levels 2 3. however additional oversight transparency obligations may extend way publication factors influence algorithmic calculations weighting pool algorithmic decision-making model comprehensible format required even “ always-on ” oversight via live interface provided protective measures prevent harm also necessary differentiated criticality case media intermediaries help algorithmic filtering media intermediaries process communicate relevant formation opinions relevant democratic decision-making process advertising purchase recommendations entertainment therefore represent perfect example situations algorithmic system differing potential harm case user interaction consumer goods sector particular advertising purchase recommendations depending personalisation model low high potential harm soon balanced variety must produced particular case topics relevant formation opinions account overarching interests maintaining free democratic basic order potential harm already higher outset due result regulatory requirements change simultaneously case consumption entertainment offerings depending personalisation criteria usage contexts welfare effects expected less stringent regulation must ensue 180 part f lgorithmic example 16 algorithmic example players huge market share determine creditworthiness individual consumer company must classified level 4. whether person receives loan decisive bearing person ’ fate high level system criticality also justified market concentration providers tendency lender rely judgment particular player regard system criticality criteria may ultimately worth considering complete partial ex-ante ban algorithmic system applications untenable potential harm level 5 ex-post ban may also consequence breaches applicable law non-fulfilment system requirements set specific system criticality example 17 lethal autonomous weapons often seen “ red line ” machines allowed kill people however apply basis algorithm-determined killings lethal autonomous weapons simply provide soldiers support recognising objects merely keep missile track face crosswinds ethical “ red line ” crossed classification algorithmic system criticality pyramid must necessary regularly reviewed light dynamic nature systems.3.3 regulation algorithmic enshrining horizontal requirements formed sectoral instruments algorithmic infiltrating areas personal social lives purposes algorithmic areas could potentially therefore set stone example facial recognition system developed private photos could also state investigative authorities law enforcement purposes prevent threats suggests addressing challenges posed algorithmic following example protection law form horizontal regulation i. e. legal instrument material scope covers algorithmic general applies private players alike addition considerable symbolic power another point favour horizontal regulation fact gaps protection would eliminated dangerous situations currently foreseen would covered one main arguments favour overarching regulation sets basic principles algorithmic also fact citizens would result idea expect areas legislators could complete task within reasonable period time result ethics commission recommends federal government work towards drawing horizontal basic regulation level form regulation algorithmic eu-asr addition key basic principles algorithmic developed requirements algorithmic horizontal legal instrument group together general substantive rules – informed concept system criticality – admissibility design algorithmic transparency individuals affected organisational technical safeguards supervisory institutions structures 181 f 3. recommendation risk-adapted regulatory approach figure 9 regulation algorithmic enshrining horizontal requirements specified sectoral instrumentssektor 1 specifying/supplementary rules requirementssektor 2 specifying/supplementary rules requirementssektor 3 specifying/supplementary rules requirementssektor 4 specifying/supplementary rules requirementsfederal government union regulation algorithmic eu-asr key basic principles algorithmic general substantive rules admissibility design algorithmic rules transparency organisational technical safeguards supervisory institutions structures.european union time ethics commission recommends federal government also advocate sectoral rules level outside competences within legislative administrative competences enact appropriate sectoral legal acts oriented towards system criticality fig 9 overarching eu-asr limited basic principles otherwise legislatory powers would overburdened legislators would rules detailed particular face issue deal general legal instrument wide variety almost impossible keep track highly dynamic perspective affected general legal instruments also carry risk administrative obligations also apply cases sufficient potential harm horizontal legal instrument distinguish risky less risky operational aims well potential exceptional configurations level detail reality regard points supplementary recourse sector-specific legislation would limited terms scope would therefore easier form would relieve burden supplementary sector-specific approach would also take consideration legislative administrative powers distributed accordance applicable law federal level states bundesländer additional fact regard official oversight supervisory institutions structures various reasons could question consolidating assigning “ overall task ” one single authority → see section 5.1 therefore addition eu-asr necessary enact several legal instruments specific provisions individual sectors potentially harmful situations view ethics commission combining general basic regulation sector-specific legal instruments major advantage enabling differentiation different needs protection involved individual usage contexts line basic concept behind risk-adapted regulation according regulatory requirements algorithmic determined based specific system criticality 182 part f lgorithmic even protection law sector numerous special laws supplement general provisions gdpr different sectors basic idea behind protection law case automated processing longer thing “ inconsequential ” hardly possible differentiate meaningfully personal basis worthiness protection criticality absence common basic rules nonetheless also true variety special provisions ensures increased level protection wide range areas state activity similarly according need supplementary sectoral provisions algorithmic application regulation also fall short result fact purpose usage context could change firstly change would especially complex inherently limited secondly issue could addressed regulatory perspective fact legal instruments would materially linked original purpose original usage context current functionality system intended purpose system way changes purpose context would necessary result application differentiated regulatory framework however primarily pragmatic considerations way affect requirement standardsetting bodies ensure greatest possible coherence legal instruments respective undertakings apply regulatory approaches developed i. e. particular notion system criticality subjects regulatory infrastructures processes also designed uniformly possible 183 36 ethics commission recommends adopting risk-adapted regulatory approach algorithmic principle underlying approach follows greater potential harm stringent requirements far-reaching intervention means regulatory instruments assessing potential harm sociotechnical system whole must considered words components algorithmic application including people involved phase – example training – implementation application environment evaluation adjustment measures 37 ethics commission recommends potential algorithmic harm individuals and/ society determined uniformly basis universally applicable model purpose legislator develop criteria-based assessment scheme tool determining criticality algorithmic scheme based general ethical legal principles presented ethics commission 38 among things regulatory instruments requirements apply algorithmic include corrective oversight mechanisms specifications transparency explainability comprehensibility ’ results rules allocation responsibility liability using 39 ethics commission believes useful first stage determining potential harm algorithmic distinguish five levels criticality applications fall lowest levels level 1 associated zero negligible potential harm unnecessary carry special oversight impose requirements general quality requirements apply products irrespective whether incorporate algorithmic 40 applications fall level 2 associated potential harm regulated as-needs basis regulatory instruments connection may include ex-post controls obligation produce publish appropriate risk assessment obligation disclose supervisory bodies also enhanced transparency obligations access individuals affected.summary important recommendations action risk-adapted regulatory approach 184 part f lgorithmic 41 addition introduction licensing procedures may justified applications fall level 3 associated regular significant potential harm applications fall level 4 associated serious potential harm ethics commission believes applications subject enhanced oversight transparency obligations may extend way publication factors influence algorithmic calculations weightings pool algorithmic decision-making model option “ always-on ” regulatory oversight via live interface system may also required 42 finally complete partial ban imposed applications untenable potential harm level 5 43 ethics commission believes measures proposed implemented regulation algorithmic enshrining general horizontal requirements regulation algorithmic eu-asr horizontal regulation incorporate fundamental requirements algorithmic sytems ethics commission developed particular group together general substantive rules – informed concept system criticality – admissibility design algorithmic transparency individuals affected organisational technical safeguards supervisory institutions structures horizontal instrument fleshed sectoral instruments member state level concept system criticality serving guiding framework 44 process drafting eu-asr recommended incorporate debate best demarcate respective scopes regulation gdpr number factors taken account respect firstly algorithmic may pose specific risks individuals groups even involve processing personal risks may relate assets ownership bodily integrity discrimination secondly regulatory framework introduced future horizontal regulation algorithmic may need flexible risk-adapted current protection regime 185 f 4. instruments obligations controllers subjects 4. instruments obligations controllers subjects order provide individuals groups effective protection dangers algorithmic ethics commission believes transparency requirements → see section 4.1 specifications algorithmic view effective protection substantively inappropriate decisions unfair decisions → section 4.2 advisable 4.1 transparency requirements 4.1.1 mandatory labelling “ ” key tool creating transparency mandatory labelling mandatory labelling scheme requires little detailed infringements fundamental system operators particular regard business secrets also less serious case access ethics commission believes justifies establishing labelling case critical level 2 blanket obligation system operators requestbased individuals affected due comparatively narrow scope article 22 gdpr relating decision based solely automated processing duties provide refer ethics commission believes existing labelling obligations gdpr3 insufficient particular significant impacts affected individuals arise even threshold article 22 gdpr applies algorithm-based algorithm-driven decisions i. e. situations humans taking decisions run risk accepting algorithmic proposed decisions without reflection default particular areas assessment expected following algorithmically determined prescribed paths 3 article 13 2 f article 14 2 g article 15 1 h conjunction article 22 gdpr.because ethics commission sees authenticity interpersonal communication fundamental condition trustworthy interaction within society mandatory labelling scheme always apply risk confusion machine therefore apply irrespective system criticality applies example voice assistants chatbots days sometimes hard identify labelling may case voice assistants example carried means regular reminder assistant ’ mechanical nature even ongoing communication also mechanical-sounding voice conversely ethics commission considers risk confusion therefore also need mandatory labelling scheme areas nature irrelevant recipient expects mechanical voice anyway case loudspeaker announcements railway stations 4.1.2 duties provide duties provide explanation access “ ” “ ” whilst mandatory labelling schemes require system operators ensure transparency regarding whether extent algorithmic “ ” duties provide access regularly focused detailed regarding decision-making mechanism “ ” “ ” algorithmic system 186 part f lgorithmic duties provide access regarding behaviour algorithmic way decisions made inside important perspective citizens able understand decisions review and/ reviewed individually help subjects exercise challenge decision informed basis following transparency requirements apply equally private state operators algorithmic special requirements regard transparency state covered detail section 7 4.1.2.1 duties provide access articles 13 14 15 gdpr already set duties provide access personal processed event automated decisionmaking within meaning article 22 gdpr gdpr grants subject “ meaningful ” “ logic involved ” well “ significance ” “ envisaged consequences ” processing.4 ethics commission takes view case mandatory labelling scheme → see section 4.1.1 legal concept behind norms also apply outside narrow scope article 22 1 gdpr integral part eu-asr suggested → see section 3.3 extent duty provide depend criticality system case applications negligible potential harm brief statements logic behind decisions suffice example pool general weighting certain factors regard result risk system involves extensive duties disclose essentially 4 article 13 2 f article 14 2 g article 15 1 h gdpr.the sensitive decision terms personality detailed relating individual case needed however also borne mind providing detailed regarding factors weighting could also potentially ethically questionable influence private lifestyle subject furthermore subject could also acquired undermine algorithmic system performs important function technical organisational requirements must met order able fulfil extensive duties provide must incorporated design algorithmic outset possible ensure operated lawfully corresponding necessary “ meaningful ” also provided system defining duties provide access order increase transparency algorithmic care taken ensure special technical skills knowledge required consumers whenever access expanded borne mind perspective subjects increase transparency prepared way suitable recipient 187 f 4. instruments obligations controllers subjects 4.1.2.2 duties provide explanation least certain areas complex algorithmic may appropriate addition general explanation regarding system ’ logic significance require explanation specific reasons system made recommendation decision specific explanation required decision concerns areas sensitive terms personality otherwise particular significance terms fundamental socioeconomics important cases subjects informed comprehensible relevant manner ethics commission therefore welcomes technical efforts improve explainability algorithmic particular self-learning explainable explicable encourages federal government promote projects ethics commission believes certain situations worth considering entitlement “ counterfactual explanations ” sometimes discussed literature.5 cases subjects informed factors decision-making process case negative decision would made positive difference i. e. would actually led desired outcome case application loan rejected based algorithmic system subject would example entitled learn system operator factors taken consideration system would different way application positive outcome however ethics commission points approach quickly reaches limits case complex subject would provided whole host different “ counterfactual ” scenarios order given reasonably complete picture otherwise would danger misinformation questionable steering even manipulation focusing certain aspects strategic educational reasons 5 sandra wachter brent mittelstadt chris russel harvard journal law 31 pp 841 et seqq.in view ethics commission given current state technical concept “ counterfactual explanation ” therefore suitable general component regulation algorithmic however could considered special processing situations 4.1.2.3 access directly affected persons addition ethics commission considers certain sectors individual also social interests affected significant extent advisable even individuals directly affected granted access regarding algorithmic would apply particular relevant opinion-forming major welfare effects population would first foremost worth considering journalistic purposes would also accompanied adequate protective measures affected interests system operators certain circumstances particular event state ’ significant potential harm unconditional access publication requirements also conceivable view ethics commission 188 part f lgorithmic 4.1.2.4 requirements defining duties particular consideration system operators ’ defining duties provide explanations access must always borne mind may also affect legally protected interests operators algorithmic well outputs includes notably protection business secrets interest preventing manipulation manipulative private system operators principle invoke fact define free-will decisions contractual decisions based outputs algorithmic system however release monitoring required check whether acting accordance law fundamental freedom action restricted bans discrimination particular general act equal treatment fundamental subjects third parties general provisions specific contractual provisions legal system furthermore transparency must always balanced provisions protection law relating protection personal third parties stored system ethics commission therefore believes appropriate legislators accompany transparency obligations rules initiative system operators also possibly affected third parties enable conflicting interests weighed transparency interests subjects private individuals entitled claim however view ethics commission rigid rules priority example general preference protection business secrets transparency interests appropriate matter concerned despite increase legal certainty might bring system operators third parties invoke conflicting interests meticulous checks must carried see whether interests taken account specific protective measures transparency obligation completely rejected private individuals access requirements regarding protective measures demonstration existence must devised act barrier preventing vulnerable consumers and/ citizens acquiring interests third parties must protected example means anonymisation 4.1.3 risk impact assessment impact assessment within meaning article 35 1 gdpr concerns impacts protection personal however include comprehensive risk analysis algorithmic system case algorithmic certain level potential harm however appropriate reasonable legally require provider/user produce publish appropriate risk impact assessment order assess risk involved system critical system comprehensive risk impact assessment must also cover assessment risks relating self-determination privacy bodily integrity personal integrity well assets property nondiscrimination also include methods gauging quality fairness model accuracy example bias rates statistical error overall certain sub-groups exhibited system forecasting/category formation 189 f 4. instruments obligations controllers subjects case personalised prices – transparency requirements increasing pricing algorithms e-commerce presents challenges consumer protection law also competition law pricing algorithms review market order adjust prices line demand competitors ’ offers real time e-commerce providers therefore apply personalised prices individual users groups directly via individual discounts algorithmic example specifically cash consumers ’ maximum willingness pay encourage users abort purchase transaction personalisation based scoring processes example using real-time analyses users ’ surfing habits collected another way underlying algorithmic usually “ black boxes ” meaning pool logic behind decisions pricing comprehensible outsiders therefore risk price discrimination example relating protected population groups within meaning general act equal treatment potential harm caused implementation higher personalised prices individual consumers vary greatly nevertheless even small price increases individual goods services added together lead significant welfare losses individuals population groups affected particular may example signalling also lead quasi-collusive high market prices competitors deviously collude prices conditions via algorithms negative effect competition innovative prowess economy ultimately consumers applies intentional algorithms influence prices also parallel behaviour high prices tacit collusion occur means algorithms without specific intention direct price-fixing undertaken humans would suffice overall high level criticality merely trigger transparency requirements labelling obligations pricing comprehensive impact assessment could also help identify discrimination risks algorithmic pricing system pool calculate personalised prices known independent experts able check whether correlate protected population groups known proxies i. e. whether example women certain religious groups pay higher prices consumers also made aware via labelling obligations prices and/or discounts personalised affected parties could exercise access check “ ” price accuracy potential discriminatory factors transparency regarding price-relevant factors also important order observe steering effects personalised pricing behaviour individual consumers may relevant freedom 190 part f lgorithmic 4.1.4 duty draw documentation keep logs complex dynamic dispersed process individual convert input output important regulatory perspective make specific causes particular decision comprehensible errors detected infringements penalised effectively one approach better understand software-based processes work record individual program steps digitally test purposes may required personal processing accordance protection law order fulfil accountability requirement firstly requirement document log sets models level granularity retention periods intended purposes specified protection law provide controllers processors greater legal clarity secondly significant potential harm level 4 required document log program processes sets models described way comprehensible supervisory institutions carrying oversight measures regards origin sets way prepared example optimisation goals pursued using models .4.2 requirements algorithmic 4.2.1 general quality requirements algorithmic system operators required standards guarantee minimum level quality technical mathematical-procedural perspective procedural criteria imposed must ensure algorithmically derived results obtained correct lawful manner purpose quality criteria imposed particular regards mathematical model specific processing methods corrective control mechanisms quality system security strike balance conflicting fundamental software operator subjects decisions requirements validity mathematical models relevance underlying become stricter potential algorithmic cause harm increases case algorithm-based algorithm-driven decisions skill sensitivity also built design example deliberately mandating completion certain training modules situations decision assistants example proven particularly helpful introduce systemimposed role changes certain intervals words assign user task making initial decision sees algorithmically derived proposal attention tests another option albeit one individual user may perceive onerous require detect incorrect decisions computer deliberately interspersed among correct ones – therefore also require true nature proposals question identified good time anyone suffers harm 191 f 4. instruments obligations controllers subjects steps also taken ensure improvement processes carried fairly regard interests everyone affected particular attention paid ensuring suitable feedback loops take interests subjects system operators account regard quality would also advisable specify extent estimated “ proxy ” → see part c section 2.2.2 et seq permitted forbidden certain areas application addition requirements placed algorithmic system actual processing purpose security requirements also fulfilled design stage individual requirements parties involved taken consideration order ensure appropriate design-related decisions taken part conceptualisation implementation operation although system operator usually main responsibility risk assessment system operator fulfil responsibility access sufficient documentation e. g. manufacturer ’ risk impact assessment also needs clarity responsible area areas identified critical ethics commission recommends setting legal specifications relating ●minimum standards required security measures taken ●specific details regarding conditions manufacturers system operators must design conduct test procedures example identify bias and/or discriminatory distortion ●legal consequences case security gaps errors ●duties draw documentation functionality tests users receive order able assess risks 6 cf article 22 3 gdpr 7 cf article 13 2 f gdpr article 14 2 g gdpr article 15 1 h gdpr ●obligations carry system updates within specified time frame report 4.2.2 special protective measures algorithmic context decision-making humans must become object key principle regulation algorithmic particularly pertinent algorithmic order support decisions automate decision-making processes i. e. replace decisionmaking technical processes article 22 gdpr codifies principle applicable existing law certain algorithmic fall within scope gdpr one subject decision based solely automated processing including profiling produces legal significant effects concerning – unless necessary entering performance contract based subject ’ explicit consent authorised law fully automated decision permitted controller must implement protective measures order safeguard subject ’ interests6 stricter duties provide access also apply.7 192 part f lgorithmic ethics commission believes various aspects rules currently require clarification duties provide access connected article 22 gdpr “ including profiling ” refer automated profiling individual credit reference agencies example consider subject rules claiming apparently simply conduct profiling “ decisions ” made companies example request credit score ethics commission believes argument sufficiently take intention gdpr account long-term effects subjects profiling could firstly significant secondly gdpr particularly emphasises profiling protection authorities courts able apply applicable law appropriate extent means interpretation based protective purpose gdpr welcomed however time given sensitive issue terms fundamental democratically legitimised legislator called upon specify legal framework conditions soon order create legal certainty quickly possible ethics commission recommends federal government advocate part evaluation gdpr clarification specification also needed regarding question decision pursuant article 22 gdpr “ based solely ” automated processing personal scope term “ similar effect ” protection article 22 3 gdpr ethics commission recommends federal government advocate evaluation gdpr scope article 22 gdpr fleshed potential harm caused algorithmdetermined decision-making original guiding principle article 22 gdpr particular categorically differ many algorithm-driven decision-making particular tendency humans involved simply accept recommendations algorithmic exercise discretion plays role.in view fact potential harm algorithm-based varies heavily detail ethics commission believe would appropriate generally broaden prohibitory principle article 22 gdpr particular principle final decision-making pursuant article 22 3 gdpr suitable algorithmic equal measure algorithmic “ decision ” taken system within meaning current wording article 22 1 gdpr final decision made would often practical also often desirable instead ethics commission recommends risk-adapted regulatory regime provides individuals appropriate safeguards particular profiling opportunities defend mistakes made jeopardised legal notion humans must become mere object technical also form central legislative anchor point within horizontal legal instrument eu-asr → see section 3.3 risk-adapted regulation algorithmic ethics commission recommends within accompanying sectoral legal instruments legal instruments therefore include provisions also set specifications algorithm-based decisionmaking outside scope article 22 gdpr far layer regulation also covers algorithmic also fall within scope article 22 gdpr may modified light recommendations made regulatory must precisely synchronised 193 f 4. instruments obligations controllers subjects 4.2.3 appropriate algorithmic inferences ethics commission believes processes involved data-based generation algorithmic inferences supposed interests tendencies character traits individuals particular consumers deserve maximum social political attention economy awash inferences characteristic many business models geared towards detailed personalisation certain offers services many consumers appreciate convenience offers services however also lead risks inferences made based incorrect pool results inappropriate contents obtained account inadequacy system components order prevent risks could posed certain algorithmic inferences many want grant subjects legal “ appropriate inferences ” .8 proposal sets comprehensive package measures would give subject effective tool monitoring inferences concerning generated operators algorithmic addition substantive subject appropriate inferences sets obligation part system operator without requested inform individual concerned inferences drawn “ appropriate ” reasons case ethics commission welcomes debate proposal “ appropriate inferences ” triggered however points could affect constitutionally protected interests operators algorithmic view ethics commission regulatory proposal take protection aspects consideration example limiting scope high level criticality due relevance terms participation fundamental 8 omer tene jules polonetsky northwestern journal intellectual property 11:5 pp 279 et seq sandra wachter brent mittelstadt columbia business law review 2 p. 1 et seqq proposal consists material component procedural component.4.2.4 legal protection discrimination one main aims regulation algorithmbased algorithm-driven algorithm-determined decision-making prevent discrimination individual based characteristic set article 3 3 basic law federal republic germany and/or article 21 1 charter fundamental union well objectively unjustified discrimination protect personal integrity individuals concerned whilst state bodies direct obligation uphold fundamental undertaking kind state activity therefore subject comprehensive prohibition discrimination sub-constitutional basis required private actors technical legal starting point essentially german general act equal treatment also serving incorporate according directives german law alongside general clauses german private law example unconscionable contracts discrimination private individuals fall general act equal treatment firstly discrimination must grounds sensitive characteristic race ethnic origin gender religion disability age sexual orientation secondly situational scope must open employment context access goods services including housing available 194 part f lgorithmic principle provisions general act equal treatment already cover discrimination algorithmic accordance applicable law however matters susceptible discrimination included scope general act equal treatment act cover sensitive situations algorithmically established results trigger facilitate discrimination e. g. case mortgage offer based individual risk assessment therefore worth considering example broaden situational scope general act equal treatment include automated decision-making processes additionally incorporating individual areas relating algorithmic inferences particularly sensitive terms personality.9 primarily concerns areas could long-lasting negative effect person ’ way life consumer contracts drawn based scoring high-risk procedures facial recognition methods price discrimination certain areas life healthcare contractual partner ’ general freedom action equally constitutionally protected must also properly taken consideration 9 mario martini juristenzeitung jz p. 2021.it also necessary discuss whether context algorithmic legislators remove restrictive reference specific grounds discrimination discriminatory effects algorithmic sometimes reflect bias exists within society regard classic grounds discrimination example far bias training model would example case system select candidates trained using successful managers past overwhelmingly male however potential algorithmic discriminate extends far beyond example disadvantage systematically associated group attributes discrimination prohibited law e. g. home address specific district correlations determined means pattern recognition really random extent situations already managed form indirect discrimination respect suitable relaxation rules relating burden proof may also possibly required extent however entirely issues fairness also arise concern distribution opportunities detriment traditionally marginalised communities also exclusion groups thrown together based less coincidental attributes specific characteristics machine creating grounds discrimination however could enormous widespread impacts account fact trained algorithms also areas application 195 f 4. instruments obligations controllers subjects therefore appropriate consider broadening protection include every systematic objectively unjustified type discrimination based group attribute ethics commission recommends federal government also examine appropriately adjusting general act equal treatment alternatively anchoring protection future specific algorithm legislation particular regulatory problem fundamentally evergrowing plethora group attributes could lead algorithmic discrimination hence systematic nature would sole criterion differentiating prejudices relevant irrelevant terms discrimination law corresponding regulation substantive protection discrimination would therefore case accompanied one hand corresponding duties disclosure duties state reasons various internal external oversight mechanisms regulation would provide substantive examination criteria consequences regulation parties involved would case meticulously assessed weighed irrespective issue broadening definition offence thought given whether rules burden proof already sufficiently reflect characteristics algorithmic ascertaining indirect discrimination requires neither proof intent discriminate unambiguous proof causality fact injured party prove correlation decisions sensitive criteria algorithmic however proof generally difficult affected parties provide.the ethics commission therefore recommends legislators enact legislation clarifying requirements providing proof discrimination operators algorithmic lower requirements affected parties needed reason general act equal treatment always considered together access duties state reasons → see section 4.1.2 without injured party would often unable exercise protection interests third parties system users affected result must given sufficient consideration 4.2.5 preventive official licensing procedures high-risk algorithmic case algorithmic regular appreciable level 3 even significant potential harm level 4 addition existing regulations would make sense establish licensing procedures preliminary checks carried supervisory institutions order prevent harm subjects certain sections population society whole teil f algorithmische ysteme summary important recommendations action instruments 45 ethics commission recommends introduction mandatory labelling scheme algorithmic enhanced criticality level 2 upwards mandatory scheme kind would oblige operators make whether i.e extent algorithmic regardless system criticality operators always obliged comply mandatory labelling scheme risk confusion machine might prove problematic ethical point view 46 individual affected decision able exercise “ meaningful logic involved well scope intended consequences ” algorithmic system cf gdpr respect fully automated also situations involve kind profiling regardless whether decision taken basis later line also expanded future apply algorithm-based decisions differing levels access decisions according system criticality measures may require clarification certain legislative provisions widening regulatory scope level 47 certain cases may appropriate ask operator algorithmic system provide individual explanation decision taken addition general explanation logic procedure scope system main objective provide individuals affected decision comprehensible relevant concrete ethics commission therefore welcomes work carried banner “ explainable ” efforts improve explainability algorithmic particular self-learning recommends federal government fund area 48 view fact certain sectors society whole may affected well individual members also particular parties individually affected algorithmic system entitled access certain types likely kind would granted primarily journalistic purposes order take due account operator ’ interests would need accompanied adequate protective measures ethics commission believes consideration also given granting unconditional access certain circumstances particular algorithmic serious potential harm level 4 state 197 f summar important recommendations action 49 appropriate reasonable impose legal requirement operators algorithmic least potential harm level 2 upwards produce publish proper risk assessment assessment kind also cover processing non-personal well risks fall heading protection particular appraise risks posed respect self- determination privacy bodily integrity personal integrity assets ownership discrimination encompass underlying logic model also methods gauging quality fairness model accuracy example bias rates statistical error overall certain sub-groups exhibited system forecasting/category formation 50 provide controllers processors greater legal clarity work must done terms fleshing requirements document log sets models level granularity retention periods intended purposes addition operators sensitive applications obliged future document log program runs software may cause lasting harm sets models described way comprehensible employees supervisory institutions carrying oversight measures regards origin sets way pre-processed example optimisation goals pursued using models 51 system operators required standard- setting guarantee minimum level quality technical mathematical-procedural perspective procedural criteria imposed must ensure algorithmically derived results obtained correct lawful manner purpose quality criteria could imposed particular regards corrective control mechanisms quality system security example would appropriate impose quality criteria relationship algorithmic processing outcomes obtain outcomes 52 ethics commission believes necessary first step clarify flesh greater detail scope legal consequences article 22 gdpr relation algorithmic context decision-making second step ethics commission recommends introduction additional protective mechanisms algorithm-based algorithm-driven decision-making since influence real-life settings may almost significant algorithm-determined applications prohibitory principle followed date article 22 gdpr replaced flexible risk-adapted regulatory framework provides adequate guarantees regards protection individuals particular profiling concerned options individuals take action mistakes made jeopardised 53 consideration given expanding scope anti-discrimination legislation cover specific situations individual discriminated basis automated analysis automated decision-making procedure addition legislator take effective steps prevent discrimination basis group characteristics qualify protected characteristics law discrimination often currently qualify indirect discrimination basis protected characteristic 54 case algorithmic regular significant level 3 even serious potential harm level 4 would useful – supplement existing regulations – covered licensing procedures preliminary checks carried supervisory institutions interests preventing harm individuals affected certain sections population society whole 198 part f lgorithmic 5. institutions ethics commission takes view burden responsibility ethically justified lawful algorithmic must shared rest several sets shoulders institutions supervisory structures currently exist sufficiently prepared effectively oversee monitoring algorithmic various levels ethics commission therefore urges federal government expand reorient competences existing supervisory institutions structures set institutions structures necessary 5.1 regulatory powers specialist expertise 5.1.1 distribution supervisory tasks within sectoral network oversight authorities ethics commission recommends federal government principle entrust regulatory supervisory tasks oversight powers case authorities already sector-specific expertise view ethics commission apply matters fall within administrative competence states bundesländer specifically ethics commission believes would make sense entrust oversight algorithmic private parties sectors economy authorities sectorspecific responsibility already exist existing authorities examples authorities federal financial supervisory authority bundesanstalt für finanzdienstleistungsaufsicht bafin federal network agency bundesnetzagentur bnetza federal office security bundesamt für sicherheit der informationstechnik bsi federal motor transport authority kraftfahrtbundesamt kba come mind furthermore federal cartel office bundeskartellamt bkarta protection supervisory authorities would special status horizontal responsibilities i. e. responsibilities span various different sectors economy.the ethics commission believes national eu-level “ oversight network critical algorithmic ” set order coordinate activities authorities entrusted algorithm supervisory tasks particular rules distribution responsibilities within network exchange organisation administrative procedures carried network legal protection would appropriate purposes order prevent gaps supervision ethics commission urges federation länder identify areas currently sector-specific authority sufficient expertise oversight tasks could assigned monitoring critical algorithmic view ethics commission cases often appropriate event corresponding need oversight entrust matters one existing authorities horizontal responsibility case algorithmic process sensitive personal protection authorities example may adequate expertise however ethics commission believes particular cases may necessary create completely regulatory control structures light ever-changing technical developments federation länder regularly review situation authorities faced structural challenge effectively executing algorithmic system oversight tasks object oversight work technically highly complex subject dynamic change ethics commission therefore believes providing authorities practical skills particularly important firmly recommends federal government provide federal authorities financial technical resources required draft salary structure modernisation act besoldungsstrukturenmodernisierungsgesetz expected increase salaries bonuses publicsector professionals establish regulations without doubt welcome first step however light difficult attract welltrained professionals sector measures soon required 199 f 5. institutions ethics commission also recommends federal government set official unit form competence centre algorithmic provide sectoral authorities support monitoring algorithmic responsibility acquire analyse develop impart technical methodological knowledge required supervising critical algorithmic coordination request sector-specific authorities also primarily support sector-specific supervisory authorities building expertise needed carry tasks assess criticality algorithmic extend particular centre ’ task developing criteria processes tools oversight algorithmic also include standards assessing criticality checking compliance critical algorithmic centre competence also important intermediary advisory role far possible advise bodies federation länder municipalities also manufacturers system operators system users subjects regard algorithmic also involved international initiatives designed build sufficient oversight expertise including standardisation procedures however competence centre supervisory powers remain sectoral supervisory authorities service unit either created autonomous federal authority attached existing cross-sectional authority federal office security ethics commission considers would also make sense establish corresponding union level future example form agency federal government work towards achieving 10 example article 58 gdpr governs investigative powers relating protection supervision section 32e german act restraints competition gesetz gegen wettbewerbsbeschränkungen gwb governs sector inquiries federal cartel office oversight high-frequency trade financial supervisory authorities based section 6 4 german securities trading act gesetz über den wertpapi erhandel wphg section 3 4 4 5 german stock exchange act börsengesetz börsg amended version conjunction section 7 3 stock exchange act.in principle ethics commission sees reason state bodies able make expertise private individuals entities carrying tasks building in-house expertise involve private individuals entities execution tasks long cooperation complies general constitutional administrative specifications applicable cooperation conversely corresponding cooperation example also entrustment may order deal current lack qualified specialists expertise sector 5.1.2 definition oversight powers according tasks involved regulating law clearly assign relevant competent authorities powers intervention including inspection access required supervision algorithmic blueprints regulatory powers control found various areas law.10 competent supervisory authorities must times able examine algorithmic sensitive areas application high potential harm audit test procedures must particular cover interaction user may example take place via standardised interfaces access carry known input-output tests check example whether algorithmic system systematically discriminates groups particularly useful case adapt internal rules time steps must taken ensure testing lead change system rules whereby system learns test test 200 part f lgorithmic assigning legal authority steps must taken ensure supervisory authorities power event proven breach law force operators algorithmic configure compliance law example adapting pool necessary apply penalties provided commensurate case question supervisory authorities also able impose official bans unlawful algorithmic components 5.1.3 criticality-adapted extent oversight elements algorithmic system must taken account order behaviour effectively audited audit conducted authorities may potentially must extend training processes final rule-based model well input output underlying decisions quality indicators regarding pool model accuracy training model final decision model also taken consideration order identify system ’ bias rates statistical error overall certain sub-groups methodological perspective test may carried analysing large amounts reviewing weighting factors complex multidimensional analysing inputthroughput-output due complex nature subject matter amounts involved control algorithms significantly increase efficiency effectiveness audit systematically look conspicuous patterns pool results algorithmic system example shed light case discrimination.the extent oversight required specific case determined based area application system criticality case potential harm level 2 may suffice legislators limit regulatory oversight inspection results event system ’ documented failure however areas high potential harm may necessary stipulate system operators must standardised interface view ethics commission question whether regulatory oversight affects system operators ’ trade business secrets third parties ’ privacy issue level criticality pyramid supervisory authorities obliged treat obtained part oversight work confidential due professional secrecy aspects represent legal obstacle far-reaching powers full detailed audits proper interpretation test results technical perspective anything trivial particular always whether really unearth error algorithmic system restricts ability provide evidence quality informative value different test procedures audits therefore also need agreed – particular regard probative value court proceedings order enforce parties affected ethics commission therefore recommends federal government support initiatives develop statistical technical standards test procedures audits necessary differentiated areas application competence centre algorithmic → see section 5.1.1 take leading role endeavours 201 f 5. institutions case personalised prices ii – ex-post controls supervisory institutions 11 cf gesellschaft für informatik technische und rechtliche betrachtungen algorithmischer entscheidungsverfahren gutachten der fach gruppe rechtsinformatik der gesellschaft für informatik e. v. im auftrag des sachverständigenrats für verbraucherfragen gesellschaft für informatik technical legal considerations regarding algorithmic decision-making processes report legal informatics expert group gesellschaft für informatik e. v. request advisory council consumer affairs berlin pp 63 et seqq available www.svr-verbraucherfragen.de/wp-content/uploads/gi_studie_algorithmenregulierung.pdf .supervisory institutions could check whether algorithmic pricing e-commerce comply law discriminate example protected population groups within meaning general act equal treatment supervisory authorities could look conspicuous patterns pool issued prices may shed light possible case discrimination carrying supervision comprehend potentially highly complex rules underlying algorithm analysing code effective oversight carried help statistical tests analyse things equal issued prices change depending input associated certain population groups example system issues higher prices consumers gender changed “ male ” “ female ” input issued prices correlate attributes protected equality legislation individual population groups example via proxies mathematically statistically determined.11 5.2 corporate self-regulation co-regulation neither possible necessary legislator implement blanket regulations covering algorithmic instead various models self-regulation co-regulation could also essentially provide sufficient responses certain situations co-regulation involves regulatory approaches navigate state regulation private self-regulation characterised combination public/state component private/institutional component.5.2.1 self-regulation self-certification ethics commission recommends selfregulation form internal audit conducted manufacturer operator algorithmic system lowest level criticality pyramid could supported self-certification manufacturers operators basis specific standards algorithmic particular advantage system would self-certification bodies would necessary know-how account close connection specific topics result experts even companies question could take legal standards monitoring compliance therewith consideration including stage necessary also incorporate corporate expertise regulatory mechanisms institutionally admittedly purely internal voluntary self-regulation would constitute independent monitoring event breaches would ensure effective implementation penalties 202 part f lgorithmic self-regulation architecture could supplemented model involving regulated self-monitoring would set external standards quality risk management self-monitoring could also externally monitored similar system set gdpr article 40 establishes option specify general clauses gdpr make applicable specific real-life circumstances significant parties subject codes conduct well set minimum standards specific sector question order able guarantee regulation would effective intended effective monitoring must ensure actual compliance approved codes conduct pursuant article 40 gdpr would codes conduct drawn procedural rules relating monitoring control implementation penalties cases non-compliance would also set provider signs voluntary self-monitoring verifiably demonstrates compliance agreed procedures standard-setting may grant privileges terms supervisory measures approach would based condition exercising corporate responsibility cooperation private self-monitoring providers would develop procedural standards would recognised supervisory authority involvement civil society organisations preparatory work would essential order able properly represent interests citizens consumers take consideration 12 mario martini juristenzeitung jz p. 1022 et seq.5.2.2 creation code conduct concept regulated self-regulation would worth considering including algorithmic accountability code adopting “ comply explain ” approach well-established parts legal system could oblige parties subject regulation state whether extent following recommendations code.12 false statements would subject sanctions code drawn could binding nature holding companies authorities responsible consequences algorithmic could example developed based corporate responsibility guidelines → see part section 2 conversely also help shape guidelines level granular detail codes guidelines practical and/or sector-specific ethical challenges specific code would useful become quality defined requirements framework conditions i. e. opportunities independent external parties carry checks ability impose penalties event breaches would essential ensuring code control function responsibility developing code assigned independent commission equal representation manufacturers operators scientific community civil society remains seen whether government commission german corporate governance code regierungskommission deutscher corporate governance kodex www.dcgk.de could model addition alternatively binding statements manufacturers operators algorithmic could considered 203 f 5. institutions 5.2.3 quality seals algorithmic establishing quality seals algorithmic sensible order support effective algorithm regulation could take form voluntary mandatory evidence protective measures would make extent algorithmic system meets certain requirements users would important clarify would define requirements quality seal would specifically responsible fulfilling requirements connected quality seal extent breaches would subject penalties case algorithmic accountability code responsibility defining requirements quality seal entrusted independent commission equal representation operators algorithmic scientific community civil society 5.2.4 contact persons algorithmic companies authorities companies authorities work critical algorithmic level 2 least starting certain company authority appoint contact person responsible communications authorities cooperation cases must ensured contact person specific expertise monitor algorithmic internally provide company ’ authority ’ management team advice functionally independent case protection officers contact person could act link supervisory authority operators algorithmic affected groups people would also help ensure proper awareness problems within companies authorities increase oversight pressure inside.5.2.5 involvement civil society stakeholders order ensure interests civil society affected companies properly taken account part audits algorithmic advisory boards set within sector-specific competent authorities civil society stakeholders also example involved connection code advisory boards feature balance representatives civil society organisations individuals appointed companies order ensure interests affected individuals groups interests affected companies properly taken account part audits 5.3 technical standardisation view ethics commission standardisation organisations iso/iec ieee ietf itu etsi w3c cen din set technical standards communications technologies could significantly help forming sector-specific requirements algorithmic technical standards take ethical legal requirements consideration could provide legal certainty companies develop algorithmic could also easily requirements legality algorithmic specific guidelines individual sectors ethics commission believes technical standards would essentially useful tools bridge gap “ classic ” state regulation purely private self-regulation therefore recommends federal government suitably work develop adopt technical standards designed prevent risks posed algorithmic 204 part f lgorithmic however view ethics commission federal government also lose sight fact technical standards limitations → see part section 6 technical standards substitute defining legal requirements algorithmic regulatory supervision constitutional reasons principle citizens ’ fundamental affected detailed legal provisions must upheld practice means legislators must first define legal framework – technical standard-setting committees least ensure integrity decision-making protected participation representatives sectors and/or affected companies ensure addition impressive technical expertise interests companies and/or sectors course also often taken consideration first hand technical standards drawn 5.4 institutional legal protection particular associations file action system granting competitors competition associations consumer associations file action important feature german legal landscape many years could play key role civil society oversight algorithmic particular private kind allow civil society players legitimate mandate enforce compliance legislative provisions area contract law fair trading law without needing rely authorities take action without needing wait individuals authorise civil law approach particularly strong market characterised swift responses therefore international standards successful associations essentially politically administratively independent therefore advocate authority common interest consumers companies competition regulations consumer efficiently protected unfair business practices also damaging consumers.anyone comply regulatory provisions potentially benefit unfair competitive advantage order prevent competitive edge gained breaking law competition associations consumer associations able stop legal infringements 205 summary important recommendations action institutions 55 ethics commission recommends federal government expand realign competencies existing supervisory institutions structures necessary set ones official supervisory tasks powers primarily entrusted sectoral supervisory authorities already built wealth expert knowledge relevant sector ensuring competent authorities financial technical resources need particularly important factor respect 56 ethics commission also recommends federal government set national centre competence algorithmic centre act repository technical regulatory expertise assist sectoral supervisory authorities task monitoring algorithmic ensure compliance law 57 ethics commission believes initiatives involving technical statistical quality standards test procedures audits differentiated according critical application areas necessary worthy support test procedures kind – provided designed adequately meaningful reliable secure – may make vital contribution future auditability algorithmic 58 opinion ethics commission particular attention paid innovative forms co- regulation self-regulation alongside complement forms state regulation recommends federal government examine various models co-regulation selfregulation potentially useful solution certain situations 59 ethics commission believes option worth considering might require operators law inspired “ comply explain ” regulatory model sign declaration confirming willingness comply algorithmic accountability code independent commission equal representation – must free state influence – could set develop code kind would apply binding basis operators algorithmic appropriate involvement civil society representatives drafting code must guaranteed 206 part f lgorithmic 60 voluntary mandatory evidence protective measures form specific quality seal may also serve guarantee consumers algorithmic system question reliable time providing incentive developers operators develop reliable 61 ethics commission takes view companies authorities operating critical algorithmic obliged future appoint contact person way companies specific currently obliged appoint protection officer communications authorities routed contact person also subject duty cooperation 62 ensure official audits algorithmic take due account interests civil society companies affected suitable advisory boards set within sectoral supervisory authorities 63 opinion ethics commission technical standards adopted accredited standardisation organisations generally useful measure occupying intermediate state regulation purely private self-regulation therefore recommends federal government engage appropriate efforts towards adoption standards 64 system granting competitors competition associations consumer associations file action important feature german legal landscape many years could play key role civil society oversight algorithmic particular private kind could allow civil society players legitimate mandate enforce compliance legal provisions area contract law fair trading law anti-discrimination law without needing rely authorities take action without needing wait individuals authorise 207 f 6. special topic algorithmic media intermediaries 6. special topic algorithmic media intermediaries 6.1 relevance democratic process example social networks many people would impossible imagine life days without social networks search engines like enable users keep date latest news around world circle friends real time platforms people portray lifestyles communicate also entertainment purposes business activity including advertising whole becoming increasingly important private opinion-forming order manage wealth available providers services algorithmic designed amongst things identify interests tendencies convictions users identify posts potential relevance present similar posts order encourage interact network filter illegal offensive posts economic aim primarily generate high advertising revenue depending reach media intermediaries profound impact democratic process people also using social networks keep abreast politics world affairs social networks therefore offer users opportunities participate society sense constitute media factors exchange opinions .at time fact debate concentrated private platforms also poses challenge democracy economic stakeholders private operators social networks vested interest directing traffic networks gearing activity primarily towards economic aspects rather focusing social interests multi-faceted opinion-forming process benefit good algorithmic predominantly oriented economic criteria negative consequences diversity opinions social networks services also lead manipulation opinions one hand happen unintentionally due certain characteristics underlying software example recommender hand intentionally various actors manipulative purposes operators social networks sufficiently guarded activities threaten foundations democracy regulatory framework social oversight needed particular view high level criticality 208 part f lgorithmic ethics commission believes future media intermediaries gatekeeper role ultimately develop high potential harm democracy resulting need regulation ethics commission believes essential legislators create appropriate regulatory framework algorithmic media intermediaries ethics commission opinion first operators platforms providers services define implement basic rules ensure fairness opinion-forming process however “ domiciliary ” limitations particular integrity democratic process affected depending market share gatekeeper role platforms services operators fundamental-rights-based obligations account indirect third-party effect.13 view ethics commission obligations specified precisely subconstitutional law particular also regard algorithmic platforms services significant market share gatekeeper role also relevant eu-asr recommended ethics commission → see section 3.3 regulation also needed ensure regulatory fairness comparison broadcasters ethics commission recommends federal government examine risks posed providers particular power influence opinions countered whole range measures possible greater transparency ex-ante controls form licensing procedure algorithmic relevant terms democracy 13 decisions federal constitutional court 128 p. 249 fraport 148 p. 267 et seqq. 32 et seqq stadionverbot 14 cf decisions federal constitutional court 136 9 28 references.6.2 diversity media intermediaries example social networks wide variety roles played social networks predominantly high level criticality algorithmic present particular challenges ethics commission ’ suggested approach risk-adapted regulation algorithmic ethics commission believes positive legal provisions social networks example increase transparency range discussions held bolster users would particularly constructive case social networks dominant market share ethics commission calls measures safeguard diversity defensive measures alone suffice algorithmic operate types networks impacts freedom diversity opinion-forming constitutive democracy extremely high level criticality account reach alone ethics commission believes legislators therefore ethical constitutional obligation establish binding normative framework regulation media intermediaries order protect democracy may require transforming regulatory framework governing media legislators must take suitable measures ensure total range offer reflects variety opinions exist guarantees balance neutrality freedom bias society .14 applies particular media intermediaries gatekeeper role power influence opinions according federal constitutional court safeguard pluralistic diversity substantive organisational procedural regulations needed focused creating freedom communication therefore suitable producing desired effects article 5 1 basic law federal republic germany 209 f 6. special topic algorithmic media intermediaries light legislators länder responsible media law obliged implement aforementioned provisions applies legislators regulation algorithmic eu-asr → see section 3.3 media intermediaries video-sharing platforms vsps already subject audiovisual media services directive15 provide user-generated general draft interstate media services agreement also covers media intermediaries ethics commission welcomes respect provisions transparency social networks set draft interstate media agreement medienstaatsvertrag mstv-e initial step direction legislators länder plenty scope freedom drawing provisions however must decide regulation model must leave private individuals agree ethics commission view plurality obligations media intermediaries case include obligation algorithmic least additional option also provide access unbiased balanced selection posts reflect diverse range different opinions.16 based considerations ethics commission also recommends federal government investigate whether areas irrespective situation relevant democracy discussed corresponding obligation establish requirements neutrality provisions diversity seems necessary protecting minors influenced social networks example one consideration 15 directive 2010/13/eu 10 march coordination certain provisions laid law regulation administrative action member states concerning provision audiovisual media services audiovisual media services directive 16 rolf schwartmann maximilian hermann robin mühlenbeck multimedia und recht mmr 8 p. 498 et seqq.6.3 labelling obligation social bots democratic process essence based people ’ freedom form opinions make decisions however bots i. e. software programs give impression users various platforms view ethics commission highly problematic bots manipulate individual users and/or debate guide result vote one way political decisions made firstly simulation traits falsely suggests statements made result independent thought independent formation political opinions secondly automation massively increase number frequency expressions opinion making harder even impossible assess actual majorities opinions ethics commission believes regulatory intervention required basis ethics commission recommends implementing measure enhance transparency form labelling obligation social bots social networks based general considerations ethics commission recommends labelling obligation implemented anywhere risk social bots could mistaken interlocutors → see section 4.1.1 given particular potential jeopardise democratic process ethics commission furthermore believes case labelling obligation social bots impact political opinion-forming processes essential even irrespective real risk confusion 210 part f lgorithmic 6.4 measures combat fake news labelling obligation social bots could help combat automated spread fake news however ethics commission also believes concept fake news suitable starting point regulation relating media legislation presentation legal definition fake news draws objective distinct line exaggerated satirical expression opinion intentional misrepresentation news impossible due complexity communications disinformation manipulation opinion-forming typically associated term “ fake news ” also result true facts presented selectively ethics commission also particular recommends legislators operators social networks grant users easy-to-exercise reply requiring network post correction statement proven false e. g. invented quote timeline newsfeed etc users network using available trace back shown false statement ethics commission emphasises state must create incentives collateral censorship social networks provide protection “ overblocking ” ethics commission therefore believes necessary parallel obligations imposed operators grant affected individuals prompt efficient procedural protection mechanisms ethics commission believes include particular effective process reinstate deleted posts provided break laws invocation networks rules alone suffice grounds permanent deletion/blocking view ethics commission must apply users respect social networks.6.5 transparency obligations news aggregators social networks algorithmic also aggregate select present journalistic/editorial third parties generally accessible way allow users interested third parties enough insight technical procedure select prioritise news make recommendation arrived individual case democratic interest would essentially take precedence business secrets media intermediaries interests fair opinion-forming process fair exchange opinions duties disclose also stretch economic ties reason well ethics commission welcomes current thoughts reforming interstate media agreement medienstaatsvertrag mstv-e call corresponding transparency obligations media intermediaries soon certain reach 211 summary important recommendations action special topic algorithmic media intermediaries 65 given specific risks posed media intermediaries act gatekeepers democracy ethics commission recommends options examined countering risks also regard influencing legislation → see recommendation 43 whole gamut risk mitigation measures considered extending ex-ante controls e.g form licensing procedure 66 national legislator constitutional obligation protect democratic system dangers free democratic pluralistic formation opinions may created providers act gatekeepers establishing binding normative framework media ethics commission believes small number operators concerned obliged algorithmic allow users least additional option access unbiased balanced selection posts embodies pluralism opinion 67 federal government consider measures take due account risks typically encountered media sector respect media intermediaries also respect providers act gatekeepers whose associated lower potential harm measures might include mechanisms enhancing transparency example ensuring available technical procedures select rank news stories introducing labelling obligations social bots establishing post countering responses timelines 212 part f lgorithmic 7. algorithmic state bodies 7.1 opportunities risks involved algorithmic state bodies citizens rightly expect state best available carry duties depending type duties also include algorithmic already exist relieve state bodies repetitive tasks thereby expediting processes freeing resources complex cases certain set-ups improve consistency quality state activity form chatbots voice assistants example facilitate citizens ’ access justice time using algorithmic state bodies must uphold particularly high standards firstly direct obligation uphold fundamental authorities secondly state activity general expected set example whole society institutional capacity expertise state must build order ensure sufficient oversight algorithmic private parties must therefore also order guide oversee work carried state bodies particular competence centre algorithmic called ethics commission likely play key role context.the algorithmic state bodies must treated principle particularly sensitive within meaning criticality pyramid least level 3 therefore view ethics commission comprehensive risk impact assessment must carried mandatory requirement ethically sound algorithmic furthermore depending criticality state necessary instruments discussed designed ensure citizens protected put place algorithmic state farther-reaching legal protection requirements remain unaffected constitutional administrative specifications design additionally view ethics commission certain sectors algorithmic conflicts constitutionally protected overriding importance algorithmic irrespective protective measures taken case question permitted restrictive conditions prohibited particular concerns algorithmic purposes law-making jurisprudence 7.2 algorithmic law-making algorithmic within government context law-making subject restrictions ethics commission sees democratic process sense people able form opinions make decisions freely possible essentially sacrosanct automated support law-making therefore acceptable low-level ancillary tasks e. g. detecting inconsistent terms and/ legal instruments far removed democratic decision-making process e. g. catalogues technical specifications subsequent regulations cases must meet extremely strict requirements quality security 213 f 7. algorithmic state bodies context ethics commission also particular opposes demand newly enacted legal instruments already formulated view possible future automated application regard must follow law reverse accordance conventional criteria assessment legislation compliance fundamental higher-ranking law impact assessment etc two equivalent versions conceivable may argument one version easier algorithmise tip scales favour 7.3 algorithmic dispensation justice ethics commission view algorithmic dispensation justice permissible peripheral tasks justice administered “ name people ” means least contentious proceedings well administrative court proceedings criminal proceedings always administered judges pacification effect court proceedings achieved judgment fairness finding also hearing weighing conflicting interests humans particular structural processing facts legal consequences procedural fairness contrast opaque blackbox decision due often high level trust placed supposed “ infallibility ” technical automation bias well low level willingness make divergent decisions particular associated additional burden reasoning proof risk “ miscarriage justice ” default effects even legally non-binding proposals decisions judgments algorithmic generally highly problematic perspective parties concerned however algorithmic provided strict quality control high security standards place useful preparatory work directly affect judicial decision e. g. file management document control .lastly retrospectively analyse judicial decisions available voluntary judges protected access third parties high-level security measures also conceivable could example work whether decisions influenced external factors ones order provide judges future ways prevent distortions thus contribute better consistent dispensation justice researchers may also legitimate interest access though sufficient safeguards would required individual cases purpose monitoring path judicial decision-making checking dispensation work judges external targets e. g. average processing time case however view objective judicial independence permissible pre-litigation domain example exercising air passenger also dunning procedure similar view ethics commission fully automated handling legal claims permissible provided procedural individual parties concerned safeguarded result however case algorithmic create correlations follow legal provisions procedural steps set current state art based classic deterministic algorithms therefore generally considered example make decisions meeting formal criteria open assessment systemic point view impending losses expertise compensated freeing resources complex individual cases 214 part f lgorithmic 7.4 algorithmic administration potentially greater scope algorithmic administration increased automation authorities ’ routine cases included subject precisely defined conditions regarding facts legal consequences may advisable interest efficiency section 10 2 administrative procedures act order carry administrative procedures appropriately swiftly possible particular relieving administrative staff routine tasks frees resources deployed handle procedures automated potential particular provision services benefits ethics commission believes algorithmic expand proactive procedure management whereby required available authorities services benefits increasingly provided without need applications educationally disadvantaged individuals needy particular could benefit cf family allowance austria provided child born without need apply however case intervention authorities algorithmic must dealt carefully fundamental particularly affected judicial applies algorithmdetermined administrative decisions also limits authorities ’ scope decision-making general assessing whether permit extent resulting intervention reversibility decisions need taken consideration essentially designing must easily accessible oversight therefore sensitive areas administration often allowed based classic deterministic algorithms proprietary software avoided reason.in case discretionary decisions executive decisions discretion external legal effect ethics commission believes currently necessary humans make final decision decision mere beneficial impacts however forming groups cases specification conceivable discretion could reduced extent view algorithmic system one option terms decision ethics commission view section 35a german administrative procedures act sufficiently reproduce range different possible types cases schematic taking account safeguards required constitutional law based article 22 gdpr legislators carefully expand scope section 35a administrative procedures act and/or set provisions differentiated terms specific legislation administrative acts supported partially fully automation regulations partial full automation administrative procedures developed part horizontal sectoral regulations algorithmic recommended ethics commission → see section 3.3 7.5 algorithmic security law discussion especially critical algorithmic security authorities administrative measures area particularly profound effect fundamental algorithmic generally restricted 215 f 7. algorithmic state bodies algorithmic predict crimes threat situations predictive policing consideration must given fact even personal directly effects relevant fundamental case particular reference person re- created means especially detailed location addition “ location-related risk prognoses ” lead excessive police checks certain neighbourhoods identified hotspots therefore ethnic social profiling population groups living measures also trigger crime relocation displacement effects ethics commission therefore recommends making security authorities effects incorporating randomisations prediction order reduce corresponding effects system-based distortions steps must also taken ensure security authorities still always carry review cases risk cases selected system cf section 88 fiscal code germany abgabenordnung ao security authorities allowed order discretionary intervention measures solely basis locationrelated forecasts risk forecasts relating individuals allowed law area security forecasts must created fully automatically could negative legal consequences parties concerned account risk automation bias even case algorithm-based decisions support decisionmakers algorithmic profiling may permissible within strict limits 17 paper part 36th conference freedom officers germany – “ transparenz der verwaltung beim einsatz von algorithmen für gelebten grundrechtsschutz unabdingbar ” “ transparency administration algorithms essential protection fundamental ” ulm 16 october available /www.datenschutzzentrum.de/uploads/informationsfreiheit/2018_ positionspapier-transparenz-von-algorithmen.pdf .7.6 transparency requirements algorithmic state actors state decisions made using algorithmic must remain transparent justifiable generally speaking even important private sector due obligation uphold fundamental need democratic accountability authority power sector therefore general transparency requirements → see section 4.1 apply state bodies state bodies must also strive particularly hard ensure openness ethics commission points many cases algorithmic state actors already fall within scope existing freedom and/or transparency laws ethics commission also welcomes paper “ transparency administration algorithms ” “ transparenz der verwaltung beim einsatz von algorithmen ” adopted 36th conference freedom officers konferenz der informationsfreiheitsbeauftragten germany according paper state bodies must meaningful comprehensive generally comprehensible regarding processing legally possible publish including categories procedure ’ input output ii logic involved particular calculation formulae including weighting input underlying expertise individual configuration deployed users iii scope resulting decisions possible consequences procedures.17 216 part f lgorithmic regard specifying corresponding transparency obligations and/or duties provide access ethics commission also points insufficient provisions transparency lead lack trust lead greater numbers appeals thereby counteracting efficiency gains intended algorithmic reason ethics commission ultimately believes justifiable cases rule access regarding algorithmic across board citing risk manipulation protection business secrets rule therefore particular interests must weighed disclosure system ’ general functionality sufficient every case algorithmic authorities often decisions made authorities must also justified parties affected i. e. “ main factual legal reasons ” led decision particular case must provided cf section 39 1 2 administrative procedures act individual explanation required constitutional subconstitutional law due technical complexity system possible possible way course official complaint procedure court enables effective review viability reasoning algorithmic must prohibited apart ethics commission believes state required build sufficient expertise within administration courts able ensure necessary oversight system-internal decision-making processes.the ethics commission points transparency state activity also negatively affected state uses proprietary software closedsource software private providers carrying duties generally speaking proprietary software makes difficult users make changes adaptations results dependent relationship addition proprietary software leads lack transparency therefore threaten acceptance especially areas sensitive terms fundamental security law proprietary software therefore avoided possible instead state bodies rely opensource solutions develop ideally interdisciplinary teams developers practical ethics commission recommends federal government consider amending procurement law minimise aforementioned negative effects proprietary software need fear effectiveness system suffer result transparency i. e. exploitation effects ruled software developed open consultative process inclusion civil society stakeholders 217 f 7. algorithmic state bodies 7.7 risk involved automated total enforcement ethics commission refuses ethical point view acknowledge general non-compliance rules regulations however automated total enforcement law raises number ethical concerns example citizens might feel full enforcement practice places everyone suspicion turn reduces general willingness obey rules regulations furthermore automated enforcement danger complexity real-life situations sufficiently portrayed particular unforeseen exceptional situations example speeding private vehicle taking seriously injured individual hospital sufficiently taken consideration finally many laws originally enacted total enforcement general rule therefore designed way override technical enforcement specific case addition law enforcement measure constitutes state intervention must based principle proportionality part f lgorithmic summary important recommendations action algorithmic state bodies 68 state must interests citizens make best available technologies including algorithmic must also exercise particular prudence actions view obligation preserve fundamental act role model general rule therefore algorithmic authorities assessed basis criticality model particularly sensitive entailing least comprehensive risk assessment 69 areas law-making dispensation justice algorithmic may peripheral tasks particular algorithmic must undermine functional independence courts democratic process way contrast enormous potential exists algorithmic connection administrative tasks particular relating provision services benefits legislator take due account fact giving green light greater number partially fully automated administrative procedures cautious consideration therefore given expanding scope section 35a german administrative procedures act verwaltungsverfahrensgesetz vwvfg couched overly restrictive terms corresponding provisions statutory law measures must accompanied adequate steps protect citizens 70 decisions taken state basis algorithmic must still transparent must still possible provide justifications may necessary clarify expand existing legislation freedom transparency order achieve goals furthermore algorithmic negate principle decisions made authorities must generally justified individually contrary principle may impose limits overly complex algorithmic finally greater priority accorded opensource solutions since latter may significantly enhance transparency government actions 71 ethical point view general non-compliance rules regulations time however automated “ total ” enforcement law raises number different ethical concerns general rule therefore designed way override technical enforcement specific case balance struck potential transgression automated perhaps preventive enforcement measure must times meet requirements proportionality principle 219 f 8. liabilit algorithmic 8. liability algorithmic 8.1 significance criminal responsibility administrative sanctions liability damages vital components ethically sound regulatory framework especially algorithmic technologies ethical perspective ethics commission also highlights particular role tort law serves compensation prevention damage therefore significantly contributes protection legally protected interests line fundamental ethical perspective following requirements inter alia must set liability system needs keep technologies sufficient compensation victims particular case legally protected interests highly relevant terms fundamental compensation comparable situation involving humans conventional would owed b provision behavioural incentives whereby damage paid actors caused damage avoidable undesirable behaviour whose sphere risk question resulted c fairness whereby actors liable pay damages example placed system market exercise control system benefit efficiency whereby costs covered internalised actors avoid insure costs least amount effort.8.2 harm caused algorithmic 8.2.1 liability “ electronic person ” ethics commission expressly advises granting robots autonomous legal personality often discussed using keyword “ e-person ” intention making liable e. g. self-driving car registered owner “ operates ” mobility service measure would achieve allocation responsibility liability harm responsible system ultimately benefit economically fact measure could conversely evade responsibility legal personality machines type legal entity would enable desirable outcome achieved could achieved freely easily another way example help company law treating autonomous machines even analogy natural persons would dangerous mistake 8.2.2 vicarious liability “ autonomous ” ethics commission believes however harm caused autonomous attributed operating according rules vicarious liability would apply case auxiliaries cf particular section 278 german civil code actor uses system order broaden range activities example hospital uses surgical robot event malfunction able release liability actor uses vicarious agent example surgeon liable culpable misconduct vicarious agent treated behaviour part actor becomes particularly important case liability algorithmic system otherwise liability loopholes easily occur breach duty care person behind proven monitoring algorithmic system 220 part f lgorithmic example 18 surgical robot hospital makes operational incision long causes complications algorithmic system incorrectly derives score creditworthiness bank ’ customer customer take one-off attractive offer relating property may occasionally difficult establish adequate equivalent “ standard care ” autonomous particular soon abilities machine exceed majority cases however malfunctions distinguishable normal functions therefore general cited operator ’ liability standard must defined based comparable available market whereby question could expected operator must decided based general principles e. g. respect question quality surgical robot differ question quality x-ray device 8.2.3 strict liability essentially well-known fact rules relating classic fault-based liability always sufficient resolving legal issues arise case dangerous products legal system far come range different answers challenge particular include ●modification fault-based liability example adaptations standard care various ways easing burden proof reversal burden proof ●various bases strict liability i. e. facilities activities typically cause harm account benefit society whole prohibited ●product liability accordance german act liability defective products gesetz über die haftung für fehlerhafte produkte prodhaftg acts special form liability regardless fault differs strict liability account fact requires inter alia product defect therefore comes fairly close fault-based liability steps must taken ensure answers lead legally watertight solutions terms compensation harm caused dangerous applications operation applications currently involves legal uncertainties liability loopholes primarily result unpredictability harmful events including applications placed market hence possibly failure classic fault-based liability also result fact various different actors applications interact generally speaking almost impossible prove error occurred and/or cause error open dynamic nature ecosystems close functional interplay products contents services also present challenge legal uncertainties perspective companies consumers obstacles innovation acceptance technologies harmful events routinely assigned terms liability compensated impact market intended achieved liability provisions achieved order create appropriate balance interests legislator must provide transparency responsibility responsibilities clarified possible insure harm damage practice 221 f 8. liabilit algorithmic ethics commission solve point complex technical legal questions arise pin solutions terms liability law especially instances chances finding solution level explored first ethical perspective crucial legal clarity legal certainty particular regard liability principles described created however debate currently stands appears highly likely addition appropriate amendments product liability directive → see section 8.2.4 certain changes may need made rules relating fault-based liability and/or bases strict liability may need introduced legislative process firstly necessary determine liability regime appropriate particular types products services exact shape regime take depending criticality → see section 3.1 relevant system also criteria specifically relevant within context liability strict liability example based model involving car owner ’ liability could appropriate cases regarding devices operational risk similarly uncontrollable could end leading harm life limb part question insurability and/or possible compulsory insurance must always play role decision must also always taken type harm subject liability e. g. personal injury damage property loss pure financial losses non-material damage ultimately case decision need taken taking consideration liability principles described party liability assigned particular three possible parties liability could assigned two could possibly also jointly severally liable 18 liability concept differentiated liability operator ecosystems see report entitled “ liability emerging technologies ” commission ’ expert group liability technologies technologies formation september 11 p. 40 et seqq ●the individual registered owner system i. e. owner person similar uses system purposes ●the manufacturer system ●the operator system i. e. whoever exercises greater control system ’ operation individual registered owner front-end operator back-end operator may also manufacturer .18 determination party type liability always depend specific type networked autonomous system identification specific spheres liability 8.2.4 product security product liability overall currently important highlight paradigm shift situation whereby products simply placed market situation whereby products placed market additional services continue provided products thereafter ongoing product monitoring product maintenance becoming important security protection standards fulfilled product leaves production plant also must continue met part subsequent software updates conversely event security gaps subsequently appear manufacturer accordance provisions directives services trade goods subject duty provide security updates line consumers ’ reasonable expectations regarding service life 222 part f lgorithmic example 19 security updates provided smart home system result following cyber-attack house broken product liability directive 1980s longer able cover features networked hybrid autonomous products ethics commission recommends part evaluation revision product liability directive level federal government push watertight legal provisions particular following aspects inclusion services including algorithmic term “ product ” b liability product defects appear product placed market result self-modifying software provision updates failure provide productspecific feeds c liability breaches product monitoring obligation inclusion legally protected interests typically affected product safety particular informational self-determination compensation regimes e adaptation risk defence.8.3 need reassessment liability law ecosystems throw variety issues connection liability responsibility example extent liability loophole current tort law cases damage products provided neither recognised ‘ ’ infringed e. g. ownership storage medium statute intended protect another person breached e. g. provisions criminal law conditions intentional damage contrary policy met technologies often also involve opportunistic people ’ infrastructures e. g. systematic collation third parties sensor generated private iot devices direct computing capacity transmission functions create complicated liability issues contexts stronger contract law major harm damage particular expense consumers caused account fact usability high-value goods real property machines cars etc becoming increasingly dependent long-term provision services software updates user accounts etc provision services guaranteed and/ even specifically suspended order put individuals pressure electronic repossession 223 f 8. liabilit algorithmic ecosystems also extent characterised interaction numerous components operators whereby often disproportionately difficult injured party prove several potential tortfeasors e. g. hardware supplier suppliers various software components feed provider network operator caused harm hand technologies create lack transparency regard cause harm damage conversely also help documenting course causal events unprecedented way question therefore arises actor obliged contribute providing clarification cause harm already logging ex-ante actually recorded via logging disclosed event harm ethics commission therefore recommends overall federal government investigate extent current liability law kept challenges ecosystems needs reworked priority must given striving achieve solution level ethics commission advises context tendency towards one-sided specific technological features particular feature machine whilst machine creates certain additional dangers involves certain additional issues regarding assignment liability challenges liability law attributable factors e. g. intangibility interaction numerous components networking decentralisation part f lgorithmic summary important recommendations action liability algorithmic 72 liability damages alongside criminal responsibility administrative sanctions vital component ethically sound regulatory framework algorithmic already apparent today algorithmic pose challenges liability law currently stands inter alia complexity dynamism growing “ autonomy ” ethics commission therefore recommends current provisions liability law undergo in-depth checks necessary revisions scope checks revisions restricted basis narrowly defined technological features machine 73 proposal future system legal personality would granted high-autonomy algorithmic would liable damages “ electronic person ” pursued far concept protagonists based purported equivalence machine ethically indefensible far boils introducing type company company law fact solve pertinent problems 74 way contrast harm caused autonomous way functionally equivalent employment auxiliaries operator ’ liability making correspond otherwise existing vicarious liability regime principal auxiliaries cf particular section 278 german civil code example bank uses autonomous system check creditworthiness customers liable towards least extent would employee perform task 75 debate currently stands appears highly likely appropriate amendments need made product liability directive dates back 1980s connection established product safety standards addition certain changes may need made rules relating fault-based liability and/ bases strict liability may need introduced case necessary determine liability regime appropriate particular types products services exact shape regime take depending criticality relevant algorithmic system consideration also given innovative liability concepts currently developed level pathpart g 226 part g e uropean path ethics commission examined great many different questions discussions questions raised ones turn alone indicate opinion serve one many building blocks larger edifice broad-based debate future ethics law must return debate must interdisciplinary outset encompass broad range sciences diverse mix representatives worlds economy civil society politics view immense economic pressure fast-paced nature technological change findings emerge debate must integrated ongoing basis activities parties involved levels shape technological future founded values transfers algorithmic transcend national boundaries means forward-looking discussion ethical legal issues arising connection algorithmic must restricted national level need view problems global perspective accordingly strive present findings perspectives pan-european debate well lessons learned implementing gdpr shown economic clout economic area significance market operators providers algorithmic may ultimately mean latter prompted economic interests comply ’ basic requirements developing implementing products services requirements also ever non-european governments reference point drafting regulatory frameworks.the debate needs take place therefore priority topic agendas international forums oecd council europe united nations g7 g20 mind ethics commission recommends federal government make voice heard within international bodies particular german presidency council second half utilised opportunity promote measures deal governance algorithmic proposed opinion level ethics commission also believes federal government actively involved early stages process ongoing basis establishment international panel ipai initiated level g7 global contest future technologies germany europe confronted value models society cultures differ widely prompted debate whether germany europe adapt one non-european models order remain competitive ethics commission supports “ path ” followed date often referred debates “ third way ” strikes balance us chinese positions asserts defining feature technologies consistent alignment values fundamental particular enshrined union ’ charter fundamental council europe ’ convention protection fundamental freedoms order remain actively involved future debate interplay ethics law sovereignty germany europe must preserved greatest extent possible reference nation states organisations term “ sovereignty ” encompasses every aspect processing i.e control storage transfer sensitive held bodies autonomous decisions access 227 part g e uropean path globalised world people states companies co-exist side side requires cross-border flows internet – serves conduit flows – global “ network networks ” distributed global structure embraces different legal societal renders complete sovereignty impossible task debate sovereignty must therefore tackle vital questions relating technical infrastructure including hardware networks control components routers address servers centres view preserving sovereignty germany europe given huge extent reliant foreign products ethics commission believes urgent need take action german level investments developing safeguarding appropriate technologies infrastructures virtually important basic internet infrastructure components germany indeed europe whole procured continents present efforts preserve sovereignty must restricted two main avenues open us first critical analysis assessment basic components second application highest possible security standards operating order minimise risk misuse foreign states organisations looking future however ethics commission believes important germany europe whole develop higher level sovereignty level technical infrastructure support available r work comply highest possible standards security work kind would include design components replace previous attempts engineer integrated solutions existing components achieve required level protection spite known suspected inadequacies security risks.the sovereignty nation state viewed relation nation states also relation non-state actors wield significant amounts power economy grows trend economic power concentrated hands emergence power imbalances apparent ever greater extent r work algorithmic technologies carried within framework established small group giants companies often act important source funding therefore say past decades intermediaries played increasingly important role forming opinions therefore influencing sociopolitical discourse means associated risk abuse also increased given importance ethical legal fundamental values freedoms preserve sovereignty germany europe ethics commission believes urgent need closely monitor shifts power structures vital functioning democratic state social market economy efficiently regulate according areas wherever needed excessive dependence others turns nation rule taker rather rule maker resulting citizens nation subject requirements imposed players elsewhere world embarking efforts safeguard sovereignty long term therefore politically far-sighted necessity also expression ethical responsibility appendix 230 appendi x 1. federal government ’ key questions ethics commission coalition agreement “ set ethics commission within next year provide government parliament proposals develop policy deal algorithms innovation clarification ethics questions add impetus process help define approach towards resolving social conflicts within area policy. ” key questions ethics commission digitisation fundamentally changing society data-based technologies beneficial people ’ everyday lives well industry environment science society whole potential enor mous time digitisation also clearly brings certain risks numerous ethical legal questions raised particularly concerning effects develop ments desired role technologies change benefit whole society need examine possible consequences technologies establish ethical safeguards one challenge develop 21st-century law way protects dignity “ must become mere object ” guarantees fundamental general personality privacy informational self-determi nation freedom discrimination freedom science freedom conduct business freedom expres sion – bringing equilibrium one another complex tensions principles common good progress innovation solidarity task commission – identified current state discussion legislation euro pean international level ascertained possibilities positive action national level given special consideration sensitive areas – develop ethical standards guidelines protection individuals preservation social cohesion safeguarding promotion prosperity age commission also tasked providing federal government recommendations regulatory proposals ethical guidelines developed respected implemented monitored propos als also include description underlying concepts well assessments possible consequences side effects appropriately involved work commission order help ethics commission carry work federal government provided following key questions three areas 231 1. federal government ’ key questions ethics commission i. algorithmic decision-making adm advanced automation increasingly shaping economic social realities people ’ everyday lives collection analysis enable develop ment innovative interpretation models also make prepare algorithm-based decisions algorithms make possible example recognise patterns differences behaviour different groups whether matter setting individual prices e-commerce assessing creditworthiness selecting candidates recruitment procedures people evaluated technical processes areas life evaluation predictions individual behaviour offer opportunities e.g aiding strengthening innovation within industry increasing efficiency processing processes also harbour risks e.g individual freedom self-determination participation equal opportunities among certain individuals social groups social inequality discrimination individuals groups individuals perpetuated biases incorporated programming algorithm training risks particularly acute participation-relevant personality-sensitive adm processes following questions arise especially regard consumer protection ●what ethical limits using adm processes ethical limits ●can ethically necessary adm processes ●are characteristics criteria certain kinds incorporated adm pro cesses – due age origin example ●how determine prejudices distor tions areas ethically undesirable effects adm processes social groups ●what regulatory approaches could prevent manipulation unequal treatment discrimination ●is advisable graduated regulatory frame work based risk social participation potential discrimination ●how reliability reproducibility scrutiny adm guaranteed ●are limits adm crite ria explained people affected ●are test methods make self-learning adm open scrutiny ii industrial administra tive environments deploying highly automated methods ability “ learn ” training addition work done simulating cognitive functions brain developments field raise question dignity autonomy self-determination individual safeguarded fostered leads questions following ●what fundamental ethical principles must observed developing programming using ●where ethical boundaries lie using robots especially special areas life care/ assistance dealing particularly vulnerable groups children elderly people disabilities ethically necessary ●is “ ethics design ” possible could implemented monitored ●how ensured machines working basis controlled 232 appendi x ●to creations/inventions generated ascribed bear responsibility malfunctioning responsibility actors involved programmers scientists clients etc made transparent ●what else necessary future sustainably guarantee freedoms fundamental upon society based iii digitisation characterised increase volume big vast accumulation individual actors high speed processing real time connectivity internet complex networks actors internet things increasing ubiquity permanence various methods analysis amount available increases ability un dertake granular analyses develop business models change value-added chains work processes regarded commodi ty enables value creation “ economy ” national level current laws e.g general protection regulation open legislation numerous legislative initiatives concern handling e.g eprivacy regula tion legislative proposals regarding free flow one hand intended safeguard funda mental informational self-de termination hand intended enable useful innovative processing proposals discussed whether access trade could regulated first time better regulated.in process following questions may arise regard ing handling general access ●what ethical limits economization ●who permitted derive economic benefit ●should obligation offer payment models ●is advisable uniform rules apply equally preference given rules apply specific areas e.g brain connecting factor rules apply specific areas ●what consequences existing access exclusivity competition innovation consequences would additional access exclusivity ●is need state offer support part provision general services citizens navigate internet social networks responsible competent confident manner learn handle provision particular open become part provision services state ●how much transparency necessary appropriate safeguard informational self-determina tion enable citizens participate economic life self-determined manner ●do particular life circumstances require special protec tion concepts specific user groups ●are existing institutions sensitive areas sufficient ensure ethically adequate stakeholder representation ensured long term 233 1. federal government ’ key questions ethics commission ●what effects extensive collections functioning market economy e.g compet itiveness asymmetry suppliers consumers possibility developing inno vative products democracy e.g recording analysing behaviour social networks necessary action taken power/data silos especially intermediaries ●should access declared good certain cases cases ethical criteria ●the non-personal collective effects example individuals certain population groups may placed disadvantage analysis shows payment habits worse par ticular neighbourhood regulatory instruments would needed sectors ●are statutory regulations improving access possible necessary advisable ●should processing prohibited certain cases ethical reasons example cases involving certain types e.g political views brain certain areas e.g profiling political purposes elections ●under circumstances ethical obligation ●does legal system sufficiently recognise possible benefits processing common good achieved ●is possible advisable create experimentation clauses testing applications regulatory instruments ●does make sense invest infrastructures ones ●how constitutionally protected interests individuals enterprises science art reconciled interest last revised 5 june 234 appendi x 2. members ethics commission co-spokespersons prof. dr christiane wendehorst ●professor civil law university vienna ●co-head department innovation digitalisation law university vienna ●president law institute eli prof. dr christiane woopen ●professor ethics theory medicine head unit ethics university clinic cologne ●executive director cologne ethics economics social sciences health ceres university cologne ●chair group ethics science technologies ege members prof. dr johanna haberer ●professor christian media studies friedrich alexander university erlangen nuremberg fau ●director institute practical theology friedrich alexander university erlangen nuremberg fau marit hansen ●data protection commissioner land schleswig-holstein ●head unabhängiges landeszentrum für datenschutz schleswig-holstein independent centre privacy protection schleswig-holstein prof. dr dirk heckmann ●full professor law security digitization technical university munich tum ●director bavarian institute transformation ●judge bavarian constitutional court prof. ulrich kelber ●federal commissioner protection freedom ●honorary professor bonn-rhein-sieg university applied sciences h-brs 235 2. members ethics commission prof. dieter kempf ●president federation german industries bdi ●honorary professor friedrich alexander university erlangen nuremberg fau prof. dr mario martini ●professor administration law administrative law law german university administrative sciences speyer duv speyer ●head programme area “ transfor mation state age ” deputy director german institute administration föv klaus müller ●executive director federation german consumer organisations vzbv ●lecturer heinrich heine university düsseldorf hhu paul nemitz ●principle advisor commis sion directorate-general justice consumers prof. dr sabine sachweh ●professor applied software engineering dortmund university applied sciences arts fh dortmund ●spokesperson board member institute transformation application living domains idial dortmund university applied sciences arts fh dortmund ●co-spokesperson “ digitalisation education elderly ” advisory council federal ministry family affairs senior citizens women youthchristin schäfer ●founder managing director company acs plus science boutique ●advisor big analytics group german economic institute cologne iw köln prof. dr rolf schwartmann ●professor civil law economic law cologne university applied sciences th köln ●head centre media law cologne university applied sciences th köln ●chairman german association protection security gdd prof. dr judith simon ●professor ethics technol ogy university hamburg uhh prof. dr wolfgang wahlster ●professor computer science chair saarland university ●ceo/cea german dfki ●head steering committee standardisation roadmap german institute standardization din prof. dr thomas wischmeyer ●assistant professor tenure track law law university bielefeld current 10 october imprint berlin december opinion ethics commissionpublisher ethics commission federal governmentfederal ministry interior building communityalt-moabit 140 10557 berlinfederal ministry justice consumer protectionmohrenstraße 37 10117 berlin e-mail datenethikkommission_gs bmi.bund.dedatenethikkommission_gs bmjv.bund.de website www.datenethikkommission.de design atelier hauer dörfler gmbh berlin photo credits p. 53 shutterstock.com p. 234 bmi group photo studio wilke christiane wendehorst reiner zensen christiane woopen bpa/kugler ulrich kelber p. 235 christian kruppa dieter kempf vzbv/gert baumbach klaus müller markus mielek sabine sachweh th köln/schmülgen rolf schwartmann uhh/nicolai judith simon jim rakete wolfgang wahlster printingbrandenburgische universitätsdruckerei und verlags gesellschaft potsdam mbh bud © dek']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Name of the document</th>\n",
       "      <th>Institution</th>\n",
       "      <th>URL</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Affiliates</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Country</th>\n",
       "      <th>Date</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "      <th>Unnamed: 29</th>\n",
       "      <th>text</th>\n",
       "      <th>langue</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>categorie Institution</th>\n",
       "      <th>theme</th>\n",
       "      <th>topic</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>The Toronto Declaration: Protecting the right ...</td>\n",
       "      <td>Access Now</td>\n",
       "      <td>https://www.torontodeclaration.org/declaration...</td>\n",
       "      <td>Anna Bacciarelli, Joe Westby, Fanny Hidvegi, E...</td>\n",
       "      <td>Access Now, AI Now Institute at New York Unive...</td>\n",
       "      <td>civil society</td>\n",
       "      <td>UK</td>\n",
       "      <td>2018-05-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\nThe Toronto Declaration\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>en</td>\n",
       "      <td>toronto declaration skip main content toronto ...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>ONG et initiatives de droits humains</td>\n",
       "      <td>Justice and fairness</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>Report on Artificial Intelligence and Human So...</td>\n",
       "      <td>Advisory Board on Artificial Intelligence and ...</td>\n",
       "      <td>https://www8.cao.go.jp/cstp/tyousakai/ai/summa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Report  on   Artificial Intelligence and...</td>\n",
       "      <td>en</td>\n",
       "      <td>report artificial intelligence human society u...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>Gouvernements et organismes publics nationaux</td>\n",
       "      <td>Beneficence</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>28</td>\n",
       "      <td>AI Now 2019 Report</td>\n",
       "      <td>AI Now Institute</td>\n",
       "      <td>https://ainowinstitute.org/wp-content/uploads/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n  AI Now 2019 Report   |   2     AUTHORS A...</td>\n",
       "      <td>en</td>\n",
       "      <td>ai 2019 report 2 authors contributors kate cra...</td>\n",
       "      <td>[0.         0.00410139 0.         ... 0.      ...</td>\n",
       "      <td>Instituts de recherche et universités</td>\n",
       "      <td>Privacy</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35</td>\n",
       "      <td>A toolkit for centering racial equity througho...</td>\n",
       "      <td>AISP</td>\n",
       "      <td>https://aisp.upenn.edu/wp-content/uploads/2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Centering Racial Equity  Throughout   Data Int...</td>\n",
       "      <td>en</td>\n",
       "      <td>centering racial equity throughout data integr...</td>\n",
       "      <td>[0.         0.00248242 0.         ... 0.      ...</td>\n",
       "      <td>Associations professionnelles et groupes de ré...</td>\n",
       "      <td>Justice and fairness</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>39</td>\n",
       "      <td>AI and Human Rights</td>\n",
       "      <td>All Tech is Human</td>\n",
       "      <td>https://alltechishuman.org/ai-human-rights-report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nAI and Human Rights: Building a ...</td>\n",
       "      <td>en</td>\n",
       "      <td>ai human rights building tech future aligned p...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>Associations professionnelles et groupes de ré...</td>\n",
       "      <td>Sustainable</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>718</td>\n",
       "      <td>AI procurement in a box: AI government procure...</td>\n",
       "      <td>World Economic Forum (WEF)</td>\n",
       "      <td>https://www3.weforum.org/docs/WEF_AI_Procureme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>international</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AI Procurement in a Box:   Project overviewUnl...</td>\n",
       "      <td>en</td>\n",
       "      <td>ai procurement box project overviewunlocking p...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>Associations professionnelles et groupes de ré...</td>\n",
       "      <td>Trust</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>719</td>\n",
       "      <td>Artificial Intelligence for Children - Toolkit</td>\n",
       "      <td>World Economic Forum (WEF)</td>\n",
       "      <td>https://www3.weforum.org/docs/WEF_Artificial_I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>international</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AI Procurement in a Box:   Project overviewUnl...</td>\n",
       "      <td>en</td>\n",
       "      <td>ai procurement box project overviewunlocking p...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>Associations professionnelles et groupes de ré...</td>\n",
       "      <td>Trust</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>720</td>\n",
       "      <td>Guidelines for AI procurement</td>\n",
       "      <td>Department for Digital</td>\n",
       "      <td>https://www.gov.uk/government/publications/gui...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White Paper Guidelines for AI Procurement Sept...</td>\n",
       "      <td>en</td>\n",
       "      <td>white paper guidelines ai procurement septembe...</td>\n",
       "      <td>[0.         0.00773414 0.         ... 0.      ...</td>\n",
       "      <td>Gouvernements et organismes publics nationaux</td>\n",
       "      <td>Responsibility</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>721</td>\n",
       "      <td>How to prevent Discriminatory Outcomes in Mach...</td>\n",
       "      <td>World Economic Forum (WEF)</td>\n",
       "      <td>http://www3.weforum.org/docs/WEF_40065_White_P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>international</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White Paper Guidelines for AI Procurement Sept...</td>\n",
       "      <td>en</td>\n",
       "      <td>white paper guidelines ai procurement septembe...</td>\n",
       "      <td>[0.         0.00773414 0.         ... 0.      ...</td>\n",
       "      <td>Associations professionnelles et groupes de ré...</td>\n",
       "      <td>Responsibility</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>722</td>\n",
       "      <td>White paper: A Framework for Responsible Limit...</td>\n",
       "      <td>World Economic Forum (WEF)</td>\n",
       "      <td>https://www3.weforum.org/docs/WEF_Framework_fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>international</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White Paper Guidelines for AI Procurement Sept...</td>\n",
       "      <td>en</td>\n",
       "      <td>white paper guidelines ai procurement septembe...</td>\n",
       "      <td>[0.         0.00773414 0.         ... 0.      ...</td>\n",
       "      <td>Associations professionnelles et groupes de ré...</td>\n",
       "      <td>Responsibility</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                               Name of the document  \\\n",
       "3        13  The Toronto Declaration: Protecting the right ...   \n",
       "9        21  Report on Artificial Intelligence and Human So...   \n",
       "15       28                                 AI Now 2019 Report   \n",
       "20       35  A toolkit for centering racial equity througho...   \n",
       "24       39                                AI and Human Rights   \n",
       "..      ...                                                ...   \n",
       "448     718  AI procurement in a box: AI government procure...   \n",
       "449     719     Artificial Intelligence for Children - Toolkit   \n",
       "450     720                      Guidelines for AI procurement   \n",
       "451     721  How to prevent Discriminatory Outcomes in Mach...   \n",
       "452     722  White paper: A Framework for Responsible Limit...   \n",
       "\n",
       "                                           Institution  \\\n",
       "3                                           Access Now   \n",
       "9    Advisory Board on Artificial Intelligence and ...   \n",
       "15                                    AI Now Institute   \n",
       "20                                                AISP   \n",
       "24                                   All Tech is Human   \n",
       "..                                                 ...   \n",
       "448                         World Economic Forum (WEF)   \n",
       "449                         World Economic Forum (WEF)   \n",
       "450                             Department for Digital   \n",
       "451                         World Economic Forum (WEF)   \n",
       "452                         World Economic Forum (WEF)   \n",
       "\n",
       "                                                   URL  \\\n",
       "3    https://www.torontodeclaration.org/declaration...   \n",
       "9    https://www8.cao.go.jp/cstp/tyousakai/ai/summa...   \n",
       "15   https://ainowinstitute.org/wp-content/uploads/...   \n",
       "20   https://aisp.upenn.edu/wp-content/uploads/2022...   \n",
       "24   https://alltechishuman.org/ai-human-rights-report   \n",
       "..                                                 ...   \n",
       "448  https://www3.weforum.org/docs/WEF_AI_Procureme...   \n",
       "449  https://www3.weforum.org/docs/WEF_Artificial_I...   \n",
       "450  https://www.gov.uk/government/publications/gui...   \n",
       "451  http://www3.weforum.org/docs/WEF_40065_White_P...   \n",
       "452  https://www3.weforum.org/docs/WEF_Framework_fo...   \n",
       "\n",
       "                                               Authors  \\\n",
       "3    Anna Bacciarelli, Joe Westby, Fanny Hidvegi, E...   \n",
       "9                                                  NaN   \n",
       "15                                                 NaN   \n",
       "20                                                 NaN   \n",
       "24                                                 NaN   \n",
       "..                                                 ...   \n",
       "448                                                NaN   \n",
       "449                                                NaN   \n",
       "450                                                NaN   \n",
       "451                                                NaN   \n",
       "452                                                NaN   \n",
       "\n",
       "                                            Affiliates         Sector  \\\n",
       "3    Access Now, AI Now Institute at New York Unive...  civil society   \n",
       "9                                                  NaN            NaN   \n",
       "15                                                 NaN            NaN   \n",
       "20                                                 NaN            NaN   \n",
       "24                                                 NaN            NaN   \n",
       "..                                                 ...            ...   \n",
       "448                                                NaN            NaN   \n",
       "449                                                NaN            NaN   \n",
       "450                                                NaN            NaN   \n",
       "451                                                NaN            NaN   \n",
       "452                                                NaN            NaN   \n",
       "\n",
       "           Country        Date Keywords  ...  Unnamed: 28 Unnamed: 29  \\\n",
       "3               UK  2018-05-16      NaN  ...          NaN         NaN   \n",
       "9            Japan         NaN      NaN  ...          NaN         NaN   \n",
       "15             USA         NaN      NaN  ...          NaN         NaN   \n",
       "20             USA         NaN      NaN  ...          NaN         NaN   \n",
       "24             USA         NaN      NaN  ...          NaN         NaN   \n",
       "..             ...         ...      ...  ...          ...         ...   \n",
       "448  international         NaN      NaN  ...          NaN         NaN   \n",
       "449  international         NaN      NaN  ...          NaN         NaN   \n",
       "450             UK         NaN      NaN  ...          NaN         NaN   \n",
       "451  international         NaN      NaN  ...          NaN         NaN   \n",
       "452  international         NaN      NaN  ...          NaN         NaN   \n",
       "\n",
       "                                                  text  langue  \\\n",
       "3    \\n\\n\\n\\n\\nThe Toronto Declaration\\n\\n\\n\\n\\n\\n\\...      en   \n",
       "9          Report  on   Artificial Intelligence and...      en   \n",
       "15     \\n  AI Now 2019 Report   |   2     AUTHORS A...      en   \n",
       "20   Centering Racial Equity  Throughout   Data Int...      en   \n",
       "24   \\n\\n\\n\\n\\n\\n\\nAI and Human Rights: Building a ...      en   \n",
       "..                                                 ...     ...   \n",
       "448  AI Procurement in a Box:   Project overviewUnl...      en   \n",
       "449  AI Procurement in a Box:   Project overviewUnl...      en   \n",
       "450  White Paper Guidelines for AI Procurement Sept...      en   \n",
       "451  White Paper Guidelines for AI Procurement Sept...      en   \n",
       "452  White Paper Guidelines for AI Procurement Sept...      en   \n",
       "\n",
       "                                        text_processed  \\\n",
       "3    toronto declaration skip main content toronto ...   \n",
       "9    report artificial intelligence human society u...   \n",
       "15   ai 2019 report 2 authors contributors kate cra...   \n",
       "20   centering racial equity throughout data integr...   \n",
       "24   ai human rights building tech future aligned p...   \n",
       "..                                                 ...   \n",
       "448  ai procurement box project overviewunlocking p...   \n",
       "449  ai procurement box project overviewunlocking p...   \n",
       "450  white paper guidelines ai procurement septembe...   \n",
       "451  white paper guidelines ai procurement septembe...   \n",
       "452  white paper guidelines ai procurement septembe...   \n",
       "\n",
       "                                                 tfidf  \\\n",
       "3                              [0. 0. 0. ... 0. 0. 0.]   \n",
       "9                              [0. 0. 0. ... 0. 0. 0.]   \n",
       "15   [0.         0.00410139 0.         ... 0.      ...   \n",
       "20   [0.         0.00248242 0.         ... 0.      ...   \n",
       "24                             [0. 0. 0. ... 0. 0. 0.]   \n",
       "..                                                 ...   \n",
       "448                            [0. 0. 0. ... 0. 0. 0.]   \n",
       "449                            [0. 0. 0. ... 0. 0. 0.]   \n",
       "450  [0.         0.00773414 0.         ... 0.      ...   \n",
       "451  [0.         0.00773414 0.         ... 0.      ...   \n",
       "452  [0.         0.00773414 0.         ... 0.      ...   \n",
       "\n",
       "                                 categorie Institution                 theme  \\\n",
       "3                 ONG et initiatives de droits humains  Justice and fairness   \n",
       "9        Gouvernements et organismes publics nationaux           Beneficence   \n",
       "15               Instituts de recherche et universités               Privacy   \n",
       "20   Associations professionnelles et groupes de ré...  Justice and fairness   \n",
       "24   Associations professionnelles et groupes de ré...           Sustainable   \n",
       "..                                                 ...                   ...   \n",
       "448  Associations professionnelles et groupes de ré...                 Trust   \n",
       "449  Associations professionnelles et groupes de ré...                 Trust   \n",
       "450      Gouvernements et organismes publics nationaux        Responsibility   \n",
       "451  Associations professionnelles et groupes de ré...        Responsibility   \n",
       "452  Associations professionnelles et groupes de ré...        Responsibility   \n",
       "\n",
       "     topic  probs  \n",
       "3       -1    0.0  \n",
       "9       -1    0.0  \n",
       "15      -1    0.0  \n",
       "20      -1    0.0  \n",
       "24      -1    0.0  \n",
       "..     ...    ...  \n",
       "448     -1    0.0  \n",
       "449     -1    0.0  \n",
       "450     -1    0.0  \n",
       "451     -1    0.0  \n",
       "452     -1    0.0  \n",
       "\n",
       "[99 rows x 39 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"topic\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Name of the document</th>\n",
       "      <th>Institution</th>\n",
       "      <th>URL</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Affiliates</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Country</th>\n",
       "      <th>Date</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "      <th>Unnamed: 29</th>\n",
       "      <th>text</th>\n",
       "      <th>langue</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>categorie Institution</th>\n",
       "      <th>theme</th>\n",
       "      <th>topic</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Intel Recommends Public Policy Principles for ...</td>\n",
       "      <td>Intel</td>\n",
       "      <td>https://community.intel.com/t5/Blogs/Intel/Pol...</td>\n",
       "      <td>Naveen Rao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\tIntel Recommends Public Policy Princip...</td>\n",
       "      <td>en</td>\n",
       "      <td>intel recommends public policy principles arti...</td>\n",
       "      <td>[0.01722362 0.         0.         ... 0.      ...</td>\n",
       "      <td>Entreprises technologiques et multinationales</td>\n",
       "      <td>Privacy</td>\n",
       "      <td>2</td>\n",
       "      <td>0.609578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>Artificial intelligence in Healthcare</td>\n",
       "      <td>Academy of Medical Royal Colleges</td>\n",
       "      <td>https://www.aomrc.org.uk/wp-content/uploads/20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Artificial Intelligence in HealthcareJanuary /...</td>\n",
       "      <td>en</td>\n",
       "      <td>artificial intelligence healthcarejanuary 2019...</td>\n",
       "      <td>[0.         0.00872152 0.         ... 0.      ...</td>\n",
       "      <td>Instituts de recherche et universités</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>Human rights in the age of Artificial Intellig...</td>\n",
       "      <td>Access Now</td>\n",
       "      <td>https://www.accessnow.org/cms/assets/uploads/2...</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HUMAN RIGHTS  IN THE AGE OF ARTIFICIAL INTELLI...</td>\n",
       "      <td>en</td>\n",
       "      <td>human rights age artificial intelligence repor...</td>\n",
       "      <td>[0.        0.0087539 0.        ... 0.        0...</td>\n",
       "      <td>ONG et initiatives de droits humains</td>\n",
       "      <td>Justice and fairness</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>The Toronto Declaration: Protecting the right ...</td>\n",
       "      <td>Access Now</td>\n",
       "      <td>https://www.torontodeclaration.org/declaration...</td>\n",
       "      <td>Anna Bacciarelli, Joe Westby, Fanny Hidvegi, E...</td>\n",
       "      <td>Access Now, AI Now Institute at New York Unive...</td>\n",
       "      <td>civil society</td>\n",
       "      <td>UK</td>\n",
       "      <td>2018-05-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\nThe Toronto Declaration\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>en</td>\n",
       "      <td>toronto declaration skip main content toronto ...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "      <td>ONG et initiatives de droits humains</td>\n",
       "      <td>Justice and fairness</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>Europe’s approach to artificial intelligence: ...</td>\n",
       "      <td>AccessNow</td>\n",
       "      <td>https://www.accessnow.org/wp-content/uploads/2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accessnow.org EUROPE’S APPROACH TO ARTIFICIAL ...</td>\n",
       "      <td>en</td>\n",
       "      <td>accessnow.org europe ’ approach artificial int...</td>\n",
       "      <td>[0.        0.0035214 0.        ... 0.        0...</td>\n",
       "      <td>ONG et initiatives de droits humains</td>\n",
       "      <td>Transparency</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                               Name of the document  \\\n",
       "0       3  Intel Recommends Public Policy Principles for ...   \n",
       "1       9              Artificial intelligence in Healthcare   \n",
       "2      12  Human rights in the age of Artificial Intellig...   \n",
       "3      13  The Toronto Declaration: Protecting the right ...   \n",
       "4      14  Europe’s approach to artificial intelligence: ...   \n",
       "\n",
       "                         Institution  \\\n",
       "0                              Intel   \n",
       "1  Academy of Medical Royal Colleges   \n",
       "2                         Access Now   \n",
       "3                         Access Now   \n",
       "4                          AccessNow   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://community.intel.com/t5/Blogs/Intel/Pol...   \n",
       "1  https://www.aomrc.org.uk/wp-content/uploads/20...   \n",
       "2  https://www.accessnow.org/cms/assets/uploads/2...   \n",
       "3  https://www.torontodeclaration.org/declaration...   \n",
       "4  https://www.accessnow.org/wp-content/uploads/2...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0                                         Naveen Rao   \n",
       "1                                                NaN   \n",
       "2                                                  .   \n",
       "3  Anna Bacciarelli, Joe Westby, Fanny Hidvegi, E...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                          Affiliates         Sector Country  \\\n",
       "0                                                NaN            NaN     NaN   \n",
       "1                                                NaN            NaN      UK   \n",
       "2                                                  .            NaN     NaN   \n",
       "3  Access Now, AI Now Institute at New York Unive...  civil society      UK   \n",
       "4                                                NaN            NaN     USA   \n",
       "\n",
       "         Date Keywords  ...  Unnamed: 28 Unnamed: 29  \\\n",
       "0         NaN      NaN  ...          NaN         NaN   \n",
       "1         NaN      NaN  ...          NaN         NaN   \n",
       "2         NaN      NaN  ...          NaN         NaN   \n",
       "3  2018-05-16      NaN  ...          NaN         NaN   \n",
       "4     2020-12      NaN  ...          NaN         NaN   \n",
       "\n",
       "                                                text  langue  \\\n",
       "0  \\n\\n\\n\\tIntel Recommends Public Policy Princip...      en   \n",
       "1  Artificial Intelligence in HealthcareJanuary /...      en   \n",
       "2  HUMAN RIGHTS  IN THE AGE OF ARTIFICIAL INTELLI...      en   \n",
       "3  \\n\\n\\n\\n\\nThe Toronto Declaration\\n\\n\\n\\n\\n\\n\\...      en   \n",
       "4  accessnow.org EUROPE’S APPROACH TO ARTIFICIAL ...      en   \n",
       "\n",
       "                                      text_processed  \\\n",
       "0  intel recommends public policy principles arti...   \n",
       "1  artificial intelligence healthcarejanuary 2019...   \n",
       "2  human rights age artificial intelligence repor...   \n",
       "3  toronto declaration skip main content toronto ...   \n",
       "4  accessnow.org europe ’ approach artificial int...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0  [0.01722362 0.         0.         ... 0.      ...   \n",
       "1  [0.         0.00872152 0.         ... 0.      ...   \n",
       "2  [0.        0.0087539 0.        ... 0.        0...   \n",
       "3                            [0. 0. 0. ... 0. 0. 0.]   \n",
       "4  [0.        0.0035214 0.        ... 0.        0...   \n",
       "\n",
       "                           categorie Institution                 theme  topic  \\\n",
       "0  Entreprises technologiques et multinationales               Privacy      2   \n",
       "1          Instituts de recherche et universités                 Trust      1   \n",
       "2           ONG et initiatives de droits humains  Justice and fairness      4   \n",
       "3           ONG et initiatives de droits humains  Justice and fairness     -1   \n",
       "4           ONG et initiatives de droits humains          Transparency      7   \n",
       "\n",
       "      probs  \n",
       "0  0.609578  \n",
       "1  1.000000  \n",
       "2  1.000000  \n",
       "3  0.000000  \n",
       "4  1.000000  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_projet_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
